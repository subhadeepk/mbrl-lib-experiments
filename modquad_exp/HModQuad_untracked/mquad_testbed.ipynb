{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71c18d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Connecting to CoppeliaSim...\n",
      "Connected!\n",
      "4 DoF [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "No propeller. Is it a magic box?\n"
     ]
    }
   ],
   "source": [
    "from IPython import display\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import omegaconf\n",
    "from experiment_logger import (\n",
    "    create_modquad_experiment_logger,\n",
    "    log_modquad_experiment_params,\n",
    "    log_modquad_environment_info,\n",
    "    log_modquad_model_info,\n",
    "    log_modquad_agent_info\n",
    ")\n",
    "\n",
    "import modquad_copp_env as modquad_env\n",
    "import meta_learning_base as mlb\n",
    "# import mbrl.env.reward_fns as reward_fns\n",
    "# import mbrl.env.termination_fns as termination_fns\n",
    "import mbrl.models as models\n",
    "import mbrl.planning as planning\n",
    "import mbrl.util.common as common_util\n",
    "import mbrl.util as util\n",
    "import modquad_utils \n",
    "import time\n",
    "from models import modquad_ModelEnv\n",
    "# import models. as mq_model\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "mpl.rcParams.update({\"font.size\": 16})\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "seed = 0\n",
    "env = modquad_env.ModQuadEnv()\n",
    "rng = np.random.default_rng(seed=0)\n",
    "generator = torch.Generator(device=device)\n",
    "generator.manual_seed(seed)\n",
    "obs_shape = env.observation_space.shape\n",
    "act_shape = env.action_space.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e5a23d",
   "metadata": {},
   "source": [
    "Get initial replay buffer. Either run simulation now or load existing flight data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fdde150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to test if gym environment simulation is working\n",
    "import pickle\n",
    "\n",
    "# replay_buffer_sim = env.run_gym_simulation_and_collect_data(cut_at = 600)\n",
    "\n",
    "# with open('quat_600_sim_data.pkl', 'wb') as f:\n",
    "#     pickle.dump(replay_buffer_sim, f)\n",
    "\n",
    "# with open('600_large_sim_data_dt_0.01.pkl', 'wb') as f:\n",
    "#     pickle.dump(replay_buffer_sim, f)\n",
    "\n",
    "# _ = env.run_gym_simulation_and_collect_data(cut_at = 300)\n",
    "\n",
    "with open('quat_600_sim_data.pkl', 'rb') as f:\n",
    "    replay_buffer_sim = pickle.load(f)\n",
    "    \n",
    "# env.end_simulation()\n",
    "# env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b0d96f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected!\n",
      "4 DoF [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "No propeller. Is it a magic box?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([-3.8312464e-08, -1.6547890e-08,  4.6264969e-08, -6.5610797e-09,\n",
       "         4.8497657e-08,  2.0069294e-07, -1.6270590e-04, -1.3943643e-05,\n",
       "         6.5666239e-04], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "env.end_simulation()\n",
    "env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48952709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14478, 4)\n",
      "Means: [ 3.25222877e+00 -2.37496764e-05 -1.14858131e-04  1.01522262e-01]\n",
      "Variances: [0.01127349 0.00054829 0.01140178 0.00576978]\n",
      "Standard Deviations: [0.10617668 0.02341554 0.10677913 0.07595909]\n",
      "Minimums: [ 2.3866805  -0.10301613 -0.28670569 -0.08023392]\n",
      "Maximums: [4.09218592 0.0866907  0.27594365 0.22322088]\n",
      "(14478, 10)\n",
      "Means: [ 3.8639180e-04  4.9862633e-03  1.0937074e-02 -1.3712974e-04\n",
      " -1.1420795e-03  1.2183436e-03  8.8508496e-06  4.9579536e-05\n",
      "  5.2475512e-02  9.9724454e-01]\n",
      "Variances: [3.96992639e-02 9.38166492e-03 1.49817085e-02 2.57527642e-03\n",
      " 2.06866488e-01 1.44890306e-04 6.68567809e-05 1.11687439e-03\n",
      " 1.57208880e-03 3.73381022e-06]\n",
      "Standard Deviations: [0.19924673 0.09685899 0.12239979 0.05074718 0.4548258  0.01203704\n",
      " 0.0081766  0.03341967 0.03964958 0.00193231]\n",
      "Minimums: [-0.6478879  -0.46388495 -0.64670855 -0.2343599  -1.7107099  -0.04097146\n",
      " -0.03536256 -0.09221905 -0.04908156  0.98986685]\n",
      "Maximums: [0.6826183  0.4886786  0.63356763 0.18391222 1.6933316  0.06400897\n",
      " 0.02784326 0.0948517  0.14193906 0.99999994]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "replay_buffer_sim\n",
    "all_actions = []\n",
    "all_obs = []\n",
    "for transition in replay_buffer_sim:\n",
    "    obs, action, next_obs, reward, terminate, truncated = transition\n",
    "    all_obs.append(obs)\n",
    "    all_actions.append(action)\n",
    "\n",
    "all_actions = np.array(all_actions)\n",
    "print(all_actions.shape)\n",
    "\n",
    "means = np.mean(all_actions, axis=0)\n",
    "variances = np.var(all_actions, axis=0)\n",
    "std_devs = np.std(all_actions, axis=0)\n",
    "mins = np.min(all_actions, axis=0)\n",
    "maxs = np.max(all_actions, axis=0)\n",
    "\n",
    "print(\"Means:\", means)\n",
    "print(\"Variances:\", variances)\n",
    "print(\"Standard Deviations:\", std_devs)\n",
    "print(\"Minimums:\", mins)\n",
    "print(\"Maximums:\", maxs)\n",
    "\n",
    "all_obs = np.array(all_obs)\n",
    "print(all_obs.shape)\n",
    "\n",
    "means = np.mean(all_obs, axis=0)\n",
    "variances = np.var(all_obs, axis=0)\n",
    "std_devs = np.std(all_obs, axis=0)\n",
    "mins = np.min(all_obs, axis=0)\n",
    "maxs = np.max(all_obs, axis=0)\n",
    "\n",
    "print(\"Means:\", means)\n",
    "print(\"Variances:\", variances)\n",
    "print(\"Standard Deviations:\", std_devs)\n",
    "print(\"Minimums:\", mins)\n",
    "print(\"Maximums:\", maxs)\n",
    "\n",
    "# pos, angles, velocity, angular_velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17a2799a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# samples stored 14478\n"
     ]
    }
   ],
   "source": [
    "trial_length = 200\n",
    "num_trials = 20\n",
    "ensemble_size = 5\n",
    "\n",
    "# Everything with \"???\" indicates an option with a missing value.\n",
    "# Our utility functions will fill in these details using the \n",
    "# environment information\n",
    "cfg_dict = {\n",
    "    # dynamics model configuration\n",
    "    \"dynamics_model\": {\n",
    "        \"_target_\": \"mbrl.models.GaussianMLP\",\n",
    "        \"device\": device,\n",
    "        \"num_layers\": 3,\n",
    "        \"ensemble_size\": ensemble_size,\n",
    "        \"hid_size\": 256,\n",
    "        \"in_size\": \"???\",\n",
    "        \"out_size\": \"???\",\n",
    "        \"deterministic\": False,\n",
    "        \"propagation_method\": \"fixed_model\",\n",
    "        # can also configure activation function for GaussianMLP\n",
    "        \"activation_fn_cfg\": {\n",
    "            \"_target_\": \"torch.nn.LeakyReLU\",\n",
    "            \"negative_slope\": 0.01\n",
    "        }\n",
    "    },\n",
    "    # options for training the dynamics model\n",
    "    \"algorithm\": {\n",
    "        \"dataset_size\": 30000,\n",
    "        \"learned_rewards\": False,\n",
    "        \"target_is_delta\": True,\n",
    "        \"normalize\": True,\n",
    "    },\n",
    "    # these are experiment specific options\n",
    "    \"overrides\": {\n",
    "        \"trial_length\": trial_length,\n",
    "        \"num_steps\": num_trials * trial_length,\n",
    "        \"model_batch_size\": 64,\n",
    "        \"validation_ratio\": 0.1\n",
    "    }\n",
    "}\n",
    "cfg = omegaconf.OmegaConf.create(cfg_dict)\n",
    "\n",
    "# Create a 1-D dynamics model for this environment\n",
    "dynamics_model = common_util.create_one_dim_tr_model(cfg, obs_shape, act_shape)\n",
    "\n",
    "# Create a gym-like environment to encapsulate the model\n",
    "model_env = modquad_ModelEnv.modquad_ModelEnv(env, dynamics_model, generator=generator)\n",
    "\n",
    "replay_buffer = common_util.create_replay_buffer(cfg, obs_shape, act_shape, rng=rng)\n",
    "\n",
    "for tr in replay_buffer_sim:\n",
    "    obs, action, next_obs, reward, terminate, truncated = tr\n",
    "    replay_buffer.add(obs, action, next_obs, reward, terminate, truncated)\n",
    "print(\"# samples stored\", replay_buffer.num_stored)\n",
    "\n",
    "agent_cfg = omegaconf.OmegaConf.create({\n",
    "    # this class evaluates many trajectories and picks the best one\n",
    "    \"_target_\": \"mbrl.planning.TrajectoryOptimizerAgent\",\n",
    "    \"planning_horizon\": 10,#was 15 initially\n",
    "    \"replan_freq\": 1,\n",
    "    \"verbose\": True,\n",
    "    \"action_lb\": \"???\",\n",
    "    \"action_ub\": \"???\",\n",
    "    # this is the optimizer to generate and choose a trajectory\n",
    "    \"optimizer_cfg\": {\n",
    "        \"_target_\": \"mbrl.planning.CEMOptimizer\",\n",
    "        \"num_iterations\": 5,\n",
    "        \"elite_ratio\": 0.1,\n",
    "        \"population_size\": 100,\n",
    "        \"alpha\": 0.175,\n",
    "        \"device\": device,\n",
    "        \"lower_bound\": \"???\",\n",
    "        \"upper_bound\": \"???\",\n",
    "        \"return_mean_elites\": True,\n",
    "        \"clipped_normal\": False\n",
    "    }\n",
    "})\n",
    "\n",
    "agent = planning.create_trajectory_optim_agent_for_model(\n",
    "    model_env,\n",
    "    agent_cfg,\n",
    "    num_particles=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d59496",
   "metadata": {},
   "source": [
    "Main experiment block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6ce8faf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected!\n",
      "4 DoF [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "No propeller. Is it a magic box?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([-3.60691637e-08, -1.70620389e-08,  1.04601611e-07,  6.79286849e-09,\n",
       "         1.04879405e-07,  2.00711838e-07, -8.13550650e-05, -6.94632308e-06,\n",
       "         3.28326743e-04,  9.99999940e-01], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.end_simulation()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c8c7d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_shape 57\n",
      "trajectory shape (57, 12)\n",
      "resetting environment, and starting trial : 0\n",
      "Connected!\n",
      "4 DoF [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "No propeller. Is it a magic box?\n",
      "setting setpoint to:  0.0 0.0 0.30000000000000004\n",
      "Number of stored transitions:  14478\n",
      "Training model\n",
      "training loss:  -42.72999692430683\n",
      "validation loss:  0.002649058820679784\n",
      "training time:  90.44977402687073\n",
      "Model trained\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.971\n",
      "action:  [2.6594467  0.06729684 0.00788644 0.09116668] reward:  -0.29999999999491545\n",
      "planning time:  0.9873785972595215\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.936\n",
      "action:  [ 4.203225   -0.00855848 -0.01908811  0.12428128] reward:  -0.29999999999491406\n",
      "planning time:  0.9528024196624756\n",
      "planning for trajectory step:  0\n",
      "Planning time: 1.043\n",
      "action:  [4.2245407  0.01636336 0.06797079 0.10393912] reward:  -0.29999998759246393\n",
      "planning time:  1.055736780166626\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.976\n",
      "action:  [ 2.5998476   0.05479727 -0.00416669  0.09898874] reward:  -2.9319900979186406\n",
      "planning time:  0.9904417991638184\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.916\n",
      "action:  [2.1771164  0.07331531 0.2354708  0.22572674] reward:  -4.240688732048619\n",
      "planning time:  0.9339447021484375\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.942\n",
      "Crash detected!\n",
      "Crash detected!\n",
      "action:  [ 3.3300526e+00 -1.3653168e-03 -1.0892028e-02 -8.4242774e-03] reward:  -4.717045151930501\n",
      "planning time:  0.9630477428436279\n",
      "resetting environment, and starting trial : 1\n",
      "Connected!\n",
      "4 DoF [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "No propeller. Is it a magic box?\n",
      "setting setpoint to:  0.0 0.0 0.30000000000000004\n",
      "Number of stored transitions:  14484\n",
      "Training model\n",
      "training loss:  -39.230762107699526\n",
      "validation loss:  0.0028250757604837418\n",
      "training time:  89.96505212783813\n",
      "Model trained\n",
      "planning for trajectory step:  0\n",
      "Planning time: 1.045\n",
      "action:  [ 3.1607628  -0.12517574  0.11242853  0.26775837] reward:  -0.2999998953983918\n",
      "planning time:  1.070936918258667\n",
      "planning for trajectory step:  0\n",
      "Planning time: 1.036\n",
      "action:  [ 3.5043306  -0.11948889 -0.11520164 -0.00920045] reward:  -0.3000002096208784\n",
      "planning time:  1.060439109802246\n",
      "planning for trajectory step:  0\n",
      "Planning time: 1.036\n",
      "action:  [ 4.3093185  -0.01841279  0.05399765  0.03570382] reward:  -1.1021429348682985\n",
      "planning time:  1.0599496364593506\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.991\n",
      "Crash detected!\n",
      "Crash detected!\n",
      "action:  [ 3.6036482e+00 -1.6612392e-02  3.0680601e-03  1.6157433e-01] reward:  -5.444431916691021\n",
      "planning time:  1.0123648643493652\n",
      "resetting environment, and starting trial : 2\n",
      "Connected!\n",
      "4 DoF [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "No propeller. Is it a magic box?\n",
      "setting setpoint to:  0.0 0.0 0.30000000000000004\n",
      "Number of stored transitions:  14488\n",
      "Training model\n",
      "training loss:  -38.095128007963595\n",
      "validation loss:  0.003132422687485814\n",
      "training time:  88.66116571426392\n",
      "Model trained\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.854\n",
      "action:  [ 3.4720533  -0.04443191 -0.08417866  0.16893822] reward:  -0.2999998953983918\n",
      "planning time:  0.8796432018280029\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.839\n",
      "action:  [ 3.4227097  -0.10571074 -0.08668968  0.07864465] reward:  -0.30000020604100736\n",
      "planning time:  0.8611440658569336\n",
      "planning for trajectory step:  0\n",
      "Planning time: 1.010\n",
      "action:  [ 3.7454693  -0.04304799 -0.09116967  0.12018648] reward:  -2.144631066647539\n",
      "planning time:  1.0289995670318604\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.851\n",
      "Crash detected!\n",
      "Crash detected!\n",
      "action:  [ 2.3686845  -0.15085207 -0.16826016  0.2098067 ] reward:  -0.33166305111834943\n",
      "planning time:  0.8719804286956787\n",
      "resetting environment, and starting trial : 3\n",
      "Connected!\n",
      "4 DoF [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "No propeller. Is it a magic box?\n",
      "setting setpoint to:  0.0 0.0 0.30000000000000004\n",
      "Number of stored transitions:  14492\n",
      "Training model\n",
      "training loss:  -40.26326444219141\n",
      "validation loss:  0.002586591523140669\n",
      "training time:  81.00122928619385\n",
      "Model trained\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.881\n",
      "action:  [ 3.3464575e+00 -5.2702188e-04  3.2818682e-02  1.3368389e-01] reward:  -0.2999998953983918\n",
      "planning time:  0.8942651748657227\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.839\n",
      "action:  [ 3.0419714  -0.12490271 -0.02685278  0.07999051] reward:  -0.6401077998456504\n",
      "planning time:  0.8585503101348877\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.870\n",
      "action:  [ 3.9791641e+00  1.0310209e-03 -1.6137201e-01 -1.9360058e-02] reward:  -0.7556891106191923\n",
      "planning time:  0.8913562297821045\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.878\n",
      "action:  [ 4.018235   -0.00875668 -0.16457108 -0.02096732] reward:  -1.3585101277367255\n",
      "planning time:  0.8915126323699951\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.884\n",
      "Crash detected!\n",
      "Crash detected!\n",
      "action:  [ 2.7626941  -0.05320831 -0.1645626   0.17266561] reward:  -0.5563529508795875\n",
      "planning time:  0.8988826274871826\n",
      "resetting environment, and starting trial : 4\n",
      "Connected!\n",
      "4 DoF [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "No propeller. Is it a magic box?\n",
      "setting setpoint to:  0.0 0.0 0.30000000000000004\n",
      "Number of stored transitions:  14497\n",
      "Training model\n",
      "training loss:  -41.93873064424477\n",
      "validation loss:  0.002637522993609309\n",
      "training time:  80.94146680831909\n",
      "Model trained\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.898\n",
      "action:  [3.2710471e+00 5.8408909e-02 1.5279932e-03 1.4068492e-01] reward:  -0.3000001878146745\n",
      "planning time:  0.9246852397918701\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.868\n",
      "action:  [3.314267   0.0071965  0.19464454 0.20038192] reward:  -0.295150744069149\n",
      "planning time:  0.8882508277893066\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.829\n",
      "action:  [ 3.2928557  -0.07522269  0.15837021  0.05782969] reward:  -0.6932658072162035\n",
      "planning time:  0.8454203605651855\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.847\n",
      "action:  [ 3.6582222   0.11355729 -0.21733296  0.22481638] reward:  -3.1912903245762783\n",
      "planning time:  0.8599450588226318\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.865\n",
      "action:  [ 2.8801460e+00 -7.0825994e-02 -6.8775810e-02 -1.2781359e-03] reward:  -1.939061735014046\n",
      "planning time:  0.8853404521942139\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.880\n",
      "Crash detected!\n",
      "Crash detected!\n",
      "action:  [ 4.2865834   0.13566872 -0.07954128  0.24602613] reward:  -0.2861401613403725\n",
      "planning time:  0.9063899517059326\n",
      "resetting environment, and starting trial : 5\n",
      "Connected!\n",
      "4 DoF [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "No propeller. Is it a magic box?\n",
      "setting setpoint to:  0.0 0.0 0.30000000000000004\n",
      "Number of stored transitions:  14503\n",
      "Training model\n",
      "training loss:  -42.60110257653629\n",
      "validation loss:  0.007424792740494013\n",
      "training time:  82.18480730056763\n",
      "Model trained\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.947\n",
      "action:  [ 3.3153677   0.01059579 -0.07397596  0.17745306] reward:  -0.299999981038218\n",
      "planning time:  0.9692120552062988\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.895\n",
      "action:  [ 3.3827372   0.03227387  0.02038442 -0.04943045] reward:  -0.2941519548343142\n",
      "planning time:  0.9141418933868408\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.870\n",
      "action:  [ 3.2886102   0.00911895 -0.02664735 -0.04215427] reward:  -1.6471783026142277\n",
      "planning time:  0.8927803039550781\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.869\n",
      "Crash detected!\n",
      "Crash detected!\n",
      "action:  [ 2.604207    0.04413351 -0.07652575 -0.03001058] reward:  -0.2994319607925714\n",
      "planning time:  0.8865330219268799\n",
      "resetting environment, and starting trial : 6\n",
      "Connected!\n",
      "4 DoF [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "No propeller. Is it a magic box?\n",
      "setting setpoint to:  0.0 0.0 0.30000000000000004\n",
      "Number of stored transitions:  14507\n",
      "Training model\n",
      "training loss:  -40.72584951912484\n",
      "validation loss:  0.003569571767002344\n",
      "training time:  81.60288286209106\n",
      "Model trained\n",
      "planning for trajectory step:  0\n",
      "Planning time: 1.008\n",
      "action:  [3.87933    0.01126639 0.0469731  0.17651434] reward:  -0.2999998953983918\n",
      "planning time:  1.0230352878570557\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.883\n",
      "action:  [3.205076   0.03198021 0.13345867 0.02451388] reward:  -0.3000001896967319\n",
      "planning time:  0.9042501449584961\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.869\n",
      "action:  [ 3.2771697   0.05198636 -0.11363506  0.19655204] reward:  -7.768058017208391\n",
      "planning time:  0.8887197971343994\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.868\n",
      "Crash detected!\n",
      "Crash detected!\n",
      "action:  [ 3.2702045  -0.08486892 -0.01395558  0.24567711] reward:  -8.423660864843665\n",
      "planning time:  0.8871333599090576\n",
      "resetting environment, and starting trial : 7\n",
      "Connected!\n",
      "4 DoF [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "No propeller. Is it a magic box?\n",
      "setting setpoint to:  0.0 0.0 0.30000000000000004\n",
      "Number of stored transitions:  14511\n",
      "Training model\n",
      "training loss:  -42.174661371184555\n",
      "validation loss:  0.0032625191379338503\n",
      "training time:  81.27171683311462\n",
      "Model trained\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.891\n",
      "action:  [ 2.9600222  -0.04417787  0.05623432  0.14642014] reward:  -0.2999998953983918\n",
      "planning time:  0.9059526920318604\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.842\n",
      "action:  [ 3.1102471e+00 -2.7252543e-03  9.5256492e-02 -9.8471306e-03] reward:  -0.3000001886377531\n",
      "planning time:  0.862483024597168\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.811\n",
      "action:  [ 3.0619533  -0.01062412  0.04899899 -0.0452487 ] reward:  -0.29987098598065665\n",
      "planning time:  0.828122615814209\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.919\n",
      "action:  [ 3.0850561e+00 -2.8528490e-03  1.3606291e-01 -4.0729627e-02] reward:  -0.30000020976080916\n",
      "planning time:  0.9392611980438232\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.869\n",
      "action:  [ 3.0311396  -0.0343855  -0.14257833 -0.0078403 ] reward:  -0.2999796078654797\n",
      "planning time:  0.8882486820220947\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.849\n",
      "action:  [3.2734094  0.01277404 0.2237988  0.20784311] reward:  -0.30005171953188414\n",
      "planning time:  0.8620865345001221\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.836\n",
      "action:  [ 3.8605447   0.0097059   0.22459663 -0.0186426 ] reward:  -0.3000002148622729\n",
      "planning time:  0.8517935276031494\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.853\n",
      "Crash detected!\n",
      "Crash detected!\n",
      "action:  [ 2.944368   -0.04255917  0.04213461 -0.0547484 ] reward:  -1.541730682879356\n",
      "planning time:  0.8740348815917969\n",
      "resetting environment, and starting trial : 8\n",
      "Connected!\n",
      "4 DoF [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "No propeller. Is it a magic box?\n",
      "setting setpoint to:  0.0 0.0 0.30000000000000004\n",
      "Number of stored transitions:  14519\n",
      "Training model\n",
      "training loss:  -41.62557925712772\n",
      "validation loss:  0.003185383277013898\n",
      "training time:  82.02126717567444\n",
      "Model trained\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.911\n",
      "action:  [ 3.0672164  -0.03503022 -0.09020723  0.18440016] reward:  -0.2999999537350342\n",
      "planning time:  0.9346344470977783\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.837\n",
      "action:  [3.23034    0.0152023  0.10449048 0.04459044] reward:  -0.2978256414428213\n",
      "planning time:  0.8596117496490479\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.901\n",
      "action:  [3.2117195  0.01758802 0.09516498 0.03966266] reward:  -0.2989901836668234\n",
      "planning time:  0.9248855113983154\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.891\n",
      "action:  [ 3.1851254   0.02692417  0.13757655 -0.00641652] reward:  -0.5132678346397039\n",
      "planning time:  0.9034960269927979\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.868\n",
      "Crash detected!\n",
      "Crash detected!\n",
      "action:  [ 3.996712   -0.12108201 -0.2079396   0.03166099] reward:  -0.46888135490587984\n",
      "planning time:  0.8873083591461182\n",
      "resetting environment, and starting trial : 9\n",
      "Connected!\n",
      "4 DoF [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "No propeller. Is it a magic box?\n",
      "setting setpoint to:  0.0 0.0 0.30000000000000004\n",
      "Number of stored transitions:  14524\n",
      "Training model\n",
      "training loss:  -40.81941608335914\n",
      "validation loss:  0.0038386962842196226\n",
      "training time:  82.37396883964539\n",
      "Model trained\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.889\n",
      "action:  [ 2.8339999   0.00802772 -0.04254607  0.09108014] reward:  -0.2999998953983918\n",
      "planning time:  0.9088411331176758\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.876\n",
      "action:  [3.2209804e+00 1.8814206e-03 2.3481263e-02 4.7245178e-02] reward:  -0.3000001912278664\n",
      "planning time:  0.8918678760528564\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.857\n",
      "action:  [3.2155833  0.00425355 0.03182942 0.05589761] reward:  -0.30000018598789646\n",
      "planning time:  0.8709399700164795\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.886\n",
      "action:  [ 3.191608   -0.0161282  -0.02589226  0.0374212 ] reward:  -0.30000263311921815\n",
      "planning time:  0.9099955558776855\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.890\n",
      "action:  [ 3.2156923  -0.04743687 -0.18237174  0.13701361] reward:  -0.3000163959849702\n",
      "planning time:  0.9097986221313477\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.866\n",
      "action:  [3.2612474  0.02825692 0.12929036 0.05221729] reward:  -0.2981934734084804\n",
      "planning time:  0.892585277557373\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.872\n",
      "action:  [3.1037872  0.01817241 0.14262168 0.03373364] reward:  -0.7068066371738503\n",
      "planning time:  0.8901913166046143\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.856\n",
      "action:  [ 3.1856694  -0.07229047  0.24999775  0.26591745] reward:  -3.8999737386352153\n",
      "planning time:  0.8769853115081787\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.860\n",
      "Crash detected!\n",
      "Crash detected!\n",
      "action:  [ 4.1192317   0.10390285 -0.08348651  0.10251094] reward:  -2.63408718687069\n",
      "planning time:  0.873232364654541\n",
      "resetting environment, and starting trial : 10\n",
      "Connected!\n",
      "4 DoF [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "No propeller. Is it a magic box?\n",
      "setting setpoint to:  0.0 0.0 0.30000000000000004\n",
      "Number of stored transitions:  14533\n",
      "Training model\n",
      "training loss:  -42.385268532357564\n",
      "validation loss:  0.0025045531801879406\n",
      "training time:  95.95433592796326\n",
      "Model trained\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.859\n",
      "action:  [ 3.202087   -0.06461441 -0.04197381  0.13270389] reward:  -0.2999998953983918\n",
      "planning time:  0.8756272792816162\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.791\n",
      "action:  [ 3.2623875  -0.00763845 -0.00408548  0.0534566 ] reward:  -0.30000022355426814\n",
      "planning time:  0.8115692138671875\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.837\n",
      "action:  [ 3.2349255  -0.0146604  -0.0202164   0.06293398] reward:  -0.29834195627102983\n",
      "planning time:  0.8550708293914795\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.778\n",
      "action:  [ 2.250911   -0.13196123  0.08976571  0.17700556] reward:  -2.0701342017101463\n",
      "planning time:  0.8000755310058594\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.795\n",
      "action:  [ 2.677185   -0.08933748  0.04999843  0.09363677] reward:  -1.6491938416019085\n",
      "planning time:  0.8113937377929688\n",
      "planning for trajectory step:  0\n",
      "Planning time: 0.810\n",
      "Crash detected!\n",
      "Crash detected!\n",
      "action:  [ 2.6609602  -0.156039    0.0045753   0.04509356] reward:  -0.300001081806114\n",
      "planning time:  0.8288297653198242\n",
      "resetting environment, and starting trial : 11\n",
      "Connected!\n",
      "4 DoF [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "No propeller. Is it a magic box?\n",
      "setting setpoint to:  0.0 0.0 0.30000000000000004\n",
      "Number of stored transitions:  14539\n",
      "Training model\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 77\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     76\u001b[0m training_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 77\u001b[0m \u001b[43mmodel_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining loss: \u001b[39m\u001b[38;5;124m\"\u001b[39m, train_losses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation loss: \u001b[39m\u001b[38;5;124m\"\u001b[39m, val_scores[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\users\\ksubh\\onedrive\\documents\\swarmslab_rl\\summer 25\\implementations\\mbrl_lib_maintained\\mbrl-lib-experiments\\mbrl\\models\\model_trainer.py:153\u001b[0m, in \u001b[0;36mModelTrainer.train\u001b[1;34m(self, dataset_train, dataset_val, num_epochs, patience, improvement_threshold, callback, batch_callback, evaluate, silent)\u001b[0m\n\u001b[0;32m    151\u001b[0m batch_losses: List[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(dataset_train, disable\u001b[38;5;241m=\u001b[39mdisable_tqdm):\n\u001b[1;32m--> 153\u001b[0m     loss, meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m     batch_losses\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_callback_epoch:\n",
      "File \u001b[1;32mc:\\users\\ksubh\\onedrive\\documents\\swarmslab_rl\\summer 25\\implementations\\mbrl_lib_maintained\\mbrl-lib-experiments\\mbrl\\models\\one_dim_tr_model.py:202\u001b[0m, in \u001b[0;36mOneDTransitionRewardModel.update\u001b[1;34m(self, batch, optimizer, target)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    201\u001b[0m model_in, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_batch(batch)\n\u001b[1;32m--> 202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\ksubh\\onedrive\\documents\\swarmslab_rl\\summer 25\\implementations\\mbrl_lib_maintained\\mbrl-lib-experiments\\mbrl\\models\\model.py:158\u001b[0m, in \u001b[0;36mModel.update\u001b[1;34m(self, model_in, optimizer, target)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    157\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 158\u001b[0m loss, meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m meta \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\users\\ksubh\\onedrive\\documents\\swarmslab_rl\\summer 25\\implementations\\mbrl_lib_maintained\\mbrl-lib-experiments\\mbrl\\models\\gaussian_mlp.py:335\u001b[0m, in \u001b[0;36mGaussianMLP.loss\u001b[1;34m(self, model_in, target)\u001b[0m\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mse_loss(model_in, target), {}\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 335\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nll_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m, {}\n",
      "File \u001b[1;32mc:\\users\\ksubh\\onedrive\\documents\\swarmslab_rl\\summer 25\\implementations\\mbrl_lib_maintained\\mbrl-lib-experiments\\mbrl\\models\\gaussian_mlp.py:296\u001b[0m, in \u001b[0;36mGaussianMLP._nll_loss\u001b[1;34m(self, model_in, target)\u001b[0m\n\u001b[0;32m    294\u001b[0m     model_in \u001b[38;5;241m=\u001b[39m model_in\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    295\u001b[0m     target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 296\u001b[0m pred_mean, pred_logvar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_propagation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_members:\n\u001b[0;32m    298\u001b[0m     target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_members, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\users\\ksubh\\onedrive\\documents\\swarmslab_rl\\summer 25\\implementations\\mbrl_lib_maintained\\mbrl-lib-experiments\\mbrl\\models\\gaussian_mlp.py:281\u001b[0m, in \u001b[0;36mGaussianMLP.forward\u001b[1;34m(self, x, rng, propagation_indices, use_propagation)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_propagation:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_ensemble(\n\u001b[0;32m    279\u001b[0m         x, rng\u001b[38;5;241m=\u001b[39mrng, propagation_indices\u001b[38;5;241m=\u001b[39mpropagation_indices\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_default_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\ksubh\\onedrive\\documents\\swarmslab_rl\\summer 25\\implementations\\mbrl_lib_maintained\\mbrl-lib-experiments\\mbrl\\models\\gaussian_mlp.py:144\u001b[0m, in \u001b[0;36mGaussianMLP._default_forward\u001b[1;34m(self, x, only_elite, **_kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_default_forward\u001b[39m(\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor, only_elite: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs\n\u001b[0;32m    142\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, Optional[torch\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_toggle_layers_use_only_elite(only_elite)\n\u001b[1;32m--> 144\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m     mean_and_logvar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_and_logvar(x)\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_toggle_layers_use_only_elite(only_elite)\n",
      "File \u001b[1;32mc:\\Users\\ksubh\\anaconda3\\envs\\modquad_exp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ksubh\\anaconda3\\envs\\modquad_exp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ksubh\\anaconda3\\envs\\modquad_exp\\lib\\site-packages\\torch\\nn\\modules\\container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ksubh\\anaconda3\\envs\\modquad_exp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ksubh\\anaconda3\\envs\\modquad_exp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ksubh\\anaconda3\\envs\\modquad_exp\\lib\\site-packages\\torch\\nn\\modules\\container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ksubh\\anaconda3\\envs\\modquad_exp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ksubh\\anaconda3\\envs\\modquad_exp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\ksubh\\onedrive\\documents\\swarmslab_rl\\summer 25\\implementations\\mbrl_lib_maintained\\mbrl-lib-experiments\\mbrl\\models\\util.py:61\u001b[0m, in \u001b[0;36mEnsembleLinearLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m xw\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 61\u001b[0m     xw \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_bias:\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m xw \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_scores = []\n",
    "train_time = []\n",
    "plan_time = []\n",
    "\n",
    "def train_callback(_model, _total_calls, _epoch, tr_loss, val_score, _best_val):\n",
    "    train_losses.append(tr_loss)\n",
    "    val_scores.append(val_score.mean().item())   # this returns val score per ensemble model\n",
    "\n",
    "\n",
    "# Create a trainer for the model\n",
    "model_trainer = models.ModelTrainer(dynamics_model, optim_lr=5e-5, weight_decay=5e-5)\n",
    "env.initialize_target_trajectory(traj = \"random trajectory\") \n",
    "model_env.dt = env.dt\n",
    "\n",
    "# Create visualization objects\n",
    "# fig, axs = plt.subplots(1, 2, figsize=(14, 3.75), gridspec_kw={\"width_ratios\": [1, 1]})\n",
    "# ax_text = axs[0].text(300, 50, \"\")\n",
    "    \n",
    "# the states will encapsulate the desired trajectory \\\n",
    "\n",
    "# Main PETS loop\n",
    "all_rewards = [0]\n",
    "trajectory_length, total_time, pos_traj, orient_traj = env.initialize_target_trajectory(traj = \"random trajectory\", position_change_scale=0.5, num_waypoints=10)\n",
    "#     trajectory parameters, default arguments used for random trajectory\n",
    "    # start_pos=[0.0, 0.0, 2.0], start_yaw=0.0, \n",
    "    # start_vel=[0.0, 0.0, 0.0], start_yaw_rate=0.0,\n",
    "    # position_change_scale=1.0, fixed_pos_change_dist=True,\n",
    "    # orientation_change_scale=0.1,\n",
    "    # std_velocity_change=0.0,\n",
    "    # std_angular_velocity_change=0.0,\n",
    "    # std_acceleration_change=0.0,\n",
    "    # std_angular_acceleration_change=0.0,\n",
    "    # num_waypoints=20, \n",
    "    # num_hover_points=3,\n",
    "    # time_step_duration=20,\n",
    "    # num_samples=3):\n",
    "# print(trajectory_length, total_time,\"\\n\", pos_traj[0],\"\\n\", pos_traj[1], \"\\n\", pos_traj[2])\n",
    "\n",
    "model_env.set_desired_trajectory(total_time, pos_traj, orient_traj)\n",
    "# total_time *= 2\n",
    "# logger._write_to_log(\"\\nStarting main experiment loop...\")\n",
    "\n",
    "for trial in range(num_trials):\n",
    "    \n",
    "    print(\"resetting environment, and starting trial :\", trial)\n",
    "    obs, _ = env.reset()  \n",
    "    agent.reset()\n",
    "    \n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    total_reward = 0.0\n",
    "    steps_trial = 0\n",
    "\n",
    "    model_env.trajectory_step = 0\n",
    "    env.trajectory_step = 0\n",
    "    env.update_setpoint(model_env.trajectory_step)\n",
    "    time.sleep(0.5)\n",
    "    # env.pause_simulation()\n",
    "\n",
    "    # --------------- Model Training -----------------\n",
    "\n",
    "    if steps_trial == 0:\n",
    "        print(\"Number of stored transitions: \", replay_buffer.num_stored)\n",
    "        dynamics_model.update_normalizer(replay_buffer.get_all())  # update normalizer stats\n",
    "        \n",
    "        dataset_train, dataset_val = common_util.get_basic_buffer_iterators(\n",
    "            replay_buffer,\n",
    "            batch_size=cfg.overrides.model_batch_size,\n",
    "            val_ratio=cfg.overrides.validation_ratio,\n",
    "            ensemble_size=ensemble_size,\n",
    "            shuffle_each_epoch=True,\n",
    "            bootstrap_permutes=False,  # build bootstrap dataset using sampling with replacement \n",
    "        )\n",
    "        print(\"Training model\")\n",
    "        training_start_time = time.time()\n",
    "        model_trainer.train(\n",
    "            dataset_train, \n",
    "            dataset_val=dataset_val, \n",
    "            num_epochs=50, \n",
    "            patience=50, \n",
    "            callback=train_callback,\n",
    "            silent=True)\n",
    "        print(\"training loss: \", train_losses[-1])\n",
    "        print(\"validation loss: \", val_scores[-1])\n",
    "        print(\"training time: \", time.time() - training_start_time)\n",
    "        print(\"Model trained\")\n",
    "\n",
    "    last_setpoint_set = time.time()\n",
    "\n",
    "\n",
    "    # update_axes(axs, env.render(), ax_text, trial, steps_trial, all_rewards)\n",
    "    while not (terminated or truncated):# or (model_env.trajectory_step >= trajectory_length-1)):\n",
    "\n",
    "        if (time.time() - last_setpoint_set > total_time/trajectory_length) :\n",
    "            # env.resume_simulation()\n",
    "            model_env.trajectory_step += 1\n",
    "            env.trajectory_step += 1\n",
    "            last_setpoint_set = time.time()\n",
    "            print(\"setpoint updated\")\n",
    "            env.update_setpoint(model_env.trajectory_step)\n",
    "            \n",
    "            \n",
    "\n",
    "            # env.pause_simulation()\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        # --- Doing env step using the agent and adding to model dataset ---\n",
    "        print(\"planning for trajectory step: \", model_env.trajectory_step)\n",
    "        planning_start_time = time.time()\n",
    "        model_env.current_pos = env.robot.get_position()\n",
    "        new_obs, reward, terminated, truncated, _ = common_util.step_env_and_add_to_buffer(\n",
    "            env, obs, agent, {}, replay_buffer)\n",
    "        print(\"planning time: \", time.time() - planning_start_time)\n",
    "        \n",
    "            \n",
    "        # update_axes(\n",
    "        #     axs, env.render(), ax_text, trial, steps_trial, all_rewards)\n",
    "        \n",
    "        obs = new_obs\n",
    "        # if truncated:\n",
    "        #     reward = -100\n",
    "        total_reward += reward\n",
    "        steps_trial += 1\n",
    "\n",
    "    env.end_simulation()\n",
    "        \n",
    "    # if steps_trial == trial_length:\n",
    "    #         break\n",
    "    \n",
    "    all_rewards.append(total_reward)\n",
    "\n",
    "# # update_axes(axs, env.render(), ax_text, trial, steps_trial, all_rewards, force_update=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20629280",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "37cddddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, -103.48258039206779, -102.37088259030631, -106.88939530264432, -102.09854677835945, -104.85436910137211, -107.32944103239133, -105.96439655475936, -102.5801105437926, -104.80176441468943, -107.67793198904218, -101.0672106775768, -105.33021814558785, -101.97609834675292, -102.35064774465766, -102.62514999888869, -104.13621741183316, -102.50408697161838, -101.05234418820046, -105.67069207317091, -102.41506569947421]\n"
     ]
    }
   ],
   "source": [
    "print(all_rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modquad_exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
