{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71c18d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Connecting to CoppeliaSim...\n",
      "Connected!\n",
      "4 DoF [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "No propeller. Is it a magic box?\n"
     ]
    }
   ],
   "source": [
    "from IPython import display\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import omegaconf\n",
    "from experiment_logger import (\n",
    "    create_modquad_experiment_logger,\n",
    "    log_modquad_experiment_params,\n",
    "    log_modquad_environment_info,\n",
    "    log_modquad_model_info,\n",
    "    log_modquad_agent_info\n",
    ")\n",
    "\n",
    "import modquad_copp_env as modquad_env\n",
    "import meta_learning_base as mlb\n",
    "# import mbrl.env.reward_fns as reward_fns\n",
    "# import mbrl.env.termination_fns as termination_fns\n",
    "import mbrl.models as models\n",
    "import mbrl.planning as planning\n",
    "import mbrl.util.common as common_util\n",
    "import mbrl.util as util\n",
    "import modquad_utils \n",
    "import time\n",
    "from models import modquad_ModelEnv\n",
    "# import models. as mq_model\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "mpl.rcParams.update({\"font.size\": 16})\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "seed = 0\n",
    "env = modquad_env.ModQuadEnv()\n",
    "rng = np.random.default_rng(seed=0)\n",
    "generator = torch.Generator(device=device)\n",
    "generator.manual_seed(seed)\n",
    "obs_shape = env.observation_space.shape\n",
    "act_shape = env.action_space.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e5a23d",
   "metadata": {},
   "source": [
    "Get initial replay buffer. Either run simulation now or load existing flight data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdde150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to test if gym environment simulation is working\n",
    "import pickle\n",
    "\n",
    "# replay_buffer_sim = env.run_gym_simulation_and_collect_data(cut_at = 300)\n",
    "# with open('540_sim_data.pkl', 'wb') as f:\n",
    "#     pickle.dump(replay_buffer_sim, f)\n",
    "\n",
    "with open('300_del_sim_data.pkl', 'rb') as f:\n",
    "    replay_buffer_sim = pickle.load(f)\n",
    "    \n",
    "# env.end_simulation()\n",
    "# env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b0d96f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15265, 4)\n",
      "Means: [ 3.23949864e+00  5.36027564e-04 -4.84867971e-04  6.32614510e-02]\n",
      "Variances: [6.20798376e-03 1.65049139e-04 5.47030526e-05 7.42734344e-02]\n",
      "Standard Deviations: [0.07879076 0.01284715 0.00739615 0.27253153]\n",
      "Minimums: [-2.97410935 -0.03208319 -0.02861625 -0.37298277]\n",
      "Maximums: [3.32384662 0.04769264 0.02794104 0.41069041]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "replay_buffer_sim\n",
    "all_actions = []\n",
    "for transition in replay_buffer_sim:\n",
    "    obs, action, next_obs, reward, terminate, truncated = transition\n",
    "    all_actions.append(action)\n",
    "\n",
    "all_actions = np.array(all_actions)\n",
    "print(all_actions.shape)\n",
    "\n",
    "means = np.mean(all_actions, axis=0)\n",
    "variances = np.var(all_actions, axis=0)\n",
    "std_devs = np.std(all_actions, axis=0)\n",
    "mins = np.min(all_actions, axis=0)\n",
    "maxs = np.max(all_actions, axis=0)\n",
    "\n",
    "print(\"Means:\", means)\n",
    "print(\"Variances:\", variances)\n",
    "print(\"Standard Deviations:\", std_devs)\n",
    "print(\"Minimums:\", mins)\n",
    "print(\"Maximums:\", maxs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17a2799a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# samples stored 10000\n"
     ]
    }
   ],
   "source": [
    "trial_length = 200\n",
    "num_trials = 20\n",
    "ensemble_size = 5\n",
    "\n",
    "# Everything with \"???\" indicates an option with a missing value.\n",
    "# Our utility functions will fill in these details using the \n",
    "# environment information\n",
    "cfg_dict = {\n",
    "    # dynamics model configuration\n",
    "    \"dynamics_model\": {\n",
    "        \"_target_\": \"mbrl.models.GaussianMLP\",\n",
    "        \"device\": device,\n",
    "        \"num_layers\": 3,\n",
    "        \"ensemble_size\": ensemble_size,\n",
    "        \"hid_size\": 200,\n",
    "        \"in_size\": \"???\",\n",
    "        \"out_size\": \"???\",\n",
    "        \"deterministic\": False,\n",
    "        \"propagation_method\": \"fixed_model\",\n",
    "        # can also configure activation function for GaussianMLP\n",
    "        \"activation_fn_cfg\": {\n",
    "            \"_target_\": \"torch.nn.LeakyReLU\",\n",
    "            \"negative_slope\": 0.01\n",
    "        }\n",
    "    },\n",
    "    # options for training the dynamics model\n",
    "    \"algorithm\": {\n",
    "        \"dataset_size\": 10000,\n",
    "        \"learned_rewards\": False,\n",
    "        \"target_is_delta\": True,\n",
    "        \"normalize\": True,\n",
    "    },\n",
    "    # these are experiment specific options\n",
    "    \"overrides\": {\n",
    "        \"trial_length\": trial_length,\n",
    "        \"num_steps\": num_trials * trial_length,\n",
    "        \"model_batch_size\": 64,\n",
    "        \"validation_ratio\": 0.05\n",
    "    }\n",
    "}\n",
    "cfg = omegaconf.OmegaConf.create(cfg_dict)\n",
    "\n",
    "# Create a 1-D dynamics model for this environment\n",
    "dynamics_model = common_util.create_one_dim_tr_model(cfg, obs_shape, act_shape)\n",
    "\n",
    "# Create a gym-like environment to encapsulate the model\n",
    "model_env = modquad_ModelEnv.modquad_ModelEnv(env, dynamics_model, generator=generator)\n",
    "\n",
    "replay_buffer = common_util.create_replay_buffer(cfg, obs_shape, act_shape, rng=rng)\n",
    "\n",
    "for tr in replay_buffer_sim:\n",
    "    obs, action, next_obs, reward, terminate, truncated = tr\n",
    "    replay_buffer.add(obs, action, next_obs, reward, terminate, truncated)\n",
    "print(\"# samples stored\", replay_buffer.num_stored)\n",
    "\n",
    "agent_cfg = omegaconf.OmegaConf.create({\n",
    "    # this class evaluates many trajectories and picks the best one\n",
    "    \"_target_\": \"mbrl.planning.TrajectoryOptimizerAgent\",\n",
    "    \"planning_horizon\": 15,#was 15 initially\n",
    "    \"replan_freq\": 1,\n",
    "    \"verbose\": False,\n",
    "    \"action_lb\": \"???\",\n",
    "    \"action_ub\": \"???\",\n",
    "    # this is the optimizer to generate and choose a trajectory\n",
    "    \"optimizer_cfg\": {\n",
    "        \"_target_\": \"mbrl.planning.CEMOptimizer\",\n",
    "        \"num_iterations\": 5,\n",
    "        \"elite_ratio\": 0.1,\n",
    "        \"population_size\": 50,\n",
    "        \"alpha\": 0.1,\n",
    "        \"device\": device,\n",
    "        \"lower_bound\": \"???\",\n",
    "        \"upper_bound\": \"???\",\n",
    "        \"return_mean_elites\": True,\n",
    "        \"clipped_normal\": False\n",
    "    }\n",
    "})\n",
    "\n",
    "agent = planning.create_trajectory_optim_agent_for_model(\n",
    "    model_env,\n",
    "    agent_cfg,\n",
    "    num_particles=20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d59496",
   "metadata": {},
   "source": [
    "Main experiment block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ce8faf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected!\n",
      "4 DoF [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "No propeller. Is it a magic box?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 5.5931177e-04,  2.9986379e-01,  9.9368937e-02, -1.6290265e-04,\n",
       "        -1.3953269e-05,  6.5769593e-04, -4.0627341e-08, -1.1241756e-08,\n",
       "        -1.5859010e-08, -5.8620262e-08, -3.7064276e-09,  2.0069589e-07],\n",
       "       dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.end_simulation()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c8c7d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0. 12. 24. 36. 48. 60.]\n",
      "[ 0.    6.25 12.5  18.75 25.  ]\n",
      "resetting environment, and starting trial : 0\n",
      "Connected!\n",
      "4 DoF [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "No propeller. Is it a magic box?\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 1.0150579389358555e-05\n",
      "Number of stored transitions:  10000\n",
      "Training model\n",
      "training loss:  -46.84023914721188\n",
      "validation loss:  2.569879507063888e-05\n",
      "training time:  53.09490656852722\n",
      "Model trained\n",
      "planning for trajectory step:  1\n",
      "action:  [ 0.21749908 -0.01546363  0.02756165 -0.1494132 ] reward:  -0.3159000012351086\n",
      "planning time:  0.26010870933532715\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 7.997055280231069e-05\n",
      "planning for trajectory step:  2\n",
      "action:  [0.27216193 0.02757093 0.03236715 0.2313004 ] reward:  -0.31590075885479907\n",
      "planning time:  0.24824094772338867\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.00026576965766001126\n",
      "planning for trajectory step:  3\n",
      "action:  [0.18656877 0.00741927 0.01164526 0.19582385] reward:  -0.31590263263494905\n",
      "planning time:  0.2618718147277832\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0006202613792457937\n",
      "planning for trajectory step:  4\n",
      "action:  [ 0.18641807 -0.02651722 -0.01978755 -0.23003814] reward:  -0.31590308916292087\n",
      "planning time:  0.2632889747619629\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0011926386612764923\n",
      "planning for trajectory step:  5\n",
      "action:  [0.10838505 0.00053701 0.03053729 0.3310399 ] reward:  -0.31590218130429404\n",
      "planning time:  0.2638986110687256\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.002028649616473763\n",
      "planning for trajectory step:  6\n",
      "action:  [ 0.2518592   0.03184947 -0.01424306 -0.23052618] reward:  -0.3159022558324286\n",
      "planning time:  0.2463686466217041\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.003170673237135412\n",
      "planning for trajectory step:  7\n",
      "action:  [ 0.13339789  0.02165627 -0.03077698  0.08892912] reward:  -0.3159018643611776\n",
      "planning time:  0.25031518936157227\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.004657795105706712\n",
      "planning for trajectory step:  8\n",
      "action:  [ 0.1809023  -0.01041656 -0.02009462  0.26124316] reward:  -0.3159027058042165\n",
      "planning time:  0.24892807006835938\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.006525883105351723\n",
      "planning for trajectory step:  9\n",
      "action:  [ 0.12981375  0.02543521  0.0321204  -0.41175687] reward:  -0.31590087882134066\n",
      "planning time:  0.2624645233154297\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.008807663130524624\n",
      "planning for trajectory step:  10\n",
      "action:  [ 0.15515557 -0.03047057  0.00349547  0.49966684] reward:  -0.3159015990300439\n",
      "planning time:  0.26320838928222656\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.011532794797541032\n",
      "planning for trajectory step:  11\n",
      "action:  [0.13391761 0.02316027 0.02514577 0.4579309 ] reward:  -0.3159014521988721\n",
      "planning time:  0.23025751113891602\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.014727947155149303\n",
      "planning for trajectory step:  12\n",
      "action:  [ 0.09779885  0.03354403  0.04777896 -0.44271058] reward:  -0.31590199264265034\n",
      "planning time:  0.2181687355041504\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.018416874395101913\n",
      "planning for trajectory step:  13\n",
      "action:  [ 0.05258762  0.02799133 -0.01869591 -0.37987196] reward:  -0.315901185433148\n",
      "planning time:  0.2316901683807373\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.022620491562726712\n",
      "planning for trajectory step:  14\n",
      "action:  [ 0.0342472  -0.04349764 -0.04007029 -0.0583217 ] reward:  -0.31589960298377273\n",
      "planning time:  0.2182927131652832\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.027356950267498275\n",
      "planning for trajectory step:  15\n",
      "action:  [ 0.15483236 -0.03513482  0.04634335  0.59315777] reward:  -0.3158994969133564\n",
      "planning time:  0.23583459854125977\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.03264171439360926\n",
      "planning for trajectory step:  16\n",
      "action:  [ 0.20881154 -0.0471118  -0.03902134  0.507987  ] reward:  -0.31590324244313983\n",
      "planning time:  0.21690011024475098\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.03848763581054164\n",
      "planning for trajectory step:  17\n",
      "action:  [ 0.08500765 -0.04068081 -0.02472477 -0.38492113] reward:  -0.3159036408398033\n",
      "planning time:  0.23069024085998535\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.044905030083638116\n",
      "planning for trajectory step:  18\n",
      "action:  [ 0.1661164   0.0467027  -0.04200748  0.33485085] reward:  -0.31590313193284825\n",
      "planning time:  0.21798300743103027\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.05190175218467344\n",
      "planning for trajectory step:  19\n",
      "action:  [ 0.12958089  0.03435618 -0.03707879  0.25574312] reward:  -0.3159022077475405\n",
      "planning time:  0.23000788688659668\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.05948327220242562\n",
      "planning for trajectory step:  20\n",
      "action:  [ 3.8695889   0.04738629 -0.04692468 -0.3742543 ] reward:  -0.3159006157342804\n",
      "planning time:  0.23300647735595703\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0676527510532474\n",
      "planning for trajectory step:  21\n",
      "action:  [ 3.8854146  -0.04312542  0.04726245 -0.26263192] reward:  -0.3159004132690415\n",
      "planning time:  0.2315216064453125\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.07641111619163748\n",
      "planning for trajectory step:  22\n",
      "action:  [ 0.03594797  0.0404945   0.04531861 -0.47271025] reward:  -0.3380252693771046\n",
      "planning time:  0.23234939575195312\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0857571373208119\n",
      "planning for trajectory step:  23\n",
      "action:  [ 0.32748908  0.04629177 -0.01169104  0.5684172 ] reward:  -0.4334094303374454\n",
      "planning time:  0.2309718132019043\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.09568750210327515\n",
      "planning for trajectory step:  24\n",
      "action:  [0.08795154 0.04729345 0.04183637 0.38312194] reward:  -0.3508302369177011\n",
      "planning time:  0.23134660720825195\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.10619689187139202\n",
      "planning for trajectory step:  25\n",
      "action:  [ 0.06166165  0.04104823 -0.04378559 -0.22795619] reward:  -0.30321465342391846\n",
      "planning time:  0.24712800979614258\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.11727805733795829\n",
      "planning for trajectory step:  26\n",
      "action:  [ 0.01532142  0.04708444  0.01993963 -0.37723303] reward:  -0.30353551841773596\n",
      "planning time:  0.23565340042114258\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.12892189430677242\n",
      "planning for trajectory step:  27\n",
      "action:  [ 0.02803602 -0.01413474  0.04849049 -0.4890299 ] reward:  -0.30354029824077866\n",
      "planning time:  0.23082685470581055\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.14111751938320677\n",
      "planning for trajectory step:  28\n",
      "action:  [ 0.06404827 -0.02161522  0.03874444  0.10904655] reward:  -0.3035397986117154\n",
      "planning time:  0.23414897918701172\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.15385234568477898\n",
      "planning for trajectory step:  29\n",
      "action:  [ 0.17469458 -0.04557656 -0.02962832 -0.37603748] reward:  -0.30354182017042647\n",
      "planning time:  0.2462475299835205\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.1671121585517233\n",
      "planning for trajectory step:  30\n",
      "action:  [ 0.24834529 -0.04348285 -0.02280704 -0.4684505 ] reward:  -0.30354386917298365\n",
      "planning time:  0.2337806224822998\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.1808811912575617\n",
      "planning for trajectory step:  31\n",
      "action:  [ 9.5020011e-02  1.5541460e-04 -2.4247099e-02  4.0272218e-01] reward:  -0.30354603283782533\n",
      "planning time:  0.2948575019836426\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.1951422007196756\n",
      "planning for trajectory step:  32\n",
      "action:  [ 0.24384935  0.03471423  0.01759057 -0.28712985] reward:  -0.30354679689749225\n",
      "planning time:  0.2324666976928711\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.2098765432098766\n",
      "planning for trajectory step:  33\n",
      "action:  [ 0.09135584  0.01535713 -0.04101263 -0.49075824] reward:  -0.3035434187333963\n",
      "planning time:  0.23334693908691406\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.22506425006497865\n",
      "planning for trajectory step:  34\n",
      "action:  [ 0.04368757 -0.0252495   0.0242175  -0.13283077] reward:  -0.30354351945132624\n",
      "planning time:  0.23541593551635742\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.24068410339736837\n",
      "planning for trajectory step:  35\n",
      "action:  [ 0.07289869 -0.03308175 -0.0448333   0.59423506] reward:  -0.30354272315743325\n",
      "planning time:  0.2318887710571289\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.25671371180557717\n",
      "planning for trajectory step:  36\n",
      "action:  [ 3.8293889   0.0429459  -0.04388252 -0.14737266] reward:  -0.3035426815722389\n",
      "planning time:  0.23197531700134277\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.2731295860848524\n",
      "planning for trajectory step:  37\n",
      "action:  [ 0.02758375 -0.02854114  0.03702218 -0.27525398] reward:  -0.30420245317361727\n",
      "planning time:  0.2184443473815918\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.2899072149377282\n",
      "planning for trajectory step:  38\n",
      "action:  [ 0.01959933  0.04820387 -0.01980132  0.42037752] reward:  -0.31938078119997293\n",
      "planning time:  0.2652320861816406\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.3070211406845973\n",
      "planning for trajectory step:  39\n",
      "action:  [ 0.08676749  0.04531527 -0.04041943  0.4075045 ] reward:  -0.30421347835750223\n",
      "planning time:  0.21596550941467285\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.3244450349742825\n",
      "planning for trajectory step:  40\n",
      "action:  [ 0.05252811  0.00898896 -0.02397004  0.5529702 ] reward:  -0.30431333592258825\n",
      "planning time:  0.2346043586730957\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.3421517744946073\n",
      "planning for trajectory step:  41\n",
      "action:  [0.3226955  0.03804807 0.04465778 0.51057714] reward:  -0.30431565248644693\n",
      "planning time:  0.21893525123596191\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.360113516682968\n",
      "planning for trajectory step:  42\n",
      "action:  [ 0.32389736  0.03395722  0.04311177 -0.5891847 ] reward:  -0.30431769358427047\n",
      "planning time:  0.2352142333984375\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.37830177543690424\n",
      "planning for trajectory step:  43\n",
      "action:  [0.10855487 0.04457773 0.04096825 0.5517504 ] reward:  -0.3043201640307052\n",
      "planning time:  0.23276233673095703\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.39668749682467125\n",
      "planning for trajectory step:  44\n",
      "action:  [ 0.06029771 -0.01123074  0.04776951 -0.48323348] reward:  -0.30431821872855835\n",
      "planning time:  0.2321770191192627\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.41524113479581026\n",
      "planning for trajectory step:  45\n",
      "action:  [ 0.03522478 -0.02450457  0.04227051 -0.39678955] reward:  -0.3043162035602875\n",
      "planning time:  0.21553349494934082\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.4339327268917206\n",
      "planning for trajectory step:  46\n",
      "action:  [ 0.13712642 -0.04305559  0.03018605 -0.11589184] reward:  -0.304316105156403\n",
      "planning time:  0.21497607231140137\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.45273196995622983\n",
      "planning for trajectory step:  47\n",
      "action:  [ 0.09034911  0.02804882 -0.0405493  -0.58975315] reward:  -0.30431596910556996\n",
      "planning time:  0.2175309658050537\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.4716082958461666\n",
      "planning for trajectory step:  48\n",
      "action:  [ 0.03769695 -0.00427313 -0.04016047 -0.07224432] reward:  -0.30431745289727674\n",
      "planning time:  0.21744179725646973\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.4905309471419318\n",
      "planning for trajectory step:  49\n",
      "action:  [ 0.02772452  0.03447656 -0.02488407  0.5711133 ] reward:  -0.3043164785897957\n",
      "planning time:  0.21914386749267578\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5094690528580682\n",
      "planning for trajectory step:  50\n",
      "action:  [ 0.22058141 -0.04946744 -0.0408579   0.23138456] reward:  -0.30431604210269325\n",
      "planning time:  0.21618413925170898\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5283917041538334\n",
      "planning for trajectory step:  51\n",
      "action:  [ 0.03851605 -0.00188182  0.0447361   0.5768176 ] reward:  -0.3043169527216075\n",
      "planning time:  0.23415017127990723\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5472680300437707\n",
      "planning for trajectory step:  52\n",
      "action:  [ 0.03713022 -0.03886858 -0.03362612 -0.54802424] reward:  -0.30432000886804833\n",
      "planning time:  0.2340986728668213\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5660672731082802\n",
      "planning for trajectory step:  53\n",
      "action:  [ 0.4009714   0.02574009  0.01046545 -0.32872805] reward:  -0.3043161382888969\n",
      "planning time:  0.21609091758728027\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5847588652041902\n",
      "planning for trajectory step:  54\n",
      "action:  [ 0.11180881  0.02202551  0.03967655 -0.50740147] reward:  -0.304319021380697\n",
      "planning time:  0.21595239639282227\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.603312503175329\n",
      "planning for trajectory step:  55\n",
      "action:  [ 0.09932882 -0.04855384  0.04675354  0.10936769] reward:  -0.3043191550085339\n",
      "planning time:  0.2304990291595459\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.621698224563096\n",
      "planning for trajectory step:  56\n",
      "action:  [0.14926115 0.04236646 0.02872747 0.52545536] reward:  -0.30431638343462464\n",
      "planning time:  0.21899032592773438\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.6398864833170326\n",
      "planning for trajectory step:  57\n",
      "action:  [ 0.2325991  -0.03953113  0.03295049 -0.5688753 ] reward:  -0.3043182103151978\n",
      "planning time:  0.2330303192138672\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.657848225505393\n",
      "planning for trajectory step:  58\n",
      "action:  [ 0.02978389 -0.04761425  0.00376002  0.39355183] reward:  -0.3043170758649032\n",
      "planning time:  0.21904611587524414\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.6755549650257178\n",
      "planning for trajectory step:  59\n",
      "action:  [ 0.18295333  0.0364584  -0.02987133  0.55565256] reward:  -0.30431999311566443\n",
      "planning time:  0.2474498748779297\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.6929788593154029\n",
      "planning for trajectory step:  60\n",
      "action:  [ 0.20018649 -0.03554599  0.04551529 -0.50929874] reward:  -0.3043173366383596\n",
      "planning time:  0.2492983341217041\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.7100927850622721\n",
      "planning for trajectory step:  61\n",
      "action:  [0.02895824 0.00076153 0.04289017 0.31446013] reward:  -0.30431735905693746\n",
      "planning time:  0.2186136245727539\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.7268704139151478\n",
      "planning for trajectory step:  62\n",
      "action:  [ 0.03912922  0.03868735  0.01220649 -0.2670781 ] reward:  -0.3043191761786238\n",
      "planning time:  0.23258686065673828\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.7432862881944238\n",
      "planning for trajectory step:  63\n",
      "action:  [ 0.0637781   0.03737198 -0.02531829 -0.44128704] reward:  -0.3043158471294855\n",
      "planning time:  0.2177729606628418\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.7593158966026328\n",
      "planning for trajectory step:  64\n",
      "action:  [ 0.03922177 -0.03837029  0.04235008 -0.5272643 ] reward:  -0.3043148408029818\n",
      "planning time:  0.24708199501037598\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.7749357499350218\n",
      "planning for trajectory step:  65\n",
      "action:  [ 0.20727208 -0.01937398  0.04730752  0.51197547] reward:  -0.3043161367258671\n",
      "planning time:  0.21501493453979492\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.7901234567901237\n",
      "planning for trajectory step:  66\n",
      "action:  [ 0.519483   -0.00567137 -0.01916595  0.52006626] reward:  -0.30431649187221166\n",
      "planning time:  0.23281288146972656\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8048577992803249\n",
      "planning for trajectory step:  67\n",
      "action:  [ 0.03517012  0.04824422 -0.0116402  -0.33895493] reward:  -0.30432209582167885\n",
      "planning time:  0.2191164493560791\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8191188087424394\n",
      "planning for trajectory step:  68\n",
      "action:  [ 0.35658506  0.04984901 -0.00315089 -0.5544356 ] reward:  -0.30432065397603214\n",
      "planning time:  0.22063779830932617\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8328878414482771\n",
      "planning for trajectory step:  69\n",
      "action:  [ 0.05304941 -0.04144271 -0.0243126  -0.3651875 ] reward:  -0.30431540049244143\n",
      "planning time:  0.21722769737243652\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8461476543152218\n",
      "planning for trajectory step:  70\n",
      "action:  [ 0.2088725  -0.04663591 -0.03707314  0.46990985] reward:  -0.30432085645831625\n",
      "planning time:  0.3106257915496826\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8588824806167938\n",
      "planning for trajectory step:  71\n",
      "action:  [ 0.13984296 -0.02614224 -0.03297697  0.44823602] reward:  -0.3043181597846971\n",
      "planning time:  0.23324084281921387\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8710781056932277\n",
      "planning for trajectory step:  72\n",
      "action:  [ 0.18030582 -0.02986475 -0.04342043  0.39537725] reward:  -0.3043198718613885\n",
      "planning time:  0.23281359672546387\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8827219426620414\n",
      "planning for trajectory step:  73\n",
      "action:  [ 0.04310024  0.02359317 -0.03799552 -0.44741675] reward:  -0.3043193138683755\n",
      "planning time:  0.21596932411193848\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8938031081286091\n",
      "planning for trajectory step:  74\n",
      "action:  [ 0.17506556 -0.04659637 -0.03994896 -0.5363612 ] reward:  -0.3043197390834655\n",
      "planning time:  0.2357926368713379\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9043124978967261\n",
      "planning for trajectory step:  75\n",
      "action:  [ 0.08806992 -0.04251126 -0.02358272  0.5661622 ] reward:  -0.3043184410105134\n",
      "planning time:  0.26540040969848633\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9142428626791894\n",
      "planning for trajectory step:  76\n",
      "action:  [ 0.06691937 -0.00519977 -0.02611453 -0.33924595] reward:  -0.30431969900807987\n",
      "planning time:  0.26558709144592285\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9235888838083635\n",
      "planning for trajectory step:  77\n",
      "action:  [ 0.11771403  0.00667707 -0.01199864 -0.52628344] reward:  -0.30431847879247054\n",
      "planning time:  0.2673182487487793\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.932347248946753\n",
      "planning for trajectory step:  78\n",
      "action:  [ 0.10091459 -0.01658131 -0.04912639  0.4164469 ] reward:  -0.304317771647753\n",
      "planning time:  0.2786092758178711\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.940516727797575\n",
      "planning for trajectory step:  79\n",
      "action:  [ 0.09652711  0.0489794  -0.00205498 -0.56478775] reward:  -0.3043182214850814\n",
      "planning time:  0.24918007850646973\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9480982478153277\n",
      "planning for trajectory step:  80\n",
      "action:  [ 0.14284955 -0.03564414  0.00695851 -0.5785765 ] reward:  -0.30431817084059726\n",
      "planning time:  0.2800712585449219\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9550949699163627\n",
      "planning for trajectory step:  81\n",
      "action:  [ 0.10510375 -0.0443367   0.0392583   0.5611758 ] reward:  -0.3043182041156248\n",
      "planning time:  0.2599053382873535\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.961512364189459\n",
      "planning for trajectory step:  82\n",
      "action:  [ 0.12695073 -0.03590661 -0.03404075  0.09201272] reward:  -0.3043196069313266\n",
      "planning time:  0.24559664726257324\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9673582856063918\n",
      "planning for trajectory step:  83\n",
      "action:  [ 0.08748883 -0.03466531 -0.01197945  0.4124015 ] reward:  -0.3043196533853226\n",
      "planning time:  0.29692935943603516\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9726430497325027\n",
      "planning for trajectory step:  84\n",
      "action:  [ 3.9804416  -0.04421251 -0.04387116  0.49658555] reward:  -0.3043193903868484\n",
      "planning time:  0.3703303337097168\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9773795084372736\n",
      "planning for trajectory step:  85\n",
      "action:  [0.02542354 0.03854788 0.04747605 0.5249928 ] reward:  -0.30431889619919333\n",
      "planning time:  0.2640552520751953\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9815831256048986\n",
      "planning for trajectory step:  86\n",
      "action:  [ 0.06488849  0.04899389 -0.03886544 -0.26762125] reward:  -0.343747590701613\n",
      "planning time:  0.23466706275939941\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9852720528448526\n",
      "planning for trajectory step:  87\n",
      "action:  [0.09923728 0.02819117 0.03714015 0.5957836 ] reward:  -0.3059211213624047\n",
      "planning time:  0.24564862251281738\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9884672052024599\n",
      "planning for trajectory step:  88\n",
      "action:  [ 0.0209174   0.03619267  0.03804408 -0.5498744 ] reward:  -0.30718373954865275\n",
      "planning time:  0.5132849216461182\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9911923368694775\n",
      "planning for trajectory step:  89\n",
      "action:  [ 0.0543327  -0.03540645 -0.03145353 -0.5274909 ] reward:  -0.30721996820643593\n",
      "planning time:  0.28153491020202637\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9934741168946499\n",
      "planning for trajectory step:  90\n",
      "action:  [ 0.24710456  0.04656821 -0.03762899 -0.5478822 ] reward:  -0.30721595257850753\n",
      "planning time:  0.2672693729400635\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9953422048942939\n",
      "planning for trajectory step:  91\n",
      "action:  [ 0.12887487  0.01666015 -0.02458053 -0.51132333] reward:  -0.30721959507110336\n",
      "planning time:  0.2627899646759033\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9968293267628683\n",
      "planning for trajectory step:  92\n",
      "action:  [ 0.05189861  0.04772499 -0.04582432  0.5759832 ] reward:  -0.3072199655380842\n",
      "planning time:  0.24867916107177734\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9979713503835264\n",
      "planning for trajectory step:  93\n",
      "action:  [ 0.18134214  0.02960724 -0.03237299 -0.5113828 ] reward:  -0.30721807954201613\n",
      "planning time:  0.20208501815795898\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9988073613387254\n",
      "planning for trajectory step:  94\n",
      "action:  [ 0.12165898  0.04539746 -0.03232609  0.50932056] reward:  -0.3072164551146858\n",
      "planning time:  0.21592402458190918\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9993797386207541\n",
      "planning for trajectory step:  95\n",
      "action:  [ 0.03882809 -0.04584805 -0.03871795 -0.51338655] reward:  -0.3072197125874883\n",
      "planning time:  0.21398138999938965\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9997342303423391\n",
      "planning for trajectory step:  96\n",
      "action:  [ 0.03874239  0.04750917 -0.04245158 -0.20124179] reward:  -0.30721784850510536\n",
      "planning time:  0.2311573028564453\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9999200294471979\n",
      "planning for trajectory step:  97\n",
      "action:  [ 0.05364945  0.04937138 -0.04261984  0.5556345 ] reward:  -0.3072191680246383\n",
      "planning time:  0.2336571216583252\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9999898494206105\n",
      "planning for trajectory step:  98\n",
      "action:  [ 0.37412152  0.01816932  0.04478043 -0.35963717] reward:  -0.30721659438924953\n",
      "planning time:  0.24626398086547852\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 1.0\n",
      "planning for trajectory step:  99\n",
      "action:  [ 0.08750245 -0.0466891  -0.04270753 -0.47663665] reward:  -0.30721914347370316\n",
      "planning time:  0.2319478988647461\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 1.0\n",
      "planning for trajectory step:  100\n",
      "action:  [ 0.45096895 -0.02212428 -0.03119183 -0.5287631 ] reward:  -0.3072226167729799\n",
      "planning time:  0.2651793956756592\n",
      "setpoint updated\n",
      "setting setpoint to:  -2.719333828923398e-07 1.3245362727190578e-06 0.9999982057832423\n",
      "planning for trajectory step:  101\n",
      "action:  [ 0.10987591 -0.00664103 -0.02795023  0.08537284] reward:  -0.3072225916798834\n",
      "planning time:  0.2671067714691162\n",
      "setpoint updated\n",
      "setting setpoint to:  -2.1326973642486238e-06 1.044615653911299e-05 0.9999858488085652\n",
      "planning for trajectory step:  102\n",
      "action:  [ 0.28446594  0.02277048 -0.04987095  0.39095366] reward:  -0.30722279624797927\n",
      "planning time:  0.24824094772338867\n",
      "setpoint updated\n",
      "setting setpoint to:  -7.054821058702212e-06 3.47530711421912e-05 0.9999529179345232\n",
      "planning for trajectory step:  103\n",
      "action:  [ 0.05810102 -0.04597279 -0.00613664 -0.2415106 ] reward:  -0.3072200289555109\n",
      "planning time:  0.21727919578552246\n",
      "setpoint updated\n",
      "setting setpoint to:  -1.6386616180580747e-05 8.119551485977874e-05 0.999889992922992\n",
      "planning for trajectory step:  104\n",
      "action:  [ 1.4283469e-01 -3.8475696e-02 -3.5924010e-04  5.8041155e-01] reward:  -0.3072209834109236\n",
      "planning time:  0.21617865562438965\n",
      "setpoint updated\n",
      "setting setpoint to:  -3.1355099684166555e-05 0.00015629462216361204 0.9997882324930554\n",
      "planning for trajectory step:  105\n",
      "action:  [ 0.14863506  0.03167324 -0.0226861   0.5068759 ] reward:  -0.30721958475182676\n",
      "planning time:  0.21711993217468262\n",
      "setpoint updated\n",
      "setting setpoint to:  -5.3068916403956985e-05 0.0002661513024784345 0.9996393623748905\n",
      "planning for trajectory step:  106\n",
      "action:  [ 0.08325566 -0.03885358 -0.02127699  0.5641462 ] reward:  -0.30721941707550265\n",
      "planning time:  0.21596407890319824\n",
      "setpoint updated\n",
      "setting setpoint to:  -8.252126169492497e-05 0.0004164551154410927 0.9994356633636531\n",
      "planning for trajectory step:  107\n",
      "action:  [ 0.20711651 -0.03103482 -0.03063008 -0.11879677] reward:  -0.30721832471381283\n",
      "planning time:  0.2152271270751953\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.00012059280407277925 0.0006124931461596317 0.9991699593733643\n",
      "planning for trajectory step:  108\n",
      "action:  [ 0.28331378 -0.04253687 -0.01978621  0.2659684 ] reward:  -0.30721988736813727\n",
      "planning time:  0.20281195640563965\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.00016805460785422485 0.0008591588804723906 0.9988356054907952\n",
      "planning for trajectory step:  109\n",
      "action:  [ 0.02720323  0.04260893  0.02421926 -0.24310951] reward:  -0.3072218347450839\n",
      "planning time:  0.19906997680664062\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.00022557105579722354 0.0011609610802070993 0.998426476029354\n",
      "planning for trajectory step:  110\n",
      "action:  [ 0.06492289 -0.04763889  0.02653412 -0.55225134] reward:  -0.30721955399776774\n",
      "planning time:  0.20101261138916016\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.00029370277174125403 0.0015220326584399727 0.9979369525829707\n",
      "planning for trajectory step:  111\n",
      "action:  [ 0.06534179 -0.04270906 -0.04346616  0.42770296] reward:  -0.3072171656491216\n",
      "planning time:  0.18491888046264648\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.00037290954324757257 0.0019461395547548071 0.997361912079983\n",
      "planning for trajectory step:  112\n",
      "action:  [ 0.10150836 -0.04806203 -0.04852205  0.5735663 ] reward:  -0.3072183520731541\n",
      "planning time:  0.20276713371276855\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0004635532442394734 0.0024366896105020783 0.9966967148370226\n",
      "planning for trajectory step:  113\n",
      "action:  [ 0.08150984 -0.03555367 -0.00456548 -0.04727177] reward:  -0.307218763317972\n",
      "planning time:  0.18390202522277832\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0005659007576425493 0.0029967414440580328 0.9959371926129004\n",
      "planning for trajectory step:  114\n",
      "action:  [ 0.15504955 -0.03962727 -0.04455833 -0.21968138] reward:  -0.30721907051803016\n",
      "planning time:  0.22062897682189941\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.000680126898024951 0.003629013326083788 0.9950796366624924\n",
      "planning for trajectory step:  115\n",
      "action:  [ 0.15078408  0.01930816  0.01139586 -0.5595346 ] reward:  -0.30721954609379215\n",
      "planning time:  0.2021031379699707\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0008063173342376489 0.004335892054784423 0.9941207857906258\n",
      "planning for trajectory step:  116\n",
      "action:  [ 0.2266894  -0.03796273  0.03324223  0.5222247 ] reward:  -0.3072195652080582\n",
      "planning time:  0.18630051612854004\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0009444715120546929 0.005119441831168079 0.9930578144059643\n",
      "planning for trajectory step:  117\n",
      "action:  [ 0.2684318  -0.04130252  0.04279559 -0.29722095] reward:  -0.3072187145840277\n",
      "planning time:  0.19891667366027832\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0010945055768134724 0.005981413134305054 0.991888320574894\n",
      "planning for trajectory step:  118\n",
      "action:  [ 0.07993189 -0.04697528 -0.04344603 -0.5565167 ] reward:  -0.30722168850120957\n",
      "planning time:  0.18579459190368652\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0012562552960549774 0.006923251596586897 0.9906103140754094\n",
      "planning for trajectory step:  119\n",
      "action:  [0.08698884 0.02813761 0.02128545 0.55559725] reward:  -0.3072222557326084\n",
      "planning time:  0.21783852577209473\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0014294789821640585 0.007946106878985504 0.9892222044509987\n",
      "planning for trajectory step:  120\n",
      "action:  [ 0.33694187 -0.0358761   0.04881037  0.465904  ] reward:  -0.3072197417552362\n",
      "planning time:  0.21832036972045898\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0016138604150096874 0.009050841546312215 0.9877227890645303\n",
      "planning for trajectory step:  121\n",
      "action:  [ 0.06857712  0.03500353 -0.00269881  0.2305787 ] reward:  -0.3072203612752823\n",
      "planning time:  0.20155024528503418\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0018090117645852173 0.010238039942476912 0.9861112411521374\n",
      "planning for trajectory step:  122\n",
      "action:  [ 0.08802698 -0.01264051  0.01463899 -0.39471108] reward:  -0.30722387643760624\n",
      "planning time:  0.18717265129089355\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.002014476513648644 0.011508017065747105 0.9843870978771051\n",
      "planning for trajectory step:  123\n",
      "action:  [ 0.21485364 -0.04722496  0.00447983 -0.38941717] reward:  -0.30721816450965667\n",
      "planning time:  0.18608498573303223\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0022297323803628626 0.012860827444007033 0.9825502483837555\n",
      "planning for trajectory step:  124\n",
      "action:  [ 0.06847917 -0.00504312 -0.04515062  0.38092893] reward:  -0.3072185426671825\n",
      "planning time:  0.18492436408996582\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0024541942409359355 0.014296274010016777 0.9806009218513326\n",
      "planning for trajectory step:  125\n",
      "action:  [ 0.10613375 -0.02575357  0.02939506  0.10678419] reward:  -0.3072198625809794\n",
      "planning time:  0.18753767013549805\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0026872170522613454 0.015813916976671324 0.9785396755478899\n",
      "planning for trajectory step:  126\n",
      "action:  [ 0.05635349 -0.04542702  0.02242885  0.00824201] reward:  -0.3072185538921967\n",
      "planning time:  0.18596887588500977\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0029280987745582605 0.017413082712259685 0.9763673828841749\n",
      "planning for trajectory step:  127\n",
      "action:  [ 0.2666305  -0.04438553 -0.01011834 -0.41860107] reward:  -0.3072191974769929\n",
      "planning time:  0.18556642532348633\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.003176083294011789 0.019092872615723993 0.974085221467515\n",
      "planning for trajectory step:  128\n",
      "action:  [ 0.07621599 -0.00620033 -0.0400445  -0.38315687] reward:  -0.3072189160587199\n",
      "planning time:  0.18381071090698242\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0034303633454132463 0.02085217199191857 0.9716946611557035\n",
      "planning for trajectory step:  129\n",
      "action:  [ 0.38770562  0.01269368  0.01757666 -0.56483597] reward:  -0.3072225276697032\n",
      "planning time:  0.18781566619873047\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.003690083434800414 0.022689658926869062 0.9691974521108854\n",
      "planning for trajectory step:  130\n",
      "action:  [ 0.08199697  0.00095955 -0.02227384  0.06965049] reward:  -0.3072208455860678\n",
      "planning time:  0.18540239334106445\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.003954342762097794 0.024603813163031497 0.9665956128534426\n",
      "planning for trajectory step:  131\n",
      "action:  [ 0.06758889 -0.04533701  0.04614803 -0.57548594] reward:  -0.3072230826055978\n",
      "planning time:  0.1989741325378418\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.004222198143756878 0.026592924974551426 0.9638914183158809\n",
      "planning for trajectory step:  132\n",
      "action:  [ 0.06741504  0.00153445 -0.04683999 -0.4749062 ] reward:  -0.30721849753321395\n",
      "planning time:  0.2031693458557129\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.004492666935396402 0.028655104042522955 0.961087387896714\n",
      "planning for trajectory step:  133\n",
      "action:  [ 0.02395972 -0.00943038 -0.03827432 -0.54195374] reward:  -0.30721851272291506\n",
      "planning time:  0.18339967727661133\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0047647299544426105 0.030788288330247937 0.9581862735143508\n",
      "planning for trajectory step:  134\n",
      "action:  [ 0.04126406  0.01785153  0.02571747 -0.5133259 ] reward:  -0.3072181841989646\n",
      "planning time:  0.18595647811889648\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.005037334402769509 0.03299025295849493 0.9551910476609806\n",
      "planning for trajectory step:  135\n",
      "action:  [ 0.20355609 -0.02390477 -0.02594161  0.29443017] reward:  -0.3072172443443195\n",
      "planning time:  0.21610116958618164\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.005309396789339135 0.035258619080758426 0.9521048914564584\n",
      "planning for trajectory step:  136\n",
      "action:  [ 0.1586649   0.04232763 -0.04892778  0.5358815 ] reward:  -0.307216729245622\n",
      "planning time:  0.1984388828277588\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.005579805852841816 0.0375908627585179 0.9489311827021917\n",
      "planning for trajectory step:  137\n",
      "action:  [3.9722395e+00 2.9580524e-02 3.0051340e-05 5.4333472e-01] reward:  -0.30722064827952433\n",
      "planning time:  0.2028675079345703\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.005847425484336417 0.03998432383649685 0.945673483935025\n",
      "planning for trajectory step:  138\n",
      "action:  [ 0.09083133 -0.03379242  0.03500652 -0.2550196 ] reward:  -0.30721810744656347\n",
      "planning time:  0.18803882598876953\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0061110976498906225 0.042436214817921994 0.9423355304811266\n",
      "planning for trajectory step:  139\n",
      "action:  [ 0.15216191 -0.02460288  0.01096265 -0.49685562] reward:  -0.31796408020947736\n",
      "planning time:  0.2311875820159912\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.006369645313221185 0.04494362973978226 0.9389212185098738\n",
      "planning for trajectory step:  140\n",
      "action:  [ 0.11181115  0.04702826  0.02001375 -0.5262015 ] reward:  -0.3074319010012638\n",
      "planning time:  0.21758604049682617\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.00662187535833417 0.04750355304808797 0.9354345930877391\n",
      "planning for trajectory step:  141\n",
      "action:  [ 0.18532836  0.0348569  -0.02871297 -0.3764505 ] reward:  -0.3075541063511283\n",
      "planning time:  0.2335526943206787\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.00686658151216526 0.050112868473129915 0.9318798362321757\n",
      "planning for trajectory step:  142\n",
      "action:  [ 0.15048976  0.04400408 -0.04356999  0.4544558 ] reward:  -0.3075577958451403\n",
      "planning time:  0.2188410758972168\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.007102547267219965 0.052768367904738385 0.9282612549655027\n",
      "planning for trajectory step:  143\n",
      "action:  [ 0.0525001   0.02785611 -0.04031397  0.11487957] reward:  -0.3075584111238886\n",
      "planning time:  0.2174522876739502\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.007328548804213908 0.05546676026754236 0.9245832693687921\n",
      "planning for trajectory step:  144\n",
      "action:  [ 0.18187562 -0.02489077  0.00548194 -0.54887724] reward:  -0.3075576565114303\n",
      "planning time:  0.2181224822998047\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.007543357914713099 0.05820468039622852 0.9208504006357532\n",
      "planning for trajectory step:  145\n",
      "action:  [ 0.10016559  0.00881459 -0.04714374 -0.2532538 ] reward:  -0.30755808083963493\n",
      "planning time:  0.21648383140563965\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.007745744923774159 0.060978697910800427 0.9170672591266197\n",
      "planning for trajectory step:  146\n",
      "action:  [ 0.10728101 -0.04750648 -0.03302994 -0.07119552] reward:  -0.30755915089562835\n",
      "planning time:  0.1845262050628662\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.007934481612584605 0.06378532609183754 0.9132385324220342\n",
      "planning for trajectory step:  147\n",
      "action:  [ 0.06467675  0.0069777  -0.03170275  0.5266577 ] reward:  -0.30755887448989744\n",
      "planning time:  0.18508338928222656\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.008108344141103119 0.06662103075575433 0.9093689733769352\n",
      "planning for trajectory step:  148\n",
      "action:  [ 0.01054112 -0.03311445 -0.00208203 -0.41985327] reward:  -0.30755960620536044\n",
      "planning time:  0.18525362014770508\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.008266115970699782 0.0694822391300595 0.9054633881744416\n",
      "planning for trajectory step:  149\n",
      "action:  [ 0.16035722  0.03635949 -0.04255061  0.03770656] reward:  -0.30755717115908016\n",
      "planning time:  0.20235514640808105\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.008406590786796352 0.07236534872861483 0.9015266243797394\n",
      "planning for trajectory step:  150\n",
      "action:  [ 0.04542327  0.00911361 -0.03858469  0.06791621] reward:  -0.3075573338323786\n",
      "planning time:  0.2046506404876709\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.00852857542150653 0.07526673622689452 0.8975635589939669\n",
      "planning for trajectory step:  151\n",
      "action:  [ 0.02506121 -0.03879591  0.0480366   0.42846653] reward:  -0.3075581400603304\n",
      "planning time:  0.18743109703063965\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.008630892776276182 0.07818276633724414 0.8935790865081009\n",
      "planning for trajectory step:  152\n",
      "action:  [ 0.16206396 -0.0017168  -0.04239458 -0.08182552] reward:  -0.30755735339602924\n",
      "planning time:  0.18606066703796387\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.00871238474452367 0.08110980068413977 0.8895781069568426\n",
      "planning for trajectory step:  153\n",
      "action:  [ 0.19390973 -0.04646612 -0.04534105  0.30112314] reward:  -0.3075585806853118\n",
      "planning time:  0.1876993179321289\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.008771915134280028 0.08404420667944704 0.8855655139725019\n",
      "planning for trajectory step:  154\n",
      "action:  [ 0.08530435 -0.03789276 -0.04384496  0.4501278 ] reward:  -0.30756010608827206\n",
      "planning time:  0.18718338012695312\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.00880837259082929 0.08698236639768042 0.8815461828388855\n",
      "planning for trajectory step:  155\n",
      "action:  [ 0.0324634  -0.03416358  0.01597532  0.5425443 ] reward:  -0.3075609141998531\n",
      "planning time:  0.1988658905029297\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.008820673519348714 0.089920685451262 0.8775249585451809\n",
      "planning for trajectory step:  156\n",
      "action:  [ 0.05020193 -0.02917244 -0.04892936 -0.33751822] reward:  -0.30755877275853083\n",
      "planning time:  0.18521547317504883\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.00880776500754908 0.09285560186578093 0.8735066438398426\n",
      "planning for trajectory step:  157\n",
      "action:  [ 0.29763663 -0.0395014  -0.0039119  -0.3018818 ] reward:  -0.30755794666667985\n",
      "planning time:  0.18570160865783691\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.008768627748314856 0.0957835949552522 0.869495987284478\n",
      "planning for trajectory step:  158\n",
      "action:  [0.3311361  0.04355965 0.01371358 0.29135478] reward:  -0.30756037974427786\n",
      "planning time:  0.18804931640625\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.008702278962344621 0.09870119419737591 0.8654976713077331\n",
      "planning for trajectory step:  159\n",
      "action:  [ 0.0178972  -0.02182497 -0.04411686  0.53339386] reward:  -0.3075626480621686\n",
      "planning time:  0.18697023391723633\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.008607775320791173 0.10160498810879656 0.8615163002591786\n",
      "planning for trajectory step:  160\n",
      "action:  [ 0.169852   -0.04281168 -0.04785269  0.40729555] reward:  -0.3075613490467045\n",
      "planning time:  0.18584632873535156\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.008484215867901837 0.10449163312036147 0.8575563884631948\n",
      "planning for trajectory step:  161\n",
      "action:  [ 0.27577627  0.0432478  -0.00634379  0.51945436] reward:  -0.3075587861365877\n",
      "planning time:  0.1838977336883545\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.008330744943658763 0.10735786245238074 0.8536223482728585\n",
      "planning for trajectory step:  162\n",
      "action:  [ 0.46239457 -0.04796958 -0.0115267   0.4446484 ] reward:  -0.30756017054408563\n",
      "planning time:  0.18771791458129883\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.008146555106419165 0.11020049498988589 0.8497184781238276\n",
      "planning for trajectory step:  163\n",
      "action:  [ 0.13508444  0.0241173  -0.03401946  0.04259722] reward:  -0.3075601056468904\n",
      "planning time:  0.18504118919372559\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.007930890055555528 0.11301644415788868 0.8458489505882278\n",
      "planning for trajectory step:  164\n",
      "action:  [ 0.14997607 -0.035997    0.0360987  -0.5561397 ] reward:  -0.3075656200000712\n",
      "planning time:  0.18729019165039062\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.007683047554095979 0.11580272679664082 0.8420178004285379\n",
      "planning for trajectory step:  165\n",
      "action:  [ 0.09446217  0.04482453 -0.0344568   0.47257313] reward:  -0.30755853076600465\n",
      "planning time:  0.18612003326416016\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0074023823513644504 0.1185564720368927 0.8382289126514753\n",
      "planning for trajectory step:  166\n",
      "action:  [ 0.13210711  0.01264283  0.0374805  -0.25151795] reward:  -0.3075581456747235\n",
      "planning time:  0.18658232688903809\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.007088309105620978 0.12127493017515265 0.8344860105618823\n",
      "planning for trajectory step:  167\n",
      "action:  [ 0.2145287  -0.00670133  0.02995298 -0.22645134] reward:  -0.30755666038954027\n",
      "planning time:  0.1837611198425293\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.006740305306701964 0.12395548154894588 0.8307926438166119\n",
      "planning for trajectory step:  168\n",
      "action:  [ 0.1022652   0.04662852 -0.03842676  0.5700166 ] reward:  -0.307557898657548\n",
      "planning time:  0.20148468017578125\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.006357914198660372 0.12659564541207322 0.8271521764784131\n",
      "planning for trajectory step:  169\n",
      "action:  [ 0.1026964   0.04515075 -0.01162541  0.34519067] reward:  -0.3075583606882445\n",
      "planning time:  0.18727374076843262\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.005940747702406192 0.12919308880987154 0.8235677750698168\n",
      "planning for trajectory step:  170\n",
      "action:  [ 0.06873453  0.04395205  0.04630099 -0.3194662 ] reward:  -0.3075565967788326\n",
      "planning time:  0.18570804595947266\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.005488489338346399 0.13174563545447113 0.8200423966270217\n",
      "planning for trajectory step:  171\n",
      "action:  [0.10194489 0.04538065 0.04776485 0.19755524] reward:  -0.30755612537257254\n",
      "planning time:  0.1836860179901123\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.005000897149025414 0.13425127460005584 0.8165787767537802\n",
      "planning for trajectory step:  172\n",
      "action:  [ 0.2539786   0.04910805  0.01296584 -0.45391208] reward:  -0.3075557061251125\n",
      "planning time:  0.1859748363494873\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.004477806621765383 0.13670816991812224 0.8131794176752838\n",
      "planning for trajectory step:  173\n",
      "action:  [ 0.19753243 -0.04979694 -0.03844186 -0.5373489 ] reward:  -0.30755762008472515\n",
      "planning time:  0.1859726905822754\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.003919133611306358 0.13911466837273803 0.8098465762920486\n",
      "planning for trajectory step:  174\n",
      "action:  [ 0.12335069 -0.03464393  0.04667488  0.56427693] reward:  -0.30755996658071894\n",
      "planning time:  0.18434619903564453\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0033248772624464504 0.14146930909580174 0.8065822522338022\n",
      "planning for trajectory step:  175\n",
      "action:  [ 0.24939291 -0.04240056 -0.01846487 -0.55233526] reward:  -0.30756107944247707\n",
      "planning time:  0.18422794342041016\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0026951229326823978 0.14377083226230142 0.8033881759133682\n",
      "planning for trajectory step:  176\n",
      "action:  [0.05469283 0.03308028 0.03616262 0.48098597] reward:  -0.30755953151807613\n",
      "planning time:  0.1867053508758545\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.002030045114849513 0.1460181879655741 0.8002657965805529\n",
      "planning for trajectory step:  177\n",
      "action:  [ 0.15967642  0.02639611  0.04193272 -0.56061864] reward:  -0.3075592170548724\n",
      "planning time:  0.20306754112243652\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0013299103597620987 0.1482105450925648 0.7972162703760303\n",
      "planning for trajectory step:  178\n",
      "action:  [ 0.09881552 -0.03807336 -0.04615873  0.55809814] reward:  -0.3075572032646718\n",
      "planning time:  0.20396852493286133\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0005950801988537369 0.15034730019908515 0.7942404483852283\n",
      "planning for trajectory step:  179\n",
      "action:  [ 0.21943617 -0.03163054 -0.04011119  0.24553841] reward:  -0.3075585444498244\n",
      "planning time:  0.20217466354370117\n",
      "setpoint updated\n",
      "setting setpoint to:  0.00017398593318251887 0.15242808638507335 0.7913388646922147\n",
      "planning for trajectory step:  180\n",
      "action:  [ 0.05835811  0.04986    -0.0478719  -0.09528905] reward:  -0.30755955348079156\n",
      "planning time:  0.18538260459899902\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0009767277757540133 0.15445278216985242 0.7885117244335818\n",
      "planning for trajectory step:  181\n",
      "action:  [ 0.28583804 -0.02528128  0.0015936  -0.47359753] reward:  -0.30755857173304646\n",
      "planning time:  0.20127654075622559\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0018124813197278877 0.15642152036738965 0.7857588918523343\n",
      "planning for trajectory step:  182\n",
      "action:  [ 0.22047171 -0.01252271 -0.02491865 -0.3218321 ] reward:  -0.30755875056466475\n",
      "planning time:  0.20056438446044922\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0026804758847910476 0.15833469696155633 0.7830798783517725\n",
      "planning for trajectory step:  183\n",
      "action:  [ 0.05696208 -0.02338579  0.02202306 -0.5930896 ] reward:  -0.30756125096721265\n",
      "planning time:  0.20311331748962402\n",
      "setpoint updated\n",
      "setting setpoint to:  0.003579831196810046 0.1601929799813852 0.7804738305493801\n",
      "planning for trajectory step:  184\n",
      "action:  [ 0.06056605  0.04818139 -0.02850918  0.14673159] reward:  -0.3075593460835473\n",
      "planning time:  0.18651127815246582\n",
      "setpoint updated\n",
      "setting setpoint to:  0.004509554465190496 0.16199731837633163 0.7779395183307087\n",
      "planning for trajectory step:  185\n",
      "action:  [ 0.08633014  0.02810956 -0.0183148  -0.587249  ] reward:  -0.3075568389750706\n",
      "planning time:  0.1874077320098877\n",
      "setpoint updated\n",
      "setting setpoint to:  0.005468537460236844 0.1637489508915307 0.7754753229032643\n",
      "planning for trajectory step:  186\n",
      "action:  [ 0.65989673  0.04958276 -0.01168189 -0.5311171 ] reward:  -0.3075558875546165\n",
      "planning time:  0.18432140350341797\n",
      "setpoint updated\n",
      "setting setpoint to:  0.006455553590512544 0.16544941494305798 0.773079224850393\n",
      "planning for trajectory step:  187\n",
      "action:  [0.05830764 0.04033946 0.04638106 0.46837652] reward:  -0.3075572113550091\n",
      "planning time:  0.17218565940856934\n",
      "setpoint updated\n",
      "setting setpoint to:  0.007469254980199347 0.16710055549318775 0.770748792185166\n",
      "planning for trajectory step:  188\n",
      "action:  [ 0.07585023 -0.03943267  0.03814911  0.462271  ] reward:  -0.30756600763133213\n",
      "planning time:  0.17159199714660645\n",
      "setpoint updated\n",
      "setting setpoint to:  0.008508169546457 0.1687045339256516 0.7684811684042661\n",
      "planning for trajectory step:  189\n",
      "action:  [ 0.0603576  -0.03350656  0.02068342  0.38014513] reward:  -0.3075570885869831\n",
      "planning time:  0.18521809577941895\n",
      "setpoint updated\n",
      "setting setpoint to:  0.009570698076783346 0.1702638369208993 0.7662730605418734\n",
      "planning for trajectory step:  190\n",
      "action:  [ 0.019725    0.00891701 -0.01528107  0.37389126] reward:  -0.3075587072161825\n",
      "planning time:  0.18758249282836914\n",
      "setpoint updated\n",
      "setting setpoint to:  0.010655111306373977 0.17178128533135556 0.7641207272235517\n",
      "planning for trajectory step:  191\n",
      "action:  [ 0.01414906 -0.01950518  0.04752803  0.05280999] reward:  -0.3075586525674937\n",
      "planning time:  0.20038104057312012\n",
      "setpoint updated\n",
      "setting setpoint to:  0.011759546995481485 0.17326004305668147 0.7620199667201315\n",
      "planning for trajectory step:  192\n",
      "action:  [ 0.06614491 -0.01414962 -0.04734334 -0.46326894] reward:  -0.3075567966314568\n",
      "planning time:  0.20255327224731445\n",
      "setpoint updated\n",
      "setting setpoint to:  0.012882007006776164 0.17470362591903132 0.7599661050016003\n",
      "planning for trajectory step:  193\n",
      "action:  [ 0.00738076  0.0278485  -0.03621125  0.557766  ] reward:  -0.30755732395823465\n",
      "planning time:  0.18599462509155273\n",
      "setpoint updated\n",
      "setting setpoint to:  0.014020354382704547 0.17611591053831432 0.7579539837909826\n",
      "planning for trajectory step:  194\n",
      "action:  [ 0.04745077 -0.04861864 -0.00551247  0.4129985 ] reward:  -0.30755854794048654\n",
      "planning time:  0.18510746955871582\n",
      "setpoint updated\n",
      "setting setpoint to:  0.01517231042285025 0.17750114320745025 0.7559779486182332\n",
      "planning for trajectory step:  195\n",
      "action:  [ 0.064919    0.04905463 -0.04035386 -0.3273988 ] reward:  -0.3075571692155485\n",
      "planning time:  0.18272638320922852\n",
      "setpoint updated\n",
      "setting setpoint to:  0.016335451761293085 0.17886394876763223 0.7540318368741142\n",
      "planning for trajectory step:  196\n",
      "action:  [ 0.03692183 -0.04465802 -0.01017442  0.37315282] reward:  -0.30755846707842693\n",
      "planning time:  0.20085477828979492\n",
      "setpoint updated\n",
      "setting setpoint to:  0.017507207443968914 0.1802093394835833 0.7521089658640882\n",
      "planning for trajectory step:  197\n",
      "action:  [ 3.9460974  -0.02954143 -0.04161854 -0.31074014] reward:  -0.3075574797978474\n",
      "planning time:  0.18629670143127441\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0186848560060294 0.18154272391881665 0.7502021208621991\n",
      "planning for trajectory step:  198\n",
      "action:  [ 0.01519462 -0.03799904 -0.04186234 -0.46689793] reward:  -0.3075582553385958\n",
      "planning time:  0.20361971855163574\n",
      "setpoint updated\n",
      "setting setpoint to:  0.01986552254920193 0.18286991581089396 0.7483035431649618\n",
      "planning for trajectory step:  199\n",
      "action:  [ 3.9787652   0.04449211 -0.04084457 -0.04917537] reward:  -0.32371504741913726\n",
      "planning time:  0.20224571228027344\n",
      "resetting environment, and starting trial : 1\n",
      "Connected!\n",
      "4 DoF [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "No propeller. Is it a magic box?\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 1.0150579389358555e-05\n",
      "Number of stored transitions:  10000\n",
      "Training model\n",
      "training loss:  -45.381180987262084\n",
      "validation loss:  0.00021513932733796537\n",
      "training time:  46.12323307991028\n",
      "Model trained\n",
      "planning for trajectory step:  1\n",
      "action:  [ 2.8247728   0.0180143  -0.03371303 -0.14525698] reward:  -0.31590003914941683\n",
      "planning time:  0.1953897476196289\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 7.997055280231069e-05\n",
      "planning for trajectory step:  2\n",
      "action:  [ 1.6835339  -0.01051044  0.0184473  -0.3049232 ] reward:  -0.315921450805563\n",
      "planning time:  0.20197486877441406\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.00026576965766001126\n",
      "planning for trajectory step:  3\n",
      "action:  [ 3.5452154  0.0294856 -0.0097613  0.3420874] reward:  -0.315942505023992\n",
      "planning time:  0.18813657760620117\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0006202613792457937\n",
      "planning for trajectory step:  4\n",
      "action:  [2.690386   0.03659441 0.00827489 0.39952883] reward:  -0.3162561451514482\n",
      "planning time:  0.20186638832092285\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0011926386612764923\n",
      "planning for trajectory step:  5\n",
      "action:  [ 3.8875754   0.03349376 -0.04116934 -0.36566544] reward:  -0.3241971463295321\n",
      "planning time:  0.2013239860534668\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.002028649616473763\n",
      "planning for trajectory step:  6\n",
      "action:  [ 0.33010164  0.04415444  0.03196529 -0.27988806] reward:  -0.3159350146597712\n",
      "planning time:  0.21842122077941895\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.003170673237135412\n",
      "planning for trajectory step:  7\n",
      "action:  [ 0.30878472  0.03352144 -0.04296303 -0.0636698 ] reward:  -0.30482730871672614\n",
      "planning time:  0.2158370018005371\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.004657795105706712\n",
      "planning for trajectory step:  8\n",
      "action:  [ 0.5554017   0.04036578 -0.04590636 -0.38194272] reward:  -0.26688206795133473\n",
      "planning time:  0.20273780822753906\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.006525883105351723\n",
      "planning for trajectory step:  9\n",
      "action:  [ 0.854619    0.03545213 -0.03838044 -0.3479295 ] reward:  -0.2657056580766497\n",
      "planning time:  0.19848060607910156\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.008807663130524624\n",
      "planning for trajectory step:  10\n",
      "action:  [0.3269648  0.04625571 0.01936857 0.56733346] reward:  -0.2657510552105205\n",
      "planning time:  0.20026373863220215\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.011532794797541032\n",
      "planning for trajectory step:  11\n",
      "action:  [ 0.09133035  0.03736686  0.04350075 -0.36701167] reward:  -0.26575202615039706\n",
      "planning time:  0.19873738288879395\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.014727947155149303\n",
      "planning for trajectory step:  12\n",
      "action:  [ 3.3623722   0.04246672 -0.03507192 -0.3797412 ] reward:  -0.26574368514979524\n",
      "planning time:  0.20368504524230957\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.018416874395101913\n",
      "planning for trajectory step:  13\n",
      "action:  [0.2772758  0.04479203 0.03963994 0.42238343] reward:  -0.26822057809022254\n",
      "planning time:  0.2168581485748291\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.022620491562726712\n",
      "planning for trajectory step:  14\n",
      "action:  [ 0.28470182  0.01561698 -0.04744995  0.27504584] reward:  -0.266355051454854\n",
      "planning time:  0.20311427116394043\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.027356950267498275\n",
      "planning for trajectory step:  15\n",
      "action:  [ 0.27969697  0.04035309 -0.03721128  0.05303945] reward:  -0.2661126642116784\n",
      "planning time:  0.2015836238861084\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.03264171439360926\n",
      "planning for trajectory step:  16\n",
      "action:  [ 3.703256    0.04859586 -0.04721392  0.01779128] reward:  -0.26612459364875574\n",
      "planning time:  0.20075416564941406\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.03848763581054164\n",
      "planning for trajectory step:  17\n",
      "action:  [0.18206885 0.04202804 0.04893603 0.48962635] reward:  -0.2661238876777736\n",
      "planning time:  0.20289373397827148\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.044905030083638116\n",
      "planning for trajectory step:  18\n",
      "action:  [ 0.08893728  0.03267423  0.04817934 -0.5467602 ] reward:  -0.27706792356600957\n",
      "planning time:  0.20206785202026367\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.05190175218467344\n",
      "planning for trajectory step:  19\n",
      "action:  [ 0.03760256 -0.04509793  0.01385468  0.5390989 ] reward:  -0.2669467291150607\n",
      "planning time:  0.20195746421813965\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.05948327220242562\n",
      "planning for trajectory step:  20\n",
      "action:  [ 1.8784709   0.0468422  -0.04832089 -0.4994871 ] reward:  -0.2669671370988788\n",
      "planning time:  0.20139169692993164\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0676527510532474\n",
      "planning for trajectory step:  21\n",
      "action:  [ 0.07366012  0.04764027  0.03673456 -0.58556783] reward:  -0.2669769971640855\n",
      "planning time:  0.20328211784362793\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.07641111619163748\n",
      "planning for trajectory step:  22\n",
      "action:  [ 0.01471974  0.03482552 -0.04483777 -0.5461564 ] reward:  -0.2670151408551274\n",
      "planning time:  0.19912147521972656\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0857571373208119\n",
      "planning for trajectory step:  23\n",
      "action:  [ 0.23124285  0.04904692  0.04488498 -0.45006484] reward:  -0.26697861907963216\n",
      "planning time:  0.2047286033630371\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.09568750210327515\n",
      "planning for trajectory step:  24\n",
      "action:  [ 0.05622591  0.03418091  0.04536459 -0.5657924 ] reward:  -0.26697621499591245\n",
      "planning time:  0.19830918312072754\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.10619689187139202\n",
      "planning for trajectory step:  25\n",
      "action:  [ 3.8050606   0.04927789 -0.04563378  0.49940458] reward:  -0.2669794286858383\n",
      "planning time:  0.19811654090881348\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.11727805733795829\n",
      "planning for trajectory step:  26\n",
      "action:  [0.0729214  0.03544581 0.04764884 0.17735772] reward:  -0.2669763467031679\n",
      "planning time:  0.20157694816589355\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.12892189430677242\n",
      "planning for trajectory step:  27\n",
      "action:  [ 0.02115391  0.0485963  -0.03772411 -0.5024134 ] reward:  -0.2806113303323533\n",
      "planning time:  0.20162630081176758\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.14111751938320677\n",
      "planning for trajectory step:  28\n",
      "action:  [0.10101992 0.04338839 0.04435873 0.3791873 ] reward:  -0.26847801418210915\n",
      "planning time:  0.20394158363342285\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.15385234568477898\n",
      "planning for trajectory step:  29\n",
      "action:  [ 0.22738849  0.01489619 -0.02651346 -0.06998136] reward:  -0.2678091212118886\n",
      "planning time:  0.20128655433654785\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.1671121585517233\n",
      "planning for trajectory step:  30\n",
      "action:  [ 0.5512696   0.04881164 -0.03538256 -0.24908508] reward:  -0.26783255852710425\n",
      "planning time:  0.21726489067077637\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.1808811912575617\n",
      "planning for trajectory step:  31\n",
      "action:  [0.2743523  0.03687681 0.04448229 0.06343981] reward:  -0.2678411998715747\n",
      "planning time:  0.2029407024383545\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.1951422007196756\n",
      "planning for trajectory step:  32\n",
      "action:  [ 0.06081543  0.04876731  0.04603618 -0.41622466] reward:  -0.2678422646236826\n",
      "planning time:  0.20005583763122559\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.2098765432098766\n",
      "planning for trajectory step:  33\n",
      "action:  [ 0.3605242   0.04814135 -0.04878251  0.35800865] reward:  -0.2678370801896022\n",
      "planning time:  0.21460270881652832\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.22506425006497865\n",
      "planning for trajectory step:  34\n",
      "action:  [ 0.06330269  0.04356119 -0.02456044 -0.3411401 ] reward:  -0.26783412737341955\n",
      "planning time:  0.18677663803100586\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.24068410339736837\n",
      "planning for trajectory step:  35\n",
      "action:  [ 0.03804236  0.04728309  0.0296938  -0.13837913] reward:  -0.2678400059804425\n",
      "planning time:  0.21762895584106445\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.25671371180557717\n",
      "planning for trajectory step:  36\n",
      "action:  [ 0.0790005   0.04078927  0.04094053 -0.13042012] reward:  -0.267834374751399\n",
      "planning time:  0.19994497299194336\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.2731295860848524\n",
      "planning for trajectory step:  37\n",
      "action:  [ 3.46745     0.04910122 -0.04606969  0.59677625] reward:  -0.26783379113620626\n",
      "planning time:  0.20178699493408203\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.2899072149377282\n",
      "planning for trajectory step:  38\n",
      "action:  [ 0.02260388  0.04033805  0.04031395 -0.5085858 ] reward:  -0.26783464535039897\n",
      "planning time:  0.20077228546142578\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.3070211406845973\n",
      "planning for trajectory step:  39\n",
      "action:  [ 3.74605     0.04803279 -0.04949082 -0.5840987 ] reward:  -0.2705367146016093\n",
      "planning time:  0.20013165473937988\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.3244450349742825\n",
      "planning for trajectory step:  40\n",
      "action:  [ 0.06285296  0.04337263  0.04554002 -0.2323186 ] reward:  -0.26852147918680896\n",
      "planning time:  0.21571969985961914\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.3421517744946073\n",
      "planning for trajectory step:  41\n",
      "action:  [ 0.2721429   0.04809319 -0.04697072  0.00851936] reward:  -0.2843195910340596\n",
      "planning time:  0.20411348342895508\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.360113516682968\n",
      "planning for trajectory step:  42\n",
      "action:  [ 0.06650107 -0.00874578  0.03326967 -0.43916905] reward:  -0.26776335675996\n",
      "planning time:  0.20046663284301758\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.37830177543690424\n",
      "planning for trajectory step:  43\n",
      "action:  [ 1.4695681   0.04342418 -0.04519933 -0.04480708] reward:  -0.2680200997358034\n",
      "planning time:  0.2027301788330078\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.39668749682467125\n",
      "planning for trajectory step:  44\n",
      "action:  [ 3.2563882   0.04139594 -0.047239    0.38985842] reward:  -0.2680438942542704\n",
      "planning time:  0.18613004684448242\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.41524113479581026\n",
      "planning for trajectory step:  45\n",
      "action:  [ 0.6713844   0.02456577 -0.04204722  0.0670036 ] reward:  -0.2681185302260071\n",
      "planning time:  0.2078385353088379\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.4339327268917206\n",
      "planning for trajectory step:  46\n",
      "action:  [ 0.18977802  0.04786506  0.04720366 -0.03689116] reward:  -0.2684456487458384\n",
      "planning time:  0.19946503639221191\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.45273196995622983\n",
      "planning for trajectory step:  47\n",
      "action:  [ 0.21651828  0.04619702 -0.04783927  0.12351651] reward:  -0.2682937909647832\n",
      "planning time:  0.21906542778015137\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.4716082958461666\n",
      "planning for trajectory step:  48\n",
      "action:  [ 0.05336919  0.04815429  0.04365942 -0.52884597] reward:  -0.26829229149660905\n",
      "planning time:  0.2025589942932129\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.4905309471419318\n",
      "planning for trajectory step:  49\n",
      "action:  [ 2.854495    0.045092   -0.03396893 -0.54737425] reward:  -0.2682941498711824\n",
      "planning time:  0.20143342018127441\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5094690528580682\n",
      "planning for trajectory step:  50\n",
      "action:  [ 0.04455969  0.04391098  0.04503471 -0.43346393] reward:  -0.2682907332424047\n",
      "planning time:  0.18552327156066895\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5283917041538334\n",
      "planning for trajectory step:  51\n",
      "action:  [ 0.04183898  0.03735711  0.04143265 -0.5651806 ] reward:  -0.2683522558184445\n",
      "planning time:  0.21539592742919922\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5472680300437707\n",
      "planning for trajectory step:  52\n",
      "action:  [ 0.3258801   0.04828483  0.04879855 -0.5649835 ] reward:  -0.2682941569065415\n",
      "planning time:  0.18500876426696777\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5660672731082802\n",
      "planning for trajectory step:  53\n",
      "action:  [ 3.5765781   0.04694584 -0.04656575 -0.3045826 ] reward:  -0.2682953742534714\n",
      "planning time:  0.20369982719421387\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5847588652041902\n",
      "planning for trajectory step:  54\n",
      "action:  [ 0.11029905  0.04960875 -0.03721658 -0.4809625 ] reward:  -0.268717380484765\n",
      "planning time:  0.20252370834350586\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.603312503175329\n",
      "planning for trajectory step:  55\n",
      "action:  [ 0.01187813  0.04425155  0.04110214 -0.54334927] reward:  -0.2804946981276448\n",
      "planning time:  0.21724176406860352\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.621698224563096\n",
      "planning for trajectory step:  56\n",
      "action:  [ 0.33947164  0.04226973  0.03946585 -0.4501148 ] reward:  -0.26839088486913854\n",
      "planning time:  0.20023703575134277\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.6398864833170326\n",
      "planning for trajectory step:  57\n",
      "action:  [ 0.25623408  0.0418123  -0.04516926 -0.2495236 ] reward:  -0.26859653863100463\n",
      "planning time:  0.21741271018981934\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.657848225505393\n",
      "planning for trajectory step:  58\n",
      "action:  [ 0.03076364  0.04561726  0.02877126 -0.33289567] reward:  -0.26861013333249983\n",
      "planning time:  0.28467416763305664\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.6755549650257178\n",
      "planning for trajectory step:  59\n",
      "action:  [ 0.38687077  0.0486447  -0.04282482 -0.45304382] reward:  -0.2686100334594808\n",
      "planning time:  0.48102664947509766\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.6929788593154029\n",
      "planning for trajectory step:  60\n",
      "action:  [ 0.51365644  0.0483974  -0.03162314 -0.03081524] reward:  -0.268608219266663\n",
      "planning time:  0.23195242881774902\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.7100927850622721\n",
      "planning for trajectory step:  61\n",
      "action:  [ 0.09672651  0.04807515 -0.04169165  0.43871883] reward:  -0.26861372735173966\n",
      "planning time:  0.23298239707946777\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.7268704139151478\n",
      "planning for trajectory step:  62\n",
      "action:  [ 0.02693576  0.03547984  0.04829181 -0.44235525] reward:  -0.2686151446868365\n",
      "planning time:  0.21927642822265625\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.7432862881944238\n",
      "planning for trajectory step:  63\n",
      "action:  [ 0.12897429  0.04695623 -0.01602969 -0.41801068] reward:  -0.2686062466770348\n",
      "planning time:  0.20356154441833496\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.7593158966026328\n",
      "planning for trajectory step:  64\n",
      "action:  [ 0.03681509  0.04650474  0.03598811 -0.04967006] reward:  -0.2686048464422022\n",
      "planning time:  0.23310542106628418\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.7749357499350218\n",
      "planning for trajectory step:  65\n",
      "action:  [ 3.913179    0.04881825 -0.04418117 -0.3894007 ] reward:  -0.26860603393306803\n",
      "planning time:  0.21863651275634766\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.7901234567901237\n",
      "planning for trajectory step:  66\n",
      "action:  [ 0.06563894  0.04863648  0.0428146  -0.3872131 ] reward:  -0.26860494953899294\n",
      "planning time:  0.23270916938781738\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8048577992803249\n",
      "planning for trajectory step:  67\n",
      "action:  [ 0.06819611 -0.04521774 -0.04405436 -0.19534887] reward:  -0.28974058036446704\n",
      "planning time:  0.23221325874328613\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8191188087424394\n",
      "planning for trajectory step:  68\n",
      "action:  [ 3.9297638   0.04949836 -0.04303689 -0.47466928] reward:  -0.2691412314052033\n",
      "planning time:  0.24599266052246094\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8328878414482771\n",
      "planning for trajectory step:  69\n",
      "action:  [ 0.2578788   0.04314853  0.04315813 -0.49603042] reward:  -0.2694616706241044\n",
      "planning time:  0.2469463348388672\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8461476543152218\n",
      "planning for trajectory step:  70\n",
      "action:  [ 0.20267446  0.03568354  0.04627563 -0.5553225 ] reward:  -0.2986044428570915\n",
      "planning time:  0.23155498504638672\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8588824806167938\n",
      "planning for trajectory step:  71\n",
      "action:  [ 0.0922306   0.0218142   0.04686185 -0.21562278] reward:  -0.2698477738881956\n",
      "planning time:  0.23621892929077148\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8710781056932277\n",
      "planning for trajectory step:  72\n",
      "action:  [ 0.20421408  0.04922195 -0.03515237 -0.5889662 ] reward:  -0.2702890142543509\n",
      "planning time:  0.23215222358703613\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8827219426620414\n",
      "planning for trajectory step:  73\n",
      "action:  [0.12665494 0.0412278  0.0481376  0.0345633 ] reward:  -0.27029451699682666\n",
      "planning time:  0.247206449508667\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8938031081286091\n",
      "planning for trajectory step:  74\n",
      "action:  [ 0.16365165  0.04921942 -0.02889128 -0.522546  ] reward:  -0.27030257951372144\n",
      "planning time:  0.24591875076293945\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9043124978967261\n",
      "planning for trajectory step:  75\n",
      "action:  [ 0.1280905   0.04082736  0.04071698 -0.56197625] reward:  -0.27030217199965434\n",
      "planning time:  0.28154468536376953\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9142428626791894\n",
      "planning for trajectory step:  76\n",
      "action:  [ 0.1094356   0.03843662  0.04401592 -0.53783214] reward:  -0.27030229636163494\n",
      "planning time:  0.23073196411132812\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9235888838083635\n",
      "planning for trajectory step:  77\n",
      "action:  [0.00694452 0.04515339 0.04529033 0.5325812 ] reward:  -0.2703016944578683\n",
      "planning time:  0.24749445915222168\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.932347248946753\n",
      "planning for trajectory step:  78\n",
      "action:  [ 1.2211214   0.0492507  -0.04540525  0.2912081 ] reward:  -0.270301233267866\n",
      "planning time:  0.24701213836669922\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.940516727797575\n",
      "planning for trajectory step:  79\n",
      "action:  [ 0.17693707 -0.01956426 -0.02632526  0.2693542 ] reward:  -0.2702996797307788\n",
      "planning time:  0.24721527099609375\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9480982478153277\n",
      "planning for trajectory step:  80\n",
      "action:  [ 0.24934287  0.048829   -0.02368495 -0.26293117] reward:  -0.27032460258703356\n",
      "planning time:  0.23496675491333008\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9550949699163627\n",
      "planning for trajectory step:  81\n",
      "action:  [ 0.03452307  0.04842944 -0.04706317 -0.45956516] reward:  -0.2703049568918218\n",
      "planning time:  0.23073887825012207\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.961512364189459\n",
      "planning for trajectory step:  82\n",
      "action:  [ 0.01407022  0.04928486  0.03678491 -0.1587143 ] reward:  -0.2703043510539386\n",
      "planning time:  0.2609069347381592\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9673582856063918\n",
      "planning for trajectory step:  83\n",
      "action:  [ 0.11316387  0.03219201 -0.04105257 -0.42478365] reward:  -0.27029987093797614\n",
      "planning time:  0.24583172798156738\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9726430497325027\n",
      "planning for trajectory step:  84\n",
      "action:  [0.09905443 0.04707976 0.02762176 0.5140912 ] reward:  -0.2703005819901945\n",
      "planning time:  0.23096609115600586\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9773795084372736\n",
      "planning for trajectory step:  85\n",
      "action:  [ 0.3911735  -0.03913867 -0.04517427  0.3456791 ] reward:  -0.27030221054526654\n",
      "planning time:  0.23081541061401367\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9815831256048986\n",
      "planning for trajectory step:  86\n",
      "action:  [ 0.07201836  0.02982825 -0.0492594   0.5862835 ] reward:  -0.2703016278601442\n",
      "planning time:  0.20301270484924316\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9852720528448526\n",
      "planning for trajectory step:  87\n",
      "action:  [ 0.03087438  0.04359542  0.0449193  -0.38019982] reward:  -0.2703094857402534\n",
      "planning time:  0.2179253101348877\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9884672052024599\n",
      "planning for trajectory step:  88\n",
      "action:  [ 0.2205305   0.04929755 -0.03970469 -0.5925823 ] reward:  -0.270301274192637\n",
      "planning time:  0.20453381538391113\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9911923368694775\n",
      "planning for trajectory step:  89\n",
      "action:  [ 0.004155    0.02006428  0.03716819 -0.39485762] reward:  -0.27029968702226115\n",
      "planning time:  0.21443557739257812\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9934741168946499\n",
      "planning for trajectory step:  90\n",
      "action:  [0.05968487 0.04921983 0.02651648 0.46853825] reward:  -0.2703021304937522\n",
      "planning time:  0.20284056663513184\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9953422048942939\n",
      "planning for trajectory step:  91\n",
      "action:  [ 3.483758    0.04243139 -0.0441343   0.54444337] reward:  -0.2703002775030969\n",
      "planning time:  0.20285773277282715\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9968293267628683\n",
      "planning for trajectory step:  92\n",
      "action:  [ 3.084716    0.04753788 -0.04739068  0.42633632] reward:  -0.270602161945326\n",
      "planning time:  0.2185804843902588\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9979713503835264\n",
      "planning for trajectory step:  93\n",
      "action:  [ 1.2439598   0.04590502 -0.03894497 -0.26228157] reward:  -0.2779085476658715\n",
      "planning time:  0.26213836669921875\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9988073613387254\n",
      "planning for trajectory step:  94\n",
      "action:  [ 0.3575009   0.02482441 -0.04611362 -0.07254513] reward:  -0.27223683781701635\n",
      "planning time:  0.281388521194458\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9993797386207541\n",
      "planning for trajectory step:  95\n",
      "action:  [ 0.0382819   0.04942752 -0.03382624 -0.41837287] reward:  -0.25610746561973186\n",
      "planning time:  0.22890138626098633\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9997342303423391\n",
      "planning for trajectory step:  96\n",
      "action:  [0.11222894 0.04342425 0.04692802 0.26055932] reward:  -0.2547483642376544\n",
      "planning time:  0.21990203857421875\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9999200294471979\n",
      "planning for trajectory step:  97\n",
      "action:  [ 0.22551411  0.04564211  0.04252023 -0.562263  ] reward:  -0.254756641069751\n",
      "planning time:  0.20320653915405273\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9999898494206105\n",
      "planning for trajectory step:  98\n",
      "action:  [ 0.55275625  0.0485085  -0.04646886  0.5708166 ] reward:  -0.25475724105308284\n",
      "planning time:  0.20197224617004395\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 1.0\n",
      "planning for trajectory step:  99\n",
      "action:  [ 0.3660542   0.03894303  0.03757578 -0.5123299 ] reward:  -0.25475921525567213\n",
      "planning time:  0.21768903732299805\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 1.0\n",
      "planning for trajectory step:  100\n",
      "action:  [ 3.7641253   0.03799057 -0.0481296   0.5375047 ] reward:  -0.2547662405221373\n",
      "planning time:  0.19987845420837402\n",
      "setpoint updated\n",
      "setting setpoint to:  -2.719333828923398e-07 1.3245362727190578e-06 0.9999982057832423\n",
      "planning for trajectory step:  101\n",
      "action:  [ 0.15390892  0.048782    0.04228143 -0.5957879 ] reward:  -0.25545870797099257\n",
      "planning time:  0.20337748527526855\n",
      "setpoint updated\n",
      "setting setpoint to:  -2.1326973642486238e-06 1.044615653911299e-05 0.9999858488085652\n",
      "planning for trajectory step:  102\n",
      "action:  [ 3.8249216   0.04603959 -0.04571063 -0.39958748] reward:  -0.27025909684212923\n",
      "planning time:  0.2039022445678711\n",
      "setpoint updated\n",
      "setting setpoint to:  -7.054821058702212e-06 3.47530711421912e-05 0.9999529179345232\n",
      "planning for trajectory step:  103\n",
      "action:  [ 0.13265972 -0.03701367  0.0367282  -0.5294065 ] reward:  -0.25672876146806684\n",
      "planning time:  0.20214390754699707\n",
      "setpoint updated\n",
      "setting setpoint to:  -1.6386616180580747e-05 8.119551485977874e-05 0.999889992922992\n",
      "planning for trajectory step:  104\n",
      "action:  [ 0.5424576   0.04781789 -0.0353521  -0.5151284 ] reward:  -0.2740977298002282\n",
      "planning time:  0.20438361167907715\n",
      "setpoint updated\n",
      "setting setpoint to:  -3.1355099684166555e-05 0.00015629462216361204 0.9997882324930554\n",
      "planning for trajectory step:  105\n",
      "action:  [ 0.78714526  0.04060022 -0.04849789  0.55290395] reward:  -0.2557142072670149\n",
      "planning time:  0.2149055004119873\n",
      "setpoint updated\n",
      "setting setpoint to:  -5.3068916403956985e-05 0.0002661513024784345 0.9996393623748905\n",
      "planning for trajectory step:  106\n",
      "action:  [ 2.6099052   0.04544482 -0.03545919 -0.5863782 ] reward:  -0.25574576015891815\n",
      "planning time:  0.20312738418579102\n",
      "setpoint updated\n",
      "setting setpoint to:  -8.252126169492497e-05 0.0004164551154410927 0.9994356633636531\n",
      "planning for trajectory step:  107\n",
      "action:  [ 0.13459179 -0.04042748 -0.03822259  0.55300105] reward:  -0.2557739272706276\n",
      "planning time:  0.20109009742736816\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.00012059280407277925 0.0006124931461596317 0.9991699593733643\n",
      "planning for trajectory step:  108\n",
      "action:  [ 3.144716    0.04465917 -0.04871511  0.2763943 ] reward:  -0.25581847945421604\n",
      "planning time:  0.20528817176818848\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.00016805460785422485 0.0008591588804723906 0.9988356054907952\n",
      "planning for trajectory step:  109\n",
      "action:  [ 0.08555441  0.0430332   0.04709746 -0.45519304] reward:  -0.2557658343191336\n",
      "planning time:  0.20298147201538086\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.00022557105579722354 0.0011609610802070993 0.998426476029354\n",
      "planning for trajectory step:  110\n",
      "action:  [0.01268319 0.04583547 0.04799843 0.1622857 ] reward:  -0.25594865315513426\n",
      "planning time:  0.2006819248199463\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.00029370277174125403 0.0015220326584399727 0.9979369525829707\n",
      "planning for trajectory step:  111\n",
      "action:  [ 0.21586007  0.04571204 -0.04493441 -0.19193184] reward:  -0.2557921257167035\n",
      "planning time:  0.21743011474609375\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.00037290954324757257 0.0019461395547548071 0.997361912079983\n",
      "planning for trajectory step:  112\n",
      "action:  [ 0.07561918  0.04665225  0.02783364 -0.37027413] reward:  -0.2557963308696992\n",
      "planning time:  0.2015073299407959\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0004635532442394734 0.0024366896105020783 0.9966967148370226\n",
      "planning for trajectory step:  113\n",
      "action:  [0.03460485 0.04251486 0.04528174 0.00729209] reward:  -0.25579725778502976\n",
      "planning time:  0.20009732246398926\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0005659007576425493 0.0029967414440580328 0.9959371926129004\n",
      "planning for trajectory step:  114\n",
      "action:  [ 0.19387914  0.04830094 -0.04909429 -0.39472735] reward:  -0.25579517818560604\n",
      "planning time:  0.20504188537597656\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.000680126898024951 0.003629013326083788 0.9950796366624924\n",
      "planning for trajectory step:  115\n",
      "action:  [ 0.5422738   0.04968974 -0.01921896 -0.50155365] reward:  -0.25579640133810627\n",
      "planning time:  0.20114850997924805\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0008063173342376489 0.004335892054784423 0.9941207857906258\n",
      "planning for trajectory step:  116\n",
      "action:  [ 0.09992491  0.0491798   0.0277777  -0.49418357] reward:  -0.255801946732763\n",
      "planning time:  0.19902586936950684\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0009444715120546929 0.005119441831168079 0.9930578144059643\n",
      "planning for trajectory step:  117\n",
      "action:  [ 3.3045778   0.04531643 -0.04606492 -0.3780313 ] reward:  -0.25580169900925764\n",
      "planning time:  0.20009183883666992\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0010945055768134724 0.005981413134305054 0.991888320574894\n",
      "planning for trajectory step:  118\n",
      "action:  [ 0.06468531  0.04018221  0.04808785 -0.48848584] reward:  -0.25579643773764643\n",
      "planning time:  0.18297982215881348\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0012562552960549774 0.006923251596586897 0.9906103140754094\n",
      "planning for trajectory step:  119\n",
      "action:  [0.1024956  0.04426006 0.04251602 0.24713662] reward:  -0.2554826476565155\n",
      "planning time:  0.20279407501220703\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0014294789821640585 0.007946106878985504 0.9892222044509987\n",
      "planning for trajectory step:  120\n",
      "action:  [ 3.7964215   0.04747116 -0.04890215 -0.29959643] reward:  -0.2557882511259224\n",
      "planning time:  0.20336008071899414\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0016138604150096874 0.009050841546312215 0.9877227890645303\n",
      "planning for trajectory step:  121\n",
      "action:  [ 0.2491068   0.03535017 -0.04738149  0.56681067] reward:  -0.2699799728588473\n",
      "planning time:  0.21456599235534668\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0018090117645852173 0.010238039942476912 0.9861112411521374\n",
      "planning for trajectory step:  122\n",
      "action:  [ 0.02041071  0.04834794  0.04871984 -0.5149934 ] reward:  -0.27303729873628996\n",
      "planning time:  0.2160050868988037\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.002014476513648644 0.011508017065747105 0.9843870978771051\n",
      "planning for trajectory step:  123\n",
      "action:  [0.16577598 0.04768381 0.04347254 0.25214645] reward:  -0.25636505454796504\n",
      "planning time:  0.22843241691589355\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0022297323803628626 0.012860827444007033 0.9825502483837555\n",
      "planning for trajectory step:  124\n",
      "action:  [ 3.5611246   0.04914108  0.03369993 -0.5307983 ] reward:  -0.2567643523918786\n",
      "planning time:  0.2153301239013672\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0024541942409359355 0.014296274010016777 0.9806009218513326\n",
      "planning for trajectory step:  125\n",
      "action:  [ 3.2805395   0.04674087 -0.04641543  0.2852832 ] reward:  -0.25677399651233657\n",
      "planning time:  0.2180781364440918\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0026872170522613454 0.015813916976671324 0.9785396755478899\n",
      "planning for trajectory step:  126\n",
      "action:  [ 0.02049559  0.04400437  0.03785455 -0.34006286] reward:  -0.26424838445166426\n",
      "planning time:  0.20559096336364746\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0029280987745582605 0.017413082712259685 0.9763673828841749\n",
      "planning for trajectory step:  127\n",
      "action:  [ 0.21084268  0.02960707  0.04389285 -0.4989175 ] reward:  -0.27530120348958176\n",
      "planning time:  0.21713900566101074\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.003176083294011789 0.019092872615723993 0.974085221467515\n",
      "planning for trajectory step:  128\n",
      "action:  [ 0.8824119   0.04892391 -0.0237844  -0.27609995] reward:  -0.2502009404164778\n",
      "planning time:  0.20020747184753418\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0034303633454132463 0.02085217199191857 0.9716946611557035\n",
      "planning for trajectory step:  129\n",
      "action:  [0.04164126 0.04905871 0.04972443 0.3294097 ] reward:  -0.24618243457326086\n",
      "planning time:  0.20100188255310059\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.003690083434800414 0.022689658926869062 0.9691974521108854\n",
      "planning for trajectory step:  130\n",
      "action:  [ 0.12075593  0.03971251  0.04680023 -0.47961256] reward:  -0.24632230295426497\n",
      "planning time:  0.20443296432495117\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.003954342762097794 0.024603813163031497 0.9665956128534426\n",
      "planning for trajectory step:  131\n",
      "action:  [ 0.5443089   0.04711445 -0.0467972  -0.50852334] reward:  -0.24632186442180268\n",
      "planning time:  0.20227527618408203\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.004222198143756878 0.026592924974551426 0.9638914183158809\n",
      "planning for trajectory step:  132\n",
      "action:  [ 0.3614598   0.04532762 -0.0441288  -0.43252462] reward:  -0.246323604625083\n",
      "planning time:  0.2046818733215332\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.004492666935396402 0.028655104042522955 0.961087387896714\n",
      "planning for trajectory step:  133\n",
      "action:  [ 0.10704018  0.04680126 -0.04556108 -0.1824732 ] reward:  -0.24633375477092886\n",
      "planning time:  0.20129704475402832\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0047647299544426105 0.030788288330247937 0.9581862735143508\n",
      "planning for trajectory step:  134\n",
      "action:  [ 0.03963102  0.04965615  0.04622026 -0.46673647] reward:  -0.24633035563610595\n",
      "planning time:  0.20140743255615234\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.005037334402769509 0.03299025295849493 0.9551910476609806\n",
      "planning for trajectory step:  135\n",
      "action:  [ 0.09117092  0.04818787 -0.04405105 -0.47487396] reward:  -0.24632434494451935\n",
      "planning time:  0.21662449836730957\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.005309396789339135 0.035258619080758426 0.9521048914564584\n",
      "planning for trajectory step:  136\n",
      "action:  [0.06133922 0.04414298 0.04447344 0.58082765] reward:  -0.24632280994282316\n",
      "planning time:  0.2018113136291504\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.005579805852841816 0.0375908627585179 0.9489311827021917\n",
      "planning for trajectory step:  137\n",
      "action:  [ 0.2034838   0.04738817  0.03578999 -0.37383723] reward:  -0.24632353937439508\n",
      "planning time:  0.2021956443786621\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.005847425484336417 0.03998432383649685 0.945673483935025\n",
      "planning for trajectory step:  138\n",
      "action:  [ 3.722062    0.04369759 -0.03746269  0.4581678 ] reward:  -0.24632447110498942\n",
      "planning time:  0.2030184268951416\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0061110976498906225 0.042436214817921994 0.9423355304811266\n",
      "planning for trajectory step:  139\n",
      "action:  [ 3.7672012   0.04763858 -0.02862695  0.43439984] reward:  -0.24698696749009322\n",
      "planning time:  0.2034156322479248\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.006369645313221185 0.04494362973978226 0.9389212185098738\n",
      "planning for trajectory step:  140\n",
      "action:  [3.9136126  0.04593892 0.03575544 0.43194354] reward:  -0.2665948381881167\n",
      "planning time:  0.2158675193786621\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.00662187535833417 0.04750355304808797 0.9354345930877391\n",
      "planning for trajectory step:  141\n",
      "action:  [ 0.11592191  0.04368699 -0.0167825  -0.46182626] reward:  -0.32700307761677017\n",
      "planning time:  0.20493006706237793\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.00686658151216526 0.050112868473129915 0.9318798362321757\n",
      "planning for trajectory step:  142\n",
      "action:  [ 0.46646503  0.03985276  0.042703   -0.11306291] reward:  -0.4563677947917485\n",
      "planning time:  0.21479415893554688\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.007102547267219965 0.052768367904738385 0.9282612549655027\n",
      "planning for trajectory step:  143\n",
      "action:  [ 0.2506204   0.03340833  0.03265075 -0.20196867] reward:  -0.4213728157726145\n",
      "planning time:  0.20120525360107422\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.007328548804213908 0.05546676026754236 0.9245832693687921\n",
      "planning for trajectory step:  144\n",
      "action:  [ 3.8909173   0.04695582 -0.04724841  0.4206114 ] reward:  -0.1826331625650233\n",
      "planning time:  0.1990809440612793\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.007543357914713099 0.05820468039622852 0.9208504006357532\n",
      "planning for trajectory step:  145\n",
      "action:  [ 0.24127385  0.04694428  0.04365649 -0.53264356] reward:  -0.18867929659292382\n",
      "planning time:  0.18480968475341797\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.007745744923774159 0.060978697910800427 0.9170672591266197\n",
      "planning for trajectory step:  146\n",
      "action:  [ 3.8372774   0.04765371 -0.01362356  0.13890642] reward:  -0.20975204061437108\n",
      "planning time:  0.198652982711792\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.007934481612584605 0.06378532609183754 0.9132385324220342\n",
      "planning for trajectory step:  147\n",
      "Crash detected!\n",
      "Crash detected!\n",
      "action:  [ 0.4700214   0.04203734  0.04337722 -0.18218012] reward:  -0.17071799063095572\n",
      "planning time:  0.19813752174377441\n",
      "resetting environment, and starting trial : 2\n",
      "Connected!\n",
      "4 DoF [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "No propeller. Is it a magic box?\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 1.0150579389358555e-05\n",
      "Number of stored transitions:  10000\n",
      "Training model\n",
      "training loss:  -35.36424876859524\n",
      "validation loss:  0.0014200531877577305\n",
      "training time:  48.37054491043091\n",
      "Model trained\n",
      "planning for trajectory step:  1\n",
      "action:  [ 1.6413996   0.00686808 -0.00514086 -0.04395751] reward:  -0.315900039152819\n",
      "planning time:  0.19651222229003906\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 7.997055280231069e-05\n",
      "planning for trajectory step:  2\n",
      "action:  [ 2.030185   -0.01025939 -0.00362607  0.07462012] reward:  -0.31591214748476465\n",
      "planning time:  0.18588662147521973\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.00026576965766001126\n",
      "planning for trajectory step:  3\n",
      "action:  [ 1.3608818  -0.02263129  0.01687368  0.26610056] reward:  -0.3159320918261358\n",
      "planning time:  0.1893002986907959\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0006202613792457937\n",
      "planning for trajectory step:  4\n",
      "action:  [ 0.5533759   0.03632217 -0.0032992   0.13139446] reward:  -0.31593732288844534\n",
      "planning time:  0.20158147811889648\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0011926386612764923\n",
      "planning for trajectory step:  5\n",
      "action:  [2.412617   0.02909581 0.02489511 0.20692956] reward:  -0.315925897540512\n",
      "planning time:  0.23126721382141113\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.002028649616473763\n",
      "planning for trajectory step:  6\n",
      "action:  [ 1.2171174  -0.03217309 -0.01251714  0.30962625] reward:  -0.3159084410716714\n",
      "planning time:  0.2016897201538086\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.003170673237135412\n",
      "planning for trajectory step:  7\n",
      "action:  [ 1.6351464  -0.04568868 -0.04173232 -0.2036747 ] reward:  -0.315932719435901\n",
      "planning time:  0.2025151252746582\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.004657795105706712\n",
      "planning for trajectory step:  8\n",
      "action:  [0.11930192 0.02215165 0.0160092  0.37888473] reward:  -0.3159267842113903\n",
      "planning time:  0.18839097023010254\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.006525883105351723\n",
      "planning for trajectory step:  9\n",
      "action:  [ 0.11138889 -0.01028644  0.01839702  0.515315  ] reward:  -0.3159175938115979\n",
      "planning time:  0.18799185752868652\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.008807663130524624\n",
      "planning for trajectory step:  10\n",
      "action:  [ 3.7273264  -0.04698011  0.01841562 -0.4082008 ] reward:  -0.31590200290151144\n",
      "planning time:  0.18632960319519043\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.011532794797541032\n",
      "planning for trajectory step:  11\n",
      "action:  [3.143122   0.04492345 0.04629501 0.2787987 ] reward:  -0.31590170821835634\n",
      "planning time:  0.20273566246032715\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.014727947155149303\n",
      "planning for trajectory step:  12\n",
      "action:  [ 0.6223131   0.02550031 -0.04504854 -0.58292353] reward:  -0.3262793311736294\n",
      "planning time:  0.2022109031677246\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.018416874395101913\n",
      "planning for trajectory step:  13\n",
      "action:  [ 9.7115290e-01 -4.1819967e-02  5.0618005e-04 -5.0655758e-01] reward:  -0.35419769590884764\n",
      "planning time:  0.2021791934967041\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.022620491562726712\n",
      "planning for trajectory step:  14\n",
      "action:  [ 0.20691048 -0.04497609  0.0474926   0.55216837] reward:  -0.3281788656846926\n",
      "planning time:  0.19979453086853027\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.027356950267498275\n",
      "planning for trajectory step:  15\n",
      "action:  [ 2.2416928  -0.02411178  0.04648831  0.49525902] reward:  -0.32915208527733\n",
      "planning time:  0.200639009475708\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.03264171439360926\n",
      "planning for trajectory step:  16\n",
      "action:  [ 3.9574718  -0.04031183 -0.03019062 -0.43431523] reward:  -0.32921428319904494\n",
      "planning time:  0.18769621849060059\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.03848763581054164\n",
      "planning for trajectory step:  17\n",
      "action:  [ 0.32582882 -0.04283845  0.04706693  0.18245105] reward:  -0.32923555794659687\n",
      "planning time:  0.20028114318847656\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.044905030083638116\n",
      "planning for trajectory step:  18\n",
      "action:  [ 3.8015094   0.0373165  -0.0169997   0.34792417] reward:  -0.34461413959965237\n",
      "planning time:  0.18886733055114746\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.05190175218467344\n",
      "planning for trajectory step:  19\n",
      "action:  [ 0.01834157 -0.03197631  0.04016833 -0.56988025] reward:  -0.3282493825231803\n",
      "planning time:  0.20398759841918945\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.05948327220242562\n",
      "planning for trajectory step:  20\n",
      "action:  [ 3.7584665   0.04133504  0.04155102 -0.35526007] reward:  -0.3398410325733953\n",
      "planning time:  0.20254111289978027\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0676527510532474\n",
      "planning for trajectory step:  21\n",
      "action:  [ 3.846561   -0.03943305  0.04009962  0.28534818] reward:  -0.3151769551937668\n",
      "planning time:  0.21663308143615723\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.07641111619163748\n",
      "planning for trajectory step:  22\n",
      "action:  [3.4578576  0.01496077 0.04610978 0.41639957] reward:  -0.39092806093244126\n",
      "planning time:  0.2182762622833252\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0857571373208119\n",
      "planning for trajectory step:  23\n",
      "action:  [ 1.0802197  -0.04220811  0.03047789  0.24775922] reward:  -0.6174577643657916\n",
      "planning time:  0.21869134902954102\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.09568750210327515\n",
      "planning for trajectory step:  24\n",
      "action:  [1.7397254  0.04164048 0.04161246 0.5812482 ] reward:  -0.9695463075617473\n",
      "planning time:  0.21750116348266602\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.10619689187139202\n",
      "planning for trajectory step:  25\n",
      "Crash detected!\n",
      "Crash detected!\n",
      "action:  [ 3.5197544  -0.04007916  0.03634272 -0.55169845] reward:  -1.2749798278255384\n",
      "planning time:  0.21656346321105957\n",
      "resetting environment, and starting trial : 3\n",
      "Connected!\n",
      "4 DoF [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "No propeller. Is it a magic box?\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 1.0150579389358555e-05\n",
      "Number of stored transitions:  10000\n",
      "Training model\n",
      "training loss:  -38.110733211440525\n",
      "validation loss:  0.0019433768466114998\n",
      "training time:  46.447938680648804\n",
      "Model trained\n",
      "planning for trajectory step:  1\n",
      "action:  [ 1.3813347  -0.02144471  0.0047653   0.03780046] reward:  -0.315900039152819\n",
      "planning time:  0.24943232536315918\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 7.997055280231069e-05\n",
      "planning for trajectory step:  2\n",
      "action:  [ 2.0850265  -0.00577816  0.00591592 -0.05408213] reward:  -0.3158995195382539\n",
      "planning time:  0.23149919509887695\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.00026576965766001126\n",
      "planning for trajectory step:  3\n",
      "action:  [ 2.4080746  -0.01765749 -0.01031717  0.49573568] reward:  -0.3207744581034077\n",
      "planning time:  0.2342548370361328\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0006202613792457937\n",
      "planning for trajectory step:  4\n",
      "action:  [2.8020365  0.0423112  0.02261972 0.30008397] reward:  -0.31580182309531013\n",
      "planning time:  0.23070621490478516\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0011926386612764923\n",
      "planning for trajectory step:  5\n",
      "action:  [ 0.8048495  -0.03582809  0.03416326 -0.20086096] reward:  -0.3158931525303381\n",
      "planning time:  0.24434328079223633\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.002028649616473763\n",
      "planning for trajectory step:  6\n",
      "action:  [3.1356235  0.03802242 0.0040341  0.03531442] reward:  -0.3159111819572406\n",
      "planning time:  0.23142409324645996\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.003170673237135412\n",
      "planning for trajectory step:  7\n",
      "action:  [ 3.5946274   0.04025402 -0.01763585 -0.14097184] reward:  -0.31587538944424337\n",
      "planning time:  0.2353987693786621\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.004657795105706712\n",
      "planning for trajectory step:  8\n",
      "action:  [ 1.5284877  -0.02707709 -0.00208342 -0.41669458] reward:  -0.3159165526300122\n",
      "planning time:  0.23094892501831055\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.006525883105351723\n",
      "planning for trajectory step:  9\n",
      "action:  [ 1.0206202   0.02987556 -0.0160648   0.24417672] reward:  -0.3269562664796303\n",
      "planning time:  0.23149490356445312\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.008807663130524624\n",
      "planning for trajectory step:  10\n",
      "action:  [ 1.1895874   0.0245665  -0.03688672 -0.266217  ] reward:  -0.3139549100499981\n",
      "planning time:  0.23286867141723633\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.011532794797541032\n",
      "planning for trajectory step:  11\n",
      "action:  [ 0.6276555   0.03600855 -0.03403436 -0.22149584] reward:  -0.3136191345439887\n",
      "planning time:  0.23310637474060059\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.014727947155149303\n",
      "planning for trajectory step:  12\n",
      "action:  [3.7012327  0.01358608 0.00417865 0.44914183] reward:  -0.31363844140839203\n",
      "planning time:  0.23638319969177246\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.018416874395101913\n",
      "planning for trajectory step:  13\n",
      "action:  [ 3.9467614e+00  3.5012860e-02  1.0653945e-05 -1.5277925e-01] reward:  -0.3141328784360326\n",
      "planning time:  0.24527430534362793\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.022620491562726712\n",
      "planning for trajectory step:  14\n",
      "action:  [ 3.3283858   0.02182883  0.04364816 -0.1576376 ] reward:  -0.33799567214580384\n",
      "planning time:  0.2319655418395996\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.027356950267498275\n",
      "planning for trajectory step:  15\n",
      "action:  [ 1.1619341  -0.04101628 -0.03652768  0.3196187 ] reward:  -0.4138923299261462\n",
      "planning time:  0.22144460678100586\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.03264171439360926\n",
      "planning for trajectory step:  16\n",
      "action:  [ 0.9249683  -0.03088324 -0.0482209  -0.44647753] reward:  -0.5773923781306148\n",
      "planning time:  0.23110318183898926\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.03848763581054164\n",
      "planning for trajectory step:  17\n",
      "action:  [ 3.8736784   0.03601703 -0.03931478  0.15922242] reward:  -0.5608921996694621\n",
      "planning time:  0.21661925315856934\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.044905030083638116\n",
      "planning for trajectory step:  18\n",
      "action:  [ 0.23266293  0.02881024 -0.03621655  0.4378885 ] reward:  -0.2092474420128536\n",
      "planning time:  0.21603989601135254\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.05190175218467344\n",
      "planning for trajectory step:  19\n",
      "action:  [ 3.9600017   0.03287815  0.00576981 -0.5230321 ] reward:  -0.21454754328127307\n",
      "planning time:  0.2322547435760498\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.05948327220242562\n",
      "planning for trajectory step:  20\n",
      "action:  [ 3.9080853   0.02294518 -0.04169624 -0.15172838] reward:  -0.2064739582110102\n",
      "planning time:  0.21733355522155762\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0676527510532474\n",
      "planning for trajectory step:  21\n",
      "action:  [3.4318857  0.04860123 0.03288015 0.5030276 ] reward:  -0.28785207140294405\n",
      "planning time:  0.21735548973083496\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.07641111619163748\n",
      "planning for trajectory step:  22\n",
      "action:  [ 0.6964074  -0.03812493  0.04011964  0.5899506 ] reward:  -0.5454587740457043\n",
      "planning time:  0.2313084602355957\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0857571373208119\n",
      "planning for trajectory step:  23\n",
      "action:  [ 1.5562531  -0.04415673 -0.03795945  0.48670456] reward:  -1.0988261306389555\n",
      "planning time:  0.2509307861328125\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.09568750210327515\n",
      "planning for trajectory step:  24\n",
      "Crash detected!\n",
      "Crash detected!\n",
      "action:  [3.2988136  0.04846273 0.04731471 0.5205714 ] reward:  -1.664409049574372\n",
      "planning time:  0.23235750198364258\n",
      "resetting environment, and starting trial : 4\n",
      "Connected!\n",
      "4 DoF [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "No propeller. Is it a magic box?\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 1.0150579389358555e-05\n",
      "Number of stored transitions:  10000\n",
      "Training model\n",
      "training loss:  -37.145765504581014\n",
      "validation loss:  0.0017359225312247872\n",
      "training time:  51.79347801208496\n",
      "Model trained\n",
      "planning for trajectory step:  1\n",
      "action:  [ 1.0639546   0.01194868 -0.00581934 -0.04869358] reward:  -0.31590003914941683\n",
      "planning time:  0.27123451232910156\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 7.997055280231069e-05\n",
      "planning for trajectory step:  2\n",
      "action:  [ 2.335705    0.01360812 -0.00400098  0.31450874] reward:  -0.3158994943385249\n",
      "planning time:  0.46271276473999023\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.00026576965766001126\n",
      "planning for trajectory step:  3\n",
      "action:  [ 2.5895302  -0.00366191  0.01372718  0.18163785] reward:  -0.3159468587513032\n",
      "planning time:  0.24961233139038086\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0006202613792457937\n",
      "planning for trajectory step:  4\n",
      "action:  [ 2.4645212  -0.00959033  0.01165429 -0.12885909] reward:  -0.3159462668001164\n",
      "planning time:  0.25141286849975586\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0011926386612764923\n",
      "planning for trajectory step:  5\n",
      "action:  [ 2.48059487e+00 -3.75026721e-03  1.91027846e-03 -1.38171185e-02] reward:  -0.3159488603239946\n",
      "planning time:  0.23419618606567383\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.002028649616473763\n",
      "planning for trajectory step:  6\n",
      "action:  [ 0.48705447  0.04400955  0.02631761 -0.51553357] reward:  -0.3159475061604445\n",
      "planning time:  0.24732112884521484\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.003170673237135412\n",
      "planning for trajectory step:  7\n",
      "action:  [3.533627   0.04548126 0.03263409 0.03951392] reward:  -0.3159295889351563\n",
      "planning time:  0.24774765968322754\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.004657795105706712\n",
      "planning for trajectory step:  8\n",
      "action:  [ 2.2712657   0.02011525 -0.00956811  0.06735309] reward:  -0.31621847388816043\n",
      "planning time:  0.24826741218566895\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.006525883105351723\n",
      "planning for trajectory step:  9\n",
      "action:  [0.5685498  0.04695778 0.04579985 0.12556131] reward:  -0.3268452829365832\n",
      "planning time:  0.23501825332641602\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.008807663130524624\n",
      "planning for trajectory step:  10\n",
      "action:  [ 3.6569877   0.02370025  0.03690599 -0.13887402] reward:  -0.31188238218339986\n",
      "planning time:  0.23108434677124023\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.011532794797541032\n",
      "planning for trajectory step:  11\n",
      "action:  [3.1262383  0.02923704 0.03550128 0.3846987 ] reward:  -0.31121473103316544\n",
      "planning time:  0.23352360725402832\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.014727947155149303\n",
      "planning for trajectory step:  12\n",
      "action:  [ 0.21944724 -0.03849422 -0.02391327 -0.01090048] reward:  -0.32501060654273733\n",
      "planning time:  0.23537445068359375\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.018416874395101913\n",
      "planning for trajectory step:  13\n",
      "action:  [ 3.9085705   0.0385838   0.04964032 -0.01849855] reward:  -0.34901245482746945\n",
      "planning time:  0.24785804748535156\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.022620491562726712\n",
      "planning for trajectory step:  14\n",
      "Crash detected!\n",
      "Crash detected!\n",
      "action:  [2.7066023  0.00718558 0.04988264 0.33524218] reward:  -0.40732789764259303\n",
      "planning time:  0.23072290420532227\n",
      "resetting environment, and starting trial : 5\n",
      "Connected!\n",
      "4 DoF [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "No propeller. Is it a magic box?\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 1.0150579389358555e-05\n",
      "Number of stored transitions:  10000\n",
      "Training model\n",
      "training loss:  -38.67656416540978\n",
      "validation loss:  0.0011673806002363563\n",
      "training time:  45.75434994697571\n",
      "Model trained\n",
      "planning for trajectory step:  1\n",
      "action:  [ 2.7826593e+00 -1.3433810e-03  6.2276027e-04 -1.6634277e-01] reward:  -0.31590003914941683\n",
      "planning time:  0.19401860237121582\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 7.997055280231069e-05\n",
      "planning for trajectory step:  2\n",
      "action:  [ 2.5960875   0.00380664 -0.00318073  0.12825805] reward:  -0.31592133151824525\n",
      "planning time:  0.20357680320739746\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.00026576965766001126\n",
      "planning for trajectory step:  3\n",
      "action:  [ 2.8014894e+00  1.6968598e-03  2.0464822e-03 -1.0549865e-01] reward:  -0.3159501070121025\n",
      "planning time:  0.1889798641204834\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0006202613792457937\n",
      "planning for trajectory step:  4\n",
      "action:  [2.7668262  0.00679095 0.02269571 0.22678891] reward:  -0.3159517771670899\n",
      "planning time:  0.19920921325683594\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0011926386612764923\n",
      "planning for trajectory step:  5\n",
      "action:  [ 2.8708334  -0.01050769  0.01735802  0.29994267] reward:  -0.3159540185474662\n",
      "planning time:  0.20478582382202148\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.002028649616473763\n",
      "planning for trajectory step:  6\n",
      "action:  [ 2.588589    0.01287488 -0.03223428 -0.17123902] reward:  -0.31595496039766935\n",
      "planning time:  0.20196223258972168\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.003170673237135412\n",
      "planning for trajectory step:  7\n",
      "action:  [ 1.2915632   0.03949265 -0.04328631 -0.34395984] reward:  -0.31595377497207466\n",
      "planning time:  0.19930076599121094\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.004657795105706712\n",
      "planning for trajectory step:  8\n",
      "action:  [ 3.094133   -0.04700874  0.03170137  0.40666312] reward:  -0.3159383204864936\n",
      "planning time:  0.19994568824768066\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.006525883105351723\n",
      "planning for trajectory step:  9\n",
      "action:  [ 2.6356351e+00  8.6865239e-03 -2.5756527e-03  1.5930928e-01] reward:  -0.3159400486451002\n",
      "planning time:  0.2014143466949463\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.008807663130524624\n",
      "planning for trajectory step:  10\n",
      "action:  [ 2.774542   -0.00307908 -0.01231483  0.03782104] reward:  -0.3159528771673227\n",
      "planning time:  0.20462822914123535\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.011532794797541032\n",
      "planning for trajectory step:  11\n",
      "action:  [ 3.946622   -0.02795666 -0.04300649 -0.4719144 ] reward:  -0.315948286358206\n",
      "planning time:  0.19853973388671875\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.014727947155149303\n",
      "planning for trajectory step:  12\n",
      "action:  [ 2.726027    0.01977287 -0.02276767 -0.1293245 ] reward:  -0.3167118104943268\n",
      "planning time:  0.20291376113891602\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.018416874395101913\n",
      "planning for trajectory step:  13\n",
      "action:  [ 2.7502863   0.0040448  -0.00534215  0.03243331] reward:  -0.34271070372445\n",
      "planning time:  0.20267415046691895\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.022620491562726712\n",
      "planning for trajectory step:  14\n",
      "action:  [ 0.05952957  0.03868584 -0.00784883  0.1842469 ] reward:  -0.3900559239925575\n",
      "planning time:  0.20322346687316895\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.027356950267498275\n",
      "planning for trajectory step:  15\n",
      "action:  [ 2.9839911   0.01937914  0.00976276 -0.5352645 ] reward:  -0.4096424569514533\n",
      "planning time:  0.20375823974609375\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.03264171439360926\n",
      "planning for trajectory step:  16\n",
      "action:  [ 0.29865566  0.0232797  -0.04954052  0.5237691 ] reward:  -0.36256057353894705\n",
      "planning time:  0.24931693077087402\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.03848763581054164\n",
      "planning for trajectory step:  17\n",
      "action:  [ 3.2686126  -0.04367356  0.03149831  0.49936432] reward:  -0.3634212940283155\n",
      "planning time:  0.546410083770752\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.044905030083638116\n",
      "planning for trajectory step:  18\n",
      "action:  [ 3.8235936  -0.03127844 -0.04095238 -0.55910623] reward:  -0.3637498627901056\n",
      "planning time:  0.32248973846435547\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.05190175218467344\n",
      "planning for trajectory step:  19\n",
      "action:  [ 3.7927213e+00 -4.4885788e-02 -3.6050288e-03  5.7518190e-01] reward:  -0.3809891315107188\n",
      "planning time:  0.23297691345214844\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.05948327220242562\n",
      "planning for trajectory step:  20\n",
      "action:  [ 3.9134514   0.02927942 -0.03666765 -0.35499945] reward:  -0.4592682236708081\n",
      "planning time:  0.23344779014587402\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0676527510532474\n",
      "planning for trajectory step:  21\n",
      "action:  [ 0.05481683 -0.03787534  0.03351692 -0.19170561] reward:  -0.7259518451827907\n",
      "planning time:  0.23429298400878906\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.07641111619163748\n",
      "planning for trajectory step:  22\n",
      "action:  [ 3.9836783  0.0392242 -0.0275942 -0.5294361] reward:  -1.068682467880582\n",
      "planning time:  0.2468569278717041\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0857571373208119\n",
      "planning for trajectory step:  23\n",
      "action:  [1.1778059  0.04773746 0.03840493 0.21604772] reward:  -1.388891885195888\n",
      "planning time:  0.25301384925842285\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.09568750210327515\n",
      "planning for trajectory step:  24\n",
      "action:  [ 3.9082797   0.03230762 -0.01098162 -0.3810017 ] reward:  -1.4472703417892954\n",
      "planning time:  0.23429584503173828\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.10619689187139202\n",
      "planning for trajectory step:  25\n",
      "action:  [ 3.5540366  -0.0123703  -0.02825493 -0.561827  ] reward:  -1.433347935383564\n",
      "planning time:  0.24856042861938477\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.11727805733795829\n",
      "planning for trajectory step:  26\n",
      "action:  [ 3.3919435  -0.03459939  0.03162669  0.47709292] reward:  -1.530669394776517\n",
      "planning time:  0.23681902885437012\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.12892189430677242\n",
      "planning for trajectory step:  27\n",
      "Crash detected!\n",
      "Crash detected!\n",
      "action:  [ 3.4666157  -0.02428692  0.03395953 -0.5588841 ] reward:  -1.4861555716863655\n",
      "planning time:  0.21406221389770508\n",
      "resetting environment, and starting trial : 6\n",
      "Connected!\n",
      "4 DoF [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "No propeller. Is it a magic box?\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 1.0150579389358555e-05\n",
      "Number of stored transitions:  10000\n",
      "Training model\n",
      "training loss:  -40.3400182788004\n",
      "validation loss:  0.0028822124004364014\n",
      "training time:  45.3635675907135\n",
      "Model trained\n",
      "planning for trajectory step:  1\n",
      "action:  [2.0585523  0.01307304 0.01396779 0.29460564] reward:  -0.315900039152819\n",
      "planning time:  0.20764684677124023\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 7.997055280231069e-05\n",
      "planning for trajectory step:  2\n",
      "action:  [ 2.289624    0.00303923 -0.00816398  0.00782036] reward:  -0.31591260543376676\n",
      "planning time:  0.20413517951965332\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.00026576965766001126\n",
      "planning for trajectory step:  3\n",
      "action:  [ 2.5707903e+00  2.5542113e-03  5.6570182e-03 -2.9981906e-02] reward:  -0.31593035205055253\n",
      "planning time:  0.20103001594543457\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0006202613792457937\n",
      "planning for trajectory step:  4\n",
      "action:  [2.7256773  0.00915833 0.00416061 0.40814558] reward:  -0.31594250984042777\n",
      "planning time:  0.19843125343322754\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0011926386612764923\n",
      "planning for trajectory step:  5\n",
      "action:  [ 2.929732   -0.0379468  -0.04023726  0.47606012] reward:  -0.3159490513370757\n",
      "planning time:  0.2041928768157959\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.002028649616473763\n",
      "planning for trajectory step:  6\n",
      "action:  [ 1.3486079   0.00418312  0.00697954 -0.01355408] reward:  -0.31595494257804\n",
      "planning time:  0.20249676704406738\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.003170673237135412\n",
      "planning for trajectory step:  7\n",
      "action:  [ 2.558552    0.00373788 -0.00686454  0.07357226] reward:  -0.31595838779206986\n",
      "planning time:  0.19767498970031738\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.004657795105706712\n",
      "planning for trajectory step:  8\n",
      "action:  [ 2.3578773   0.02780536 -0.01094317 -0.11099568] reward:  -0.31592647025513637\n",
      "planning time:  0.2040574550628662\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.006525883105351723\n",
      "planning for trajectory step:  9\n",
      "action:  [ 3.758046   -0.04907092  0.04119911 -0.25984728] reward:  -0.31594731330889075\n",
      "planning time:  0.20189428329467773\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.008807663130524624\n",
      "planning for trajectory step:  10\n",
      "action:  [ 2.3330405   0.02128268 -0.01503592 -0.41676053] reward:  -0.31594436559126093\n",
      "planning time:  0.2028512954711914\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.011532794797541032\n",
      "planning for trajectory step:  11\n",
      "action:  [3.0474584e+00 7.1962094e-03 2.7800978e-03 2.4212804e-01] reward:  -0.3270359342532913\n",
      "planning time:  0.21396422386169434\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.014727947155149303\n",
      "planning for trajectory step:  12\n",
      "action:  [ 2.709711   -0.0431692   0.0388878   0.41517928] reward:  -0.32794661339928927\n",
      "planning time:  0.21521258354187012\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.018416874395101913\n",
      "planning for trajectory step:  13\n",
      "action:  [1.5553169  0.04596781 0.00739502 0.3841999 ] reward:  -0.3284060156058151\n",
      "planning time:  0.20346951484680176\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.022620491562726712\n",
      "planning for trajectory step:  14\n",
      "action:  [3.4957323  0.04380264 0.04060725 0.1707704 ] reward:  -0.3282333420871202\n",
      "planning time:  0.21621084213256836\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.027356950267498275\n",
      "planning for trajectory step:  15\n",
      "action:  [ 3.3620529e+00  3.3518868e-03  3.5233520e-02 -5.4961503e-01] reward:  -0.3282138356275544\n",
      "planning time:  0.21714019775390625\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.03264171439360926\n",
      "planning for trajectory step:  16\n",
      "action:  [3.5190294  0.04221557 0.01685016 0.18571386] reward:  -0.3331071073404697\n",
      "planning time:  0.18644309043884277\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.03848763581054164\n",
      "planning for trajectory step:  17\n",
      "action:  [ 2.6752129   0.04839563  0.04778842 -0.5757432 ] reward:  -0.3283669281251692\n",
      "planning time:  0.20163750648498535\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.044905030083638116\n",
      "planning for trajectory step:  18\n",
      "action:  [ 0.77767795  0.03753011  0.02071683 -0.31761554] reward:  -0.29881732294804075\n",
      "planning time:  0.2050926685333252\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.05190175218467344\n",
      "planning for trajectory step:  19\n",
      "action:  [ 3.4856822  -0.0226125   0.03513113  0.12331761] reward:  -0.2760125931886678\n",
      "planning time:  0.2172396183013916\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.05948327220242562\n",
      "planning for trajectory step:  20\n",
      "action:  [ 1.3954183  -0.02935966  0.04603679  0.28884375] reward:  -0.24744177216582766\n",
      "planning time:  0.2185077667236328\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0676527510532474\n",
      "planning for trajectory step:  21\n",
      "action:  [ 0.5355935  -0.04370465  0.03720641 -0.18028311] reward:  -0.2916856120958232\n",
      "planning time:  0.21596217155456543\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.07641111619163748\n",
      "planning for trajectory step:  22\n",
      "action:  [ 0.9347739  -0.02686448  0.04834208  0.37052828] reward:  -0.333403546302548\n",
      "planning time:  0.2046823501586914\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0857571373208119\n",
      "planning for trajectory step:  23\n",
      "action:  [0.4060601  0.02343263 0.04912066 0.551232  ] reward:  -0.3079603681684057\n",
      "planning time:  0.20276665687561035\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.09568750210327515\n",
      "planning for trajectory step:  24\n",
      "action:  [ 3.3253162   0.03376872  0.03441065 -0.49023616] reward:  -0.30559001994088586\n",
      "planning time:  0.19895362854003906\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.10619689187139202\n",
      "planning for trajectory step:  25\n",
      "action:  [ 0.23062868  0.03001596 -0.02686632  0.35467193] reward:  -0.31329296988643257\n",
      "planning time:  0.20332098007202148\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.11727805733795829\n",
      "planning for trajectory step:  26\n",
      "action:  [ 3.5543256   0.0201308  -0.04398187 -0.5212604 ] reward:  -0.33590017458004917\n",
      "planning time:  0.2026362419128418\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.12892189430677242\n",
      "planning for trajectory step:  27\n",
      "action:  [ 3.5495415   0.04556267  0.01159999 -0.5656277 ] reward:  -0.33481148074768846\n",
      "planning time:  0.20198392868041992\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.14111751938320677\n",
      "planning for trajectory step:  28\n",
      "action:  [ 0.21569124 -0.04469467 -0.00069842  0.0846478 ] reward:  -0.3895628261636183\n",
      "planning time:  0.20437932014465332\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.15385234568477898\n",
      "planning for trajectory step:  29\n",
      "action:  [ 2.669249e+00 -3.768945e-02  7.157320e-04 -2.987558e-01] reward:  -0.5483606378802383\n",
      "planning time:  0.20047521591186523\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.1671121585517233\n",
      "planning for trajectory step:  30\n",
      "action:  [ 3.5084836  -0.04855482  0.03941811  0.4606003 ] reward:  -0.6391336764968945\n",
      "planning time:  0.20253491401672363\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.1808811912575617\n",
      "planning for trajectory step:  31\n",
      "action:  [ 0.03052866 -0.01953821  0.02174728  0.59700185] reward:  -0.6606317602119137\n",
      "planning time:  0.21619462966918945\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.1951422007196756\n",
      "planning for trajectory step:  32\n",
      "action:  [0.13436273 0.01041635 0.03289838 0.3920796 ] reward:  -0.6618593706441591\n",
      "planning time:  0.18576288223266602\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.2098765432098766\n",
      "planning for trajectory step:  33\n",
      "action:  [ 2.7641928  -0.04520268 -0.00624207  0.59158343] reward:  -0.6833656474271191\n",
      "planning time:  0.21489930152893066\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.22506425006497865\n",
      "planning for trajectory step:  34\n",
      "action:  [0.04260723 0.04489031 0.0469862  0.24929234] reward:  -0.6323247628818447\n",
      "planning time:  0.18642520904541016\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.24068410339736837\n",
      "planning for trajectory step:  35\n",
      "action:  [ 3.7388024   0.04911424 -0.02945434  0.5127723 ] reward:  -0.6273180712514237\n",
      "planning time:  0.21593260765075684\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.25671371180557717\n",
      "planning for trajectory step:  36\n",
      "action:  [ 2.2064338  -0.01663143  0.02289695  0.05025562] reward:  -0.5897769301739787\n",
      "planning time:  0.2028522491455078\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.2731295860848524\n",
      "planning for trajectory step:  37\n",
      "action:  [ 0.7073279   0.01263982  0.03912451 -0.40369904] reward:  -0.6399658755249077\n",
      "planning time:  0.21293997764587402\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.2899072149377282\n",
      "planning for trajectory step:  38\n",
      "action:  [ 0.61141574 -0.01633489 -0.02289131 -0.502182  ] reward:  -0.8346481981130559\n",
      "planning time:  0.20487666130065918\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.3070211406845973\n",
      "planning for trajectory step:  39\n",
      "action:  [3.9933705  0.03027006 0.04402504 0.46490118] reward:  -0.98822430903339\n",
      "planning time:  0.21656179428100586\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.3244450349742825\n",
      "planning for trajectory step:  40\n",
      "action:  [0.3809806  0.04391255 0.03107471 0.44200075] reward:  -1.0008478744459357\n",
      "planning time:  0.2137143611907959\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.3421517744946073\n",
      "planning for trajectory step:  41\n",
      "action:  [ 0.10677208 -0.00672343  0.01043429  0.10795257] reward:  -1.0635155989710112\n",
      "planning time:  0.20241808891296387\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.360113516682968\n",
      "planning for trajectory step:  42\n",
      "action:  [ 0.82501936  0.03630901 -0.04650647 -0.5165313 ] reward:  -1.2253161231085785\n",
      "planning time:  0.2011430263519287\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.37830177543690424\n",
      "planning for trajectory step:  43\n",
      "action:  [0.31411046 0.04761996 0.04147464 0.43793422] reward:  -1.1830836372114746\n",
      "planning time:  0.23235559463500977\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.39668749682467125\n",
      "planning for trajectory step:  44\n",
      "action:  [ 3.8412194  -0.04684093 -0.02114986  0.5607142 ] reward:  -1.162703663543589\n",
      "planning time:  0.5725376605987549\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.41524113479581026\n",
      "planning for trajectory step:  45\n",
      "action:  [ 0.18424554 -0.02432235 -0.02947706 -0.48019308] reward:  -1.2047865931203325\n",
      "planning time:  0.2357327938079834\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.4339327268917206\n",
      "planning for trajectory step:  46\n",
      "action:  [3.9721935  0.04709585 0.03459083 0.5363612 ] reward:  -1.2988984026519703\n",
      "planning time:  0.24621939659118652\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.45273196995622983\n",
      "planning for trajectory step:  47\n",
      "action:  [ 0.6002766   0.04464526  0.02905907 -0.3493025 ] reward:  -1.3825124196309433\n",
      "planning time:  0.2495887279510498\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.4716082958461666\n",
      "planning for trajectory step:  48\n",
      "action:  [3.5351653  0.0474702  0.01899682 0.54174584] reward:  -1.3500072401978482\n",
      "planning time:  0.23335576057434082\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.4905309471419318\n",
      "planning for trajectory step:  49\n",
      "action:  [0.14145988 0.04729776 0.0344946  0.45076746] reward:  -1.2820526588122296\n",
      "planning time:  0.23370981216430664\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5094690528580682\n",
      "planning for trajectory step:  50\n",
      "action:  [ 3.838818    0.04747695 -0.04876927  0.44579947] reward:  -1.1452007944196054\n",
      "planning time:  0.24763774871826172\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5283917041538334\n",
      "planning for trajectory step:  51\n",
      "action:  [ 3.4933224   0.00489942 -0.01034899 -0.35516673] reward:  -1.1081133635986995\n",
      "planning time:  0.23308372497558594\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5472680300437707\n",
      "planning for trajectory step:  52\n",
      "action:  [ 3.7261105   0.03444884  0.0353635  -0.20173317] reward:  -1.0842266995251917\n",
      "planning time:  0.24390864372253418\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5660672731082802\n",
      "planning for trajectory step:  53\n",
      "action:  [ 1.831763   -0.03323682  0.04402466 -0.0905593 ] reward:  -0.8764683180354017\n",
      "planning time:  0.2655477523803711\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5847588652041902\n",
      "planning for trajectory step:  54\n",
      "Crash detected!\n",
      "Crash detected!\n",
      "action:  [ 2.6074622  -0.04391353 -0.01867533  0.41043022] reward:  -0.749687560096198\n",
      "planning time:  0.2937619686126709\n",
      "resetting environment, and starting trial : 7\n",
      "Connected!\n",
      "4 DoF [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "No propeller. Is it a magic box?\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 1.0150579389358555e-05\n",
      "Number of stored transitions:  10000\n",
      "Training model\n",
      "training loss:  -39.97680221147985\n",
      "validation loss:  0.008221574127674103\n",
      "training time:  45.43849539756775\n",
      "Model trained\n",
      "planning for trajectory step:  1\n",
      "action:  [2.2243986  0.00416773 0.00340111 0.21070457] reward:  -0.315900039152819\n",
      "planning time:  0.19031143188476562\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 7.997055280231069e-05\n",
      "planning for trajectory step:  2\n",
      "action:  [ 2.8508351e+00  2.7695354e-03 -1.6718211e-02  2.8982475e-02] reward:  -0.31591474442927925\n",
      "planning time:  0.20346975326538086\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.00026576965766001126\n",
      "planning for trajectory step:  3\n",
      "action:  [ 2.4363086e+00 -3.9920881e-03  2.1260852e-02 -2.2788425e-03] reward:  -0.315942836889954\n",
      "planning time:  0.20484447479248047\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0006202613792457937\n",
      "planning for trajectory step:  4\n",
      "action:  [ 3.2069051e+00  2.6358603e-03 -1.0351333e-03 -2.8080657e-01] reward:  -0.315954694887688\n",
      "planning time:  0.18528389930725098\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0011926386612764923\n",
      "planning for trajectory step:  5\n",
      "action:  [2.6856744  0.00727632 0.01971339 0.1910211 ] reward:  -0.31594727354742763\n",
      "planning time:  0.20122838020324707\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.002028649616473763\n",
      "planning for trajectory step:  6\n",
      "action:  [ 0.83341897  0.0060529  -0.01630029  0.00633032] reward:  -0.3159480300837238\n",
      "planning time:  0.20166683197021484\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.003170673237135412\n",
      "planning for trajectory step:  7\n",
      "action:  [ 2.9100237e+00  2.1038141e-02 -2.7283665e-03 -4.3439880e-01] reward:  -0.3159345308363516\n",
      "planning time:  0.20299601554870605\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.004657795105706712\n",
      "planning for trajectory step:  8\n",
      "action:  [2.6775198  0.02921092 0.02195428 0.07445129] reward:  -0.3159310927842283\n",
      "planning time:  0.20128703117370605\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.006525883105351723\n",
      "planning for trajectory step:  9\n",
      "action:  [ 3.1371803   0.01698383  0.007316   -0.3703902 ] reward:  -0.31595373150173545\n",
      "planning time:  0.2022533416748047\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.008807663130524624\n",
      "planning for trajectory step:  10\n",
      "action:  [ 3.2595563  -0.04417879  0.02356991 -0.14235973] reward:  -0.3159504087921955\n",
      "planning time:  0.21689343452453613\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.011532794797541032\n",
      "planning for trajectory step:  11\n",
      "action:  [ 0.95840883  0.04639769 -0.04665383  0.52187854] reward:  -0.3159620697113524\n",
      "planning time:  0.20221495628356934\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.014727947155149303\n",
      "planning for trajectory step:  12\n",
      "action:  [ 3.3279984e+00 -1.3258386e-03  7.1940897e-03 -1.7922236e-01] reward:  -0.3171546817260789\n",
      "planning time:  0.20083856582641602\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.018416874395101913\n",
      "planning for trajectory step:  13\n",
      "action:  [ 2.0003557  -0.01357147 -0.01302344  0.5070115 ] reward:  -0.31608204317135574\n",
      "planning time:  0.2030472755432129\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.022620491562726712\n",
      "planning for trajectory step:  14\n",
      "action:  [ 0.03434711 -0.03145964  0.04201045  0.5519481 ] reward:  -0.31662869703936247\n",
      "planning time:  0.19970011711120605\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.027356950267498275\n",
      "planning for trajectory step:  15\n",
      "action:  [2.3474078  0.04662139 0.01256351 0.5425586 ] reward:  -0.316225990779952\n",
      "planning time:  0.20543813705444336\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.03264171439360926\n",
      "planning for trajectory step:  16\n",
      "action:  [ 3.2218549e+00 -2.2769142e-02  3.2365617e-05  3.4695050e-01] reward:  -0.3163201573620259\n",
      "planning time:  0.19941377639770508\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.03848763581054164\n",
      "planning for trajectory step:  17\n",
      "action:  [ 2.4636722  -0.00801851  0.0395107   0.39935917] reward:  -0.31637221660924514\n",
      "planning time:  0.20491790771484375\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.044905030083638116\n",
      "planning for trajectory step:  18\n",
      "action:  [ 2.6458101   0.04220596 -0.0496302   0.52865803] reward:  -0.3164026255150622\n",
      "planning time:  0.20338153839111328\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.05190175218467344\n",
      "planning for trajectory step:  19\n",
      "action:  [ 0.5656705  -0.0023977   0.03732888 -0.5807556 ] reward:  -0.31633726901165754\n",
      "planning time:  0.18921732902526855\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.05948327220242562\n",
      "planning for trajectory step:  20\n",
      "action:  [ 0.23358218 -0.04734501 -0.00969207 -0.3745982 ] reward:  -0.31632693009560714\n",
      "planning time:  0.20430421829223633\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0676527510532474\n",
      "planning for trajectory step:  21\n",
      "action:  [ 0.6695125  -0.04609201  0.0483985   0.16112295] reward:  -0.3163034238119233\n",
      "planning time:  0.20100665092468262\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.07641111619163748\n",
      "planning for trajectory step:  22\n",
      "action:  [ 3.7390225   0.00988935  0.02147561 -0.5382511 ] reward:  -0.3163034372354627\n",
      "planning time:  0.20085406303405762\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0857571373208119\n",
      "planning for trajectory step:  23\n",
      "action:  [ 3.8204322  -0.04147805  0.03722098  0.10340465] reward:  -0.31684267753784245\n",
      "planning time:  0.21716809272766113\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.09568750210327515\n",
      "planning for trajectory step:  24\n",
      "action:  [ 2.8773408  -0.0267484  -0.04728565  0.5104596 ] reward:  -0.3333574985297942\n",
      "planning time:  0.20325183868408203\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.10619689187139202\n",
      "planning for trajectory step:  25\n",
      "action:  [ 0.944984    0.01413638 -0.04002445  0.47703075] reward:  -0.3888799766087058\n",
      "planning time:  0.20313572883605957\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.11727805733795829\n",
      "planning for trajectory step:  26\n",
      "action:  [ 3.6309557e+00  2.8889140e-02 -1.4437331e-03 -4.6838957e-01] reward:  -0.452692227214408\n",
      "planning time:  0.19991254806518555\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.12892189430677242\n",
      "planning for trajectory step:  27\n",
      "action:  [ 3.8957558   0.04344569 -0.01116673 -0.282152  ] reward:  -0.40706326025825057\n",
      "planning time:  0.20185208320617676\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.14111751938320677\n",
      "planning for trajectory step:  28\n",
      "action:  [ 3.8394337   0.03966634  0.04309566 -0.5036638 ] reward:  -0.3234294934982344\n",
      "planning time:  0.1886308193206787\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.15385234568477898\n",
      "planning for trajectory step:  29\n",
      "action:  [ 0.3221151   0.02843525  0.04381429 -0.41902778] reward:  -0.3274410152348654\n",
      "planning time:  0.2033097743988037\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.1671121585517233\n",
      "planning for trajectory step:  30\n",
      "Crash detected!\n",
      "Crash detected!\n",
      "action:  [ 0.3229368   0.01926882 -0.04324167  0.06135538] reward:  -0.3492912664699522\n",
      "planning time:  0.2016890048980713\n",
      "resetting environment, and starting trial : 8\n",
      "Connected!\n",
      "4 DoF [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "No propeller. Is it a magic box?\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 1.0150579389358555e-05\n",
      "Number of stored transitions:  10000\n",
      "Training model\n",
      "training loss:  -38.43189987720259\n",
      "validation loss:  0.00214712368324399\n",
      "training time:  48.92155599594116\n",
      "Model trained\n",
      "planning for trajectory step:  1\n",
      "action:  [ 2.2855675   0.01744377  0.01581441 -0.21629344] reward:  -0.315900039152819\n",
      "planning time:  0.252453088760376\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 7.997055280231069e-05\n",
      "planning for trajectory step:  2\n",
      "action:  [ 2.7991235  -0.0125125   0.0267463  -0.21418631] reward:  -0.31589948921088834\n",
      "planning time:  0.24544787406921387\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.00026576965766001126\n",
      "planning for trajectory step:  3\n",
      "action:  [ 0.4599781  -0.02411203 -0.02859036  0.3479327 ] reward:  -0.3159412671669159\n",
      "planning time:  0.23453950881958008\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0006202613792457937\n",
      "planning for trajectory step:  4\n",
      "action:  [ 3.0818856e+00  2.6766714e-03  2.9325366e-02 -7.5883247e-02] reward:  -0.31593398784821014\n",
      "planning time:  0.23220014572143555\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0011926386612764923\n",
      "planning for trajectory step:  5\n",
      "action:  [ 2.9261408  -0.01703158  0.03136107  0.09153377] reward:  -0.3159090996505286\n",
      "planning time:  0.2194066047668457\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.002028649616473763\n",
      "planning for trajectory step:  6\n",
      "action:  [ 1.0250739   0.04004573 -0.0119036  -0.31900772] reward:  -0.31595730669990146\n",
      "planning time:  0.21452569961547852\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.003170673237135412\n",
      "planning for trajectory step:  7\n",
      "action:  [ 2.73898506e+00 -4.50931664e-04  1.63355861e-02  1.14572905e-01] reward:  -0.31595796214053096\n",
      "planning time:  0.2151501178741455\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.004657795105706712\n",
      "planning for trajectory step:  8\n",
      "action:  [ 2.8361127  -0.02798777  0.00632543 -0.3904132 ] reward:  -0.3159193023296132\n",
      "planning time:  0.21674847602844238\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.006525883105351723\n",
      "planning for trajectory step:  9\n",
      "action:  [ 2.7568665  -0.04183507  0.00758893  0.48594218] reward:  -0.3159529196128942\n",
      "planning time:  0.2326366901397705\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.008807663130524624\n",
      "planning for trajectory step:  10\n",
      "action:  [ 0.38659954  0.04720166 -0.01841741  0.34566623] reward:  -0.31595594384254094\n",
      "planning time:  0.21450424194335938\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.011532794797541032\n",
      "planning for trajectory step:  11\n",
      "action:  [ 1.5804744  -0.00856341 -0.01332146 -0.40176067] reward:  -0.31593309921182383\n",
      "planning time:  0.23239755630493164\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.014727947155149303\n",
      "planning for trajectory step:  12\n",
      "action:  [2.6939704  0.04942374 0.00691313 0.25323638] reward:  -0.315905891804392\n",
      "planning time:  0.2190995216369629\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.018416874395101913\n",
      "planning for trajectory step:  13\n",
      "action:  [ 0.4033325  -0.04606062 -0.04186766 -0.55912256] reward:  -0.3159281647384848\n",
      "planning time:  0.23134756088256836\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.022620491562726712\n",
      "planning for trajectory step:  14\n",
      "action:  [0.42204008 0.00182907 0.02099653 0.51836604] reward:  -0.3159487142392969\n",
      "planning time:  0.23359036445617676\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.027356950267498275\n",
      "planning for trajectory step:  15\n",
      "action:  [ 1.0854682  -0.03736672 -0.02623477 -0.22061552] reward:  -0.3159086953066709\n",
      "planning time:  0.23222684860229492\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.03264171439360926\n",
      "planning for trajectory step:  16\n",
      "action:  [ 3.7919314   0.01177083  0.00958644 -0.59005535] reward:  -0.31591269401303357\n",
      "planning time:  0.24558019638061523\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.03848763581054164\n",
      "planning for trajectory step:  17\n",
      "action:  [ 2.8559518   0.04524336 -0.03721719 -0.31871846] reward:  -0.3165134106900058\n",
      "planning time:  0.23214292526245117\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.044905030083638116\n",
      "planning for trajectory step:  18\n",
      "action:  [ 2.1410553  -0.0180156   0.04144965  0.0073449 ] reward:  -0.3351720417081104\n",
      "planning time:  0.23150062561035156\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.05190175218467344\n",
      "planning for trajectory step:  19\n",
      "action:  [ 1.2857437  -0.04277645  0.02808192 -0.05017927] reward:  -0.3669563593417884\n",
      "planning time:  0.23547077178955078\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.05948327220242562\n",
      "planning for trajectory step:  20\n",
      "action:  [ 0.2957713  -0.0420862  -0.04575234 -0.27030694] reward:  -0.31124006430410145\n",
      "planning time:  0.22993206977844238\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0676527510532474\n",
      "planning for trajectory step:  21\n",
      "action:  [ 0.05309733 -0.01146424 -0.00696031  0.5712923 ] reward:  -0.2960489341102998\n",
      "planning time:  0.2325882911682129\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.07641111619163748\n",
      "planning for trajectory step:  22\n",
      "action:  [2.3404715  0.04523407 0.04045339 0.49210975] reward:  -0.2923297246738075\n",
      "planning time:  0.21593976020812988\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0857571373208119\n",
      "planning for trajectory step:  23\n",
      "action:  [ 1.1795806  -0.02140478  0.00619295 -0.3511325 ] reward:  -0.2923685594359835\n",
      "planning time:  0.23252558708190918\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.09568750210327515\n",
      "planning for trajectory step:  24\n",
      "action:  [ 3.1576331e+00 -1.7332296e-03 -6.1218319e-03 -3.5014766e-01] reward:  -0.29240648563232685\n",
      "planning time:  0.23419189453125\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.10619689187139202\n",
      "planning for trajectory step:  25\n",
      "action:  [ 0.12059894  0.04945349 -0.03352846 -0.5018292 ] reward:  -0.2924126542581513\n",
      "planning time:  0.23230624198913574\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.11727805733795829\n",
      "planning for trajectory step:  26\n",
      "action:  [ 3.4739463   0.01544489 -0.01833695 -0.36337364] reward:  -0.29243765778184083\n",
      "planning time:  0.23231101036071777\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.12892189430677242\n",
      "planning for trajectory step:  27\n",
      "action:  [0.6434762  0.02558192 0.04880031 0.28294665] reward:  -0.29264044420667407\n",
      "planning time:  0.24894428253173828\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.14111751938320677\n",
      "planning for trajectory step:  28\n",
      "action:  [ 3.187761   -0.02801336  0.00444503  0.23943523] reward:  -0.30000227234656\n",
      "planning time:  0.23006725311279297\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.15385234568477898\n",
      "planning for trajectory step:  29\n",
      "action:  [ 0.5331203  -0.04147898 -0.04437319  0.47968596] reward:  -0.2926619338417583\n",
      "planning time:  0.23257708549499512\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.1671121585517233\n",
      "planning for trajectory step:  30\n",
      "action:  [ 2.9773002   0.03763824  0.01255282 -0.5547603 ] reward:  -0.29380927540650176\n",
      "planning time:  0.24719834327697754\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.1808811912575617\n",
      "planning for trajectory step:  31\n",
      "action:  [ 2.7560306  -0.0493081   0.02780712 -0.24323691] reward:  -0.2921479834888477\n",
      "planning time:  0.23146772384643555\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.1951422007196756\n",
      "planning for trajectory step:  32\n",
      "action:  [3.9080915  0.04518273 0.04443128 0.49592695] reward:  -0.2922133558425049\n",
      "planning time:  0.23405838012695312\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.2098765432098766\n",
      "planning for trajectory step:  33\n",
      "action:  [3.1675947  0.03562257 0.02732923 0.57714826] reward:  -0.29299227882733564\n",
      "planning time:  0.24659276008605957\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.22506425006497865\n",
      "planning for trajectory step:  34\n",
      "action:  [ 2.7862735   0.04393328  0.01699677 -0.4572111 ] reward:  -0.31772840152235304\n",
      "planning time:  0.23227334022521973\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.24068410339736837\n",
      "planning for trajectory step:  35\n",
      "action:  [ 2.9058862e+00 -6.2487775e-04 -2.4835691e-02  3.7326482e-01] reward:  -0.3826545036917191\n",
      "planning time:  0.23346662521362305\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.25671371180557717\n",
      "planning for trajectory step:  36\n",
      "action:  [3.7736604  0.00723563 0.0449519  0.53782296] reward:  -0.42205491980352344\n",
      "planning time:  0.23479223251342773\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.2731295860848524\n",
      "planning for trajectory step:  37\n",
      "action:  [ 3.8395257   0.03768056  0.014892   -0.58184576] reward:  -0.42909942025526315\n",
      "planning time:  0.2328338623046875\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.2899072149377282\n",
      "planning for trajectory step:  38\n",
      "action:  [ 1.5643384  -0.03405887  0.01480361 -0.46202844] reward:  -0.31785938693046817\n",
      "planning time:  0.24708294868469238\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.3070211406845973\n",
      "planning for trajectory step:  39\n",
      "action:  [ 2.8369632   0.01845389  0.03887315 -0.43258125] reward:  -0.4053841783335508\n",
      "planning time:  0.23475337028503418\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.3244450349742825\n",
      "planning for trajectory step:  40\n",
      "action:  [ 0.19770512 -0.01978437  0.04442079  0.19266258] reward:  -0.676163478162038\n",
      "planning time:  0.2317047119140625\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.3421517744946073\n",
      "planning for trajectory step:  41\n",
      "action:  [ 2.2612312  -0.04753806  0.04279492 -0.5120479 ] reward:  -0.7213746854493186\n",
      "planning time:  0.23169732093811035\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.360113516682968\n",
      "planning for trajectory step:  42\n",
      "action:  [ 1.9457724   0.04901532 -0.01255771  0.04815182] reward:  -0.7020511578681577\n",
      "planning time:  0.24941730499267578\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.37830177543690424\n",
      "planning for trajectory step:  43\n",
      "action:  [ 0.7550332  -0.04769564 -0.04237199 -0.28533772] reward:  -0.6786552832038327\n",
      "planning time:  0.23219943046569824\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.39668749682467125\n",
      "planning for trajectory step:  44\n",
      "action:  [0.32358438 0.013041   0.03363428 0.2747085 ] reward:  -0.6622717913237329\n",
      "planning time:  0.23342633247375488\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.41524113479581026\n",
      "planning for trajectory step:  45\n",
      "action:  [1.2584535  0.02618845 0.03657947 0.19186585] reward:  -0.6624446836380653\n",
      "planning time:  0.23320412635803223\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.4339327268917206\n",
      "planning for trajectory step:  46\n",
      "action:  [ 3.070938    0.0416915  -0.04732355 -0.40632218] reward:  -0.6624463334680598\n",
      "planning time:  0.2304821014404297\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.45273196995622983\n",
      "planning for trajectory step:  47\n",
      "action:  [ 3.2699375   0.04135307 -0.00620467  0.43550935] reward:  -0.6624605145332851\n",
      "planning time:  0.23239421844482422\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.4716082958461666\n",
      "planning for trajectory step:  48\n",
      "action:  [ 3.1884527  -0.04287343 -0.04512244  0.5018175 ] reward:  -0.6624707515404628\n",
      "planning time:  0.24946880340576172\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.4905309471419318\n",
      "planning for trajectory step:  49\n",
      "action:  [ 2.7733815   0.02426808 -0.04115465  0.34735924] reward:  -0.6641343421134702\n",
      "planning time:  0.5930750370025635\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5094690528580682\n",
      "planning for trajectory step:  50\n",
      "action:  [ 1.5237789  -0.02545745  0.00445396 -0.37797505] reward:  -0.7030799533149577\n",
      "planning time:  0.2819974422454834\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5283917041538334\n",
      "planning for trajectory step:  51\n",
      "action:  [2.6427772  0.04755924 0.04185041 0.43439564] reward:  -0.7086064920114018\n",
      "planning time:  0.26396632194519043\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5472680300437707\n",
      "planning for trajectory step:  52\n",
      "action:  [ 3.9943593  -0.04606893 -0.04839383  0.4635897 ] reward:  -0.7085996791095299\n",
      "planning time:  0.26397228240966797\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5660672731082802\n",
      "planning for trajectory step:  53\n",
      "action:  [ 0.6090056   0.0316116  -0.04129232 -0.55850434] reward:  -0.7086043562278151\n",
      "planning time:  0.2656729221343994\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5847588652041902\n",
      "planning for trajectory step:  54\n",
      "action:  [ 2.3046136  -0.02621445  0.04161981  0.5835095 ] reward:  -0.729311728021641\n",
      "planning time:  0.27733778953552246\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.603312503175329\n",
      "planning for trajectory step:  55\n",
      "action:  [ 0.10008095  0.00067989  0.01038027 -0.2593102 ] reward:  -0.713154993186008\n",
      "planning time:  0.262225866317749\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.621698224563096\n",
      "planning for trajectory step:  56\n",
      "action:  [ 0.14227422 -0.00340185  0.04764093  0.18442659] reward:  -0.7140340148308242\n",
      "planning time:  0.35593175888061523\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.6398864833170326\n",
      "planning for trajectory step:  57\n",
      "action:  [ 2.235714    0.02485205 -0.01010725 -0.5051973 ] reward:  -0.7140487877638471\n",
      "planning time:  0.2638266086578369\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.657848225505393\n",
      "planning for trajectory step:  58\n",
      "action:  [ 3.1202729   0.00497917 -0.0354967  -0.5226027 ] reward:  -0.7140543668151416\n",
      "planning time:  0.28173232078552246\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.6755549650257178\n",
      "planning for trajectory step:  59\n",
      "action:  [ 0.4421469   0.03275746  0.01905867 -0.5910364 ] reward:  -0.7140637186822253\n",
      "planning time:  0.262768030166626\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.6929788593154029\n",
      "planning for trajectory step:  60\n",
      "action:  [ 2.9202003  -0.03301888  0.01579808 -0.55094856] reward:  -0.7140637238389587\n",
      "planning time:  0.2652416229248047\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.7100927850622721\n",
      "planning for trajectory step:  61\n",
      "action:  [2.376593   0.00941493 0.04654915 0.51171434] reward:  -0.7140531559161981\n",
      "planning time:  0.2631707191467285\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.7268704139151478\n",
      "planning for trajectory step:  62\n",
      "action:  [ 3.8317332   0.04735002 -0.04611523  0.39247024] reward:  -0.7140679031858314\n",
      "planning time:  0.26494908332824707\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.7432862881944238\n",
      "planning for trajectory step:  63\n",
      "action:  [ 0.5246114  -0.02757917 -0.02864811 -0.40954465] reward:  -0.7143481036498839\n",
      "planning time:  0.26497459411621094\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.7593158966026328\n",
      "planning for trajectory step:  64\n",
      "action:  [ 2.7143967   0.04288423 -0.04576716 -0.4221429 ] reward:  -0.7293720924782426\n",
      "planning time:  0.24997472763061523\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.7749357499350218\n",
      "planning for trajectory step:  65\n",
      "action:  [ 1.8651166   0.03403603  0.01361316 -0.54607534] reward:  -0.7178225472344982\n",
      "planning time:  0.2617835998535156\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.7901234567901237\n",
      "planning for trajectory step:  66\n",
      "action:  [1.6567019  0.01001491 0.04766861 0.57306206] reward:  -0.7179278232452864\n",
      "planning time:  0.25014424324035645\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8048577992803249\n",
      "planning for trajectory step:  67\n",
      "action:  [ 3.3137698e+00  3.1409685e-03 -3.4043163e-02  3.8798243e-01] reward:  -0.7179274341037628\n",
      "planning time:  0.24704360961914062\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8191188087424394\n",
      "planning for trajectory step:  68\n",
      "action:  [ 2.4132302   0.04173339 -0.01047112 -0.4585918 ] reward:  -0.7179251745055206\n",
      "planning time:  0.24706053733825684\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8328878414482771\n",
      "planning for trajectory step:  69\n",
      "action:  [ 0.585897    0.00800619  0.03975079 -0.46521056] reward:  -0.7191579046268536\n",
      "planning time:  0.2369546890258789\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8461476543152218\n",
      "planning for trajectory step:  70\n",
      "action:  [ 1.017002    0.01435038 -0.04193421 -0.28553113] reward:  -0.718313959443437\n",
      "planning time:  0.26564550399780273\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8588824806167938\n",
      "planning for trajectory step:  71\n",
      "action:  [ 1.1999136   0.01014615 -0.04203447 -0.21769285] reward:  -0.7183094391705269\n",
      "planning time:  0.26288580894470215\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8710781056932277\n",
      "planning for trajectory step:  72\n",
      "action:  [ 3.771799   -0.01954587 -0.04394726 -0.35315531] reward:  -0.718313498099315\n",
      "planning time:  0.26400327682495117\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8827219426620414\n",
      "planning for trajectory step:  73\n",
      "action:  [ 3.081129   -0.02521089  0.0447588  -0.5606798 ] reward:  -0.7185684946202342\n",
      "planning time:  0.2670745849609375\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8938031081286091\n",
      "planning for trajectory step:  74\n",
      "action:  [ 0.08100159 -0.03626106  0.04518816  0.35606104] reward:  -0.7331390472423991\n",
      "planning time:  0.2805955410003662\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9043124978967261\n",
      "planning for trajectory step:  75\n",
      "action:  [ 1.1915178  -0.02853748 -0.04467517 -0.5944731 ] reward:  -0.7767163069272041\n",
      "planning time:  0.2783188819885254\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9142428626791894\n",
      "planning for trajectory step:  76\n",
      "action:  [ 3.2852578   0.03092017 -0.0177678   0.16675586] reward:  -0.7676481931361259\n",
      "planning time:  0.27950477600097656\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9235888838083635\n",
      "planning for trajectory step:  77\n",
      "action:  [ 2.9993556   0.0336138  -0.00833729 -0.11084805] reward:  -0.766375399057501\n",
      "planning time:  0.2952117919921875\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.932347248946753\n",
      "planning for trajectory step:  78\n",
      "action:  [ 0.47950122 -0.02028213 -0.02795198 -0.570147  ] reward:  -0.7645443102433793\n",
      "planning time:  0.2937183380126953\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.940516727797575\n",
      "planning for trajectory step:  79\n",
      "action:  [ 1.4234359   0.03121868 -0.01447125  0.1552425 ] reward:  -0.7473659931174422\n",
      "planning time:  0.3283119201660156\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9480982478153277\n",
      "planning for trajectory step:  80\n",
      "action:  [ 2.43095    -0.04752421  0.04246859  0.42559212] reward:  -0.7465551086474386\n",
      "planning time:  0.2769937515258789\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9550949699163627\n",
      "planning for trajectory step:  81\n",
      "action:  [ 0.37927938 -0.04935479  0.0423969   0.30458114] reward:  -0.7467061799776291\n",
      "planning time:  0.2762110233306885\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.961512364189459\n",
      "planning for trajectory step:  82\n",
      "action:  [ 2.1718934   0.03534338 -0.03414319 -0.22619915] reward:  -0.7467114178547664\n",
      "planning time:  0.24857211112976074\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9673582856063918\n",
      "planning for trajectory step:  83\n",
      "action:  [2.2853231  0.04525864 0.04814495 0.30610436] reward:  -0.7466998385884842\n",
      "planning time:  0.24919557571411133\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9726430497325027\n",
      "planning for trajectory step:  84\n",
      "action:  [3.6696491  0.04154728 0.02053394 0.56916434] reward:  -0.7467113146197536\n",
      "planning time:  0.24591445922851562\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9773795084372736\n",
      "planning for trajectory step:  85\n",
      "action:  [3.7892463  0.04251453 0.04191504 0.5581289 ] reward:  -0.746711514433208\n",
      "planning time:  0.24713850021362305\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9815831256048986\n",
      "planning for trajectory step:  86\n",
      "action:  [ 0.44722542  0.03992625 -0.04511345  0.18411818] reward:  -0.7522618047974687\n",
      "planning time:  0.2481701374053955\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9852720528448526\n",
      "planning for trajectory step:  87\n",
      "action:  [ 3.5083046   0.03603092 -0.02519982 -0.4101229 ] reward:  -0.7697892895169678\n",
      "planning time:  0.24880743026733398\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9884672052024599\n",
      "planning for trajectory step:  88\n",
      "action:  [ 0.03773551 -0.00239203 -0.04588814  0.02788506] reward:  -0.7087548592050866\n",
      "planning time:  0.247406005859375\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9911923368694775\n",
      "planning for trajectory step:  89\n",
      "action:  [ 0.15377606 -0.04327136 -0.04242963 -0.36058366] reward:  -0.719705655110167\n",
      "planning time:  0.23438143730163574\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9934741168946499\n",
      "planning for trajectory step:  90\n",
      "action:  [0.6354086  0.00926682 0.00184541 0.44446874] reward:  -0.7220649891072012\n",
      "planning time:  0.23546290397644043\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9953422048942939\n",
      "planning for trajectory step:  91\n",
      "action:  [ 2.761205    0.03593521 -0.04415156 -0.02621459] reward:  -0.7316166781486859\n",
      "planning time:  0.2323920726776123\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9968293267628683\n",
      "planning for trajectory step:  92\n",
      "action:  [ 0.11761558  0.01894299  0.0405035  -0.09537546] reward:  -0.7232711684392341\n",
      "planning time:  0.23121380805969238\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9979713503835264\n",
      "planning for trajectory step:  93\n",
      "action:  [ 1.6279211  -0.0242416   0.02577623  0.40590465] reward:  -0.7325710718387052\n",
      "planning time:  0.23330926895141602\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9988073613387254\n",
      "planning for trajectory step:  94\n",
      "action:  [ 2.7198882  -0.03556916 -0.03434218  0.57514167] reward:  -0.7195419310507503\n",
      "planning time:  0.21740221977233887\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9993797386207541\n",
      "planning for trajectory step:  95\n",
      "action:  [0.06063637 0.03899721 0.01164035 0.4839666 ] reward:  -0.7407887260569277\n",
      "planning time:  0.23362255096435547\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9997342303423391\n",
      "planning for trajectory step:  96\n",
      "action:  [1.144951   0.04626534 0.01424206 0.5795472 ] reward:  -0.763577273225148\n",
      "planning time:  0.24890661239624023\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9999200294471979\n",
      "planning for trajectory step:  97\n",
      "action:  [ 3.505433    0.02703886  0.02902541 -0.12059063] reward:  -0.7709224965358018\n",
      "planning time:  0.2334299087524414\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9999898494206105\n",
      "planning for trajectory step:  98\n",
      "action:  [ 0.75901526  0.01268501  0.03986036 -0.5207001 ] reward:  -0.7786254421583786\n",
      "planning time:  0.23205041885375977\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 1.0\n",
      "planning for trajectory step:  99\n",
      "action:  [1.8600335  0.01953443 0.03103691 0.50994813] reward:  -0.9222171537445623\n",
      "planning time:  0.2499842643737793\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 1.0\n",
      "planning for trajectory step:  100\n",
      "action:  [ 1.1247354   0.01217775 -0.03070874  0.0549825 ] reward:  -1.0920316464003959\n",
      "planning time:  0.24495935440063477\n",
      "setpoint updated\n",
      "setting setpoint to:  -2.719333828923398e-07 1.3245362727190578e-06 0.9999982057832423\n",
      "planning for trajectory step:  101\n",
      "action:  [ 1.738789   -0.03051707  0.01985879 -0.18367134] reward:  -1.1397336052773812\n",
      "planning time:  0.23360657691955566\n",
      "setpoint updated\n",
      "setting setpoint to:  -2.1326973642486238e-06 1.044615653911299e-05 0.9999858488085652\n",
      "planning for trajectory step:  102\n",
      "action:  [ 2.0732112   0.0369177  -0.04479463  0.39610225] reward:  -1.0877580411423506\n",
      "planning time:  0.23411273956298828\n",
      "setpoint updated\n",
      "setting setpoint to:  -7.054821058702212e-06 3.47530711421912e-05 0.9999529179345232\n",
      "planning for trajectory step:  103\n",
      "action:  [ 2.7654338   0.04349678  0.04574771 -0.40267003] reward:  -1.084112363551744\n",
      "planning time:  0.23655486106872559\n",
      "setpoint updated\n",
      "setting setpoint to:  -1.6386616180580747e-05 8.119551485977874e-05 0.999889992922992\n",
      "planning for trajectory step:  104\n",
      "action:  [ 3.9401402   0.04369856 -0.03694944 -0.5814504 ] reward:  -1.1372989188038427\n",
      "planning time:  0.2502861022949219\n",
      "setpoint updated\n",
      "setting setpoint to:  -3.1355099684166555e-05 0.00015629462216361204 0.9997882324930554\n",
      "planning for trajectory step:  105\n",
      "action:  [ 3.174451   -0.02700974 -0.04975938 -0.54324234] reward:  -1.1870340287005263\n",
      "planning time:  0.2348334789276123\n",
      "setpoint updated\n",
      "setting setpoint to:  -5.3068916403956985e-05 0.0002661513024784345 0.9996393623748905\n",
      "planning for trajectory step:  106\n",
      "action:  [ 2.7772408e+00 -4.2540517e-02  1.4304343e-03 -4.0806010e-02] reward:  -1.1512282616082592\n",
      "planning time:  0.24701595306396484\n",
      "setpoint updated\n",
      "setting setpoint to:  -8.252126169492497e-05 0.0004164551154410927 0.9994356633636531\n",
      "planning for trajectory step:  107\n",
      "action:  [ 3.7598639   0.01125292 -0.04478098 -0.11497425] reward:  -1.0951841421981856\n",
      "planning time:  0.24662470817565918\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.00012059280407277925 0.0006124931461596317 0.9991699593733643\n",
      "planning for trajectory step:  108\n",
      "action:  [ 3.791205   -0.04748119 -0.04037565  0.26490423] reward:  -1.2438085674654504\n",
      "planning time:  0.2764101028442383\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.00016805460785422485 0.0008591588804723906 0.9988356054907952\n",
      "planning for trajectory step:  109\n",
      "Crash detected!\n",
      "Crash detected!\n",
      "action:  [ 1.0076696   0.02822213  0.03472761 -0.12821293] reward:  -1.336826143779451\n",
      "planning time:  0.24846315383911133\n",
      "resetting environment, and starting trial : 9\n",
      "Connected!\n",
      "4 DoF [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "No propeller. Is it a magic box?\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 1.0150579389358555e-05\n",
      "Number of stored transitions:  10000\n",
      "Training model\n",
      "training loss:  -39.5458492000631\n",
      "validation loss:  0.0179336816072464\n",
      "training time:  52.34874486923218\n",
      "Model trained\n",
      "planning for trajectory step:  1\n",
      "action:  [ 1.5551232   0.00603151 -0.01298324 -0.22597095] reward:  -0.31590003914941683\n",
      "planning time:  0.23738574981689453\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 7.997055280231069e-05\n",
      "planning for trajectory step:  2\n",
      "action:  [ 3.0179160e+00 -1.0553393e-02 -1.1259700e-03  1.9170383e-01] reward:  -0.315899473203379\n",
      "planning time:  0.21498489379882812\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.00026576965766001126\n",
      "planning for trajectory step:  3\n",
      "action:  [ 2.9473472  -0.0122614   0.01899152 -0.26986772] reward:  -0.3159389729889992\n",
      "planning time:  0.20367217063903809\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0006202613792457937\n",
      "planning for trajectory step:  4\n",
      "action:  [ 2.705189   -0.01917091  0.00871073 -0.20728393] reward:  -0.3159575964468055\n",
      "planning time:  0.20440196990966797\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0011926386612764923\n",
      "planning for trajectory step:  5\n",
      "action:  [ 3.118368    0.00464699  0.01823762 -0.09367605] reward:  -0.3159560251111043\n",
      "planning time:  0.2183246612548828\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.002028649616473763\n",
      "planning for trajectory step:  6\n",
      "action:  [ 0.26702064  0.00605033  0.0378453  -0.31377062] reward:  -0.3159531177077686\n",
      "planning time:  0.2323927879333496\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.003170673237135412\n",
      "planning for trajectory step:  7\n",
      "action:  [ 3.0478325e+00 -8.6540999e-03  2.0218415e-03  1.2104549e-02] reward:  -0.31593576936416323\n",
      "planning time:  0.21901488304138184\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.004657795105706712\n",
      "planning for trajectory step:  8\n",
      "action:  [ 0.65054005  0.0344956   0.03681453 -0.24067035] reward:  -0.3159048000510487\n",
      "planning time:  0.2194514274597168\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.006525883105351723\n",
      "planning for trajectory step:  9\n",
      "action:  [3.1270576e+00 1.0628906e-03 3.3404555e-02 2.7238840e-01] reward:  -0.3159359075369514\n",
      "planning time:  0.23287248611450195\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.008807663130524624\n",
      "planning for trajectory step:  10\n",
      "action:  [ 0.19327398  0.01289481 -0.01029352 -0.0675557 ] reward:  -0.3159106515084469\n",
      "planning time:  0.23308801651000977\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.011532794797541032\n",
      "planning for trajectory step:  11\n",
      "action:  [ 3.0301275e+00 -6.5545819e-04  1.5595522e-02  9.5516264e-02] reward:  -0.31593347936766086\n",
      "planning time:  0.216719388961792\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.014727947155149303\n",
      "planning for trajectory step:  12\n",
      "action:  [ 3.3472729e+00 -2.9204222e-03  2.7624762e-02  5.6770599e-01] reward:  -0.31590342014828976\n",
      "planning time:  0.23322153091430664\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.018416874395101913\n",
      "planning for trajectory step:  13\n",
      "action:  [ 2.2885654  -0.04584891 -0.01308593 -0.33680564] reward:  -0.31606877739371564\n",
      "planning time:  0.25017356872558594\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.022620491562726712\n",
      "planning for trajectory step:  14\n",
      "action:  [ 0.49283606 -0.02657318 -0.04878398  0.51564693] reward:  -0.31969697333876296\n",
      "planning time:  0.23293161392211914\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.027356950267498275\n",
      "planning for trajectory step:  15\n",
      "action:  [ 0.38594136 -0.02926112  0.03883265 -0.54758906] reward:  -0.3156316170956704\n",
      "planning time:  0.23603582382202148\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.03264171439360926\n",
      "planning for trajectory step:  16\n",
      "action:  [ 3.3449926  -0.00425668  0.02206052  0.22296171] reward:  -0.31565219708528924\n",
      "planning time:  0.21358561515808105\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.03848763581054164\n",
      "planning for trajectory step:  17\n",
      "action:  [ 0.12657034 -0.0492815   0.04466659  0.29866955] reward:  -0.315763137680558\n",
      "planning time:  0.21720290184020996\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.044905030083638116\n",
      "planning for trajectory step:  18\n",
      "action:  [0.0618398  0.04087155 0.04901945 0.49723437] reward:  -0.3188807140004511\n",
      "planning time:  0.23070049285888672\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.05190175218467344\n",
      "planning for trajectory step:  19\n",
      "action:  [ 0.15506662 -0.03232934 -0.00219055 -0.47436237] reward:  -0.315516962687635\n",
      "planning time:  0.23085951805114746\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.05948327220242562\n",
      "planning for trajectory step:  20\n",
      "action:  [0.40203032 0.00354169 0.02715863 0.40028578] reward:  -0.31555299565204026\n",
      "planning time:  0.23313641548156738\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0676527510532474\n",
      "planning for trajectory step:  21\n",
      "action:  [ 0.09127217 -0.03917556  0.04660179 -0.25425   ] reward:  -0.3155599983468209\n",
      "planning time:  0.23284053802490234\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.07641111619163748\n",
      "planning for trajectory step:  22\n",
      "action:  [ 3.1972013  -0.01613126  0.04472282 -0.06540106] reward:  -0.31556008319227447\n",
      "planning time:  0.23406052589416504\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0857571373208119\n",
      "planning for trajectory step:  23\n",
      "action:  [ 3.0269294   0.04608666  0.04520878 -0.54835767] reward:  -0.31555759025679725\n",
      "planning time:  0.21672725677490234\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.09568750210327515\n",
      "planning for trajectory step:  24\n",
      "action:  [ 3.0882187  -0.00436263 -0.0062325  -0.5937512 ] reward:  -0.3156143587144186\n",
      "planning time:  0.2361125946044922\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.10619689187139202\n",
      "planning for trajectory step:  25\n",
      "action:  [ 0.09098558  0.04875714  0.01694352 -0.48355496] reward:  -0.315612568588472\n",
      "planning time:  0.23635339736938477\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.11727805733795829\n",
      "planning for trajectory step:  26\n",
      "action:  [ 3.369618   -0.00395224  0.01822193  0.16045724] reward:  -0.31561599667734735\n",
      "planning time:  0.23012852668762207\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.12892189430677242\n",
      "planning for trajectory step:  27\n",
      "action:  [ 1.3841362  -0.04521434 -0.04757507  0.41833687] reward:  -0.3156894106605849\n",
      "planning time:  0.233931303024292\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.14111751938320677\n",
      "planning for trajectory step:  28\n",
      "action:  [ 3.4496064   0.04876188 -0.04501168  0.59339863] reward:  -0.31951999126613995\n",
      "planning time:  0.23176884651184082\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.15385234568477898\n",
      "planning for trajectory step:  29\n",
      "action:  [ 2.8174903  -0.04835071  0.03896036  0.43210953] reward:  -0.31569348661092334\n",
      "planning time:  0.23167943954467773\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.1671121585517233\n",
      "planning for trajectory step:  30\n",
      "action:  [ 0.33162943 -0.04765183 -0.01365578 -0.3244097 ] reward:  -0.32236381421659044\n",
      "planning time:  0.23163127899169922\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.1808811912575617\n",
      "planning for trajectory step:  31\n",
      "action:  [ 0.7651957   0.01279291 -0.04299567 -0.12174148] reward:  -0.30792946058328674\n",
      "planning time:  0.2299816608428955\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.1951422007196756\n",
      "planning for trajectory step:  32\n",
      "action:  [ 3.7571487   0.00548272  0.03614716 -0.05652146] reward:  -0.2937152358824179\n",
      "planning time:  0.23341631889343262\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.2098765432098766\n",
      "planning for trajectory step:  33\n",
      "action:  [ 2.4769757  -0.04919592  0.03969133 -0.12433863] reward:  -0.2945523295390364\n",
      "planning time:  0.23453474044799805\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.22506425006497865\n",
      "planning for trajectory step:  34\n",
      "action:  [3.235765   0.04319857 0.0439085  0.40086877] reward:  -0.31319238407541217\n",
      "planning time:  0.2176356315612793\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.24068410339736837\n",
      "planning for trajectory step:  35\n",
      "action:  [0.7811467  0.02637202 0.04612946 0.52493024] reward:  -0.32702567011193157\n",
      "planning time:  0.20222759246826172\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.25671371180557717\n",
      "planning for trajectory step:  36\n",
      "action:  [ 3.7381692e+00 -2.3122113e-04  4.2966209e-02  5.5969816e-01] reward:  -0.32337758808877287\n",
      "planning time:  0.23007655143737793\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.2731295860848524\n",
      "planning for trajectory step:  37\n",
      "action:  [ 2.7590854  -0.02794832  0.01833543  0.18119386] reward:  -0.31292213383478523\n",
      "planning time:  0.23328351974487305\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.2899072149377282\n",
      "planning for trajectory step:  38\n",
      "action:  [ 3.40677     0.00892098  0.02661544 -0.07807352] reward:  -0.3308584074413346\n",
      "planning time:  0.24866461753845215\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.3070211406845973\n",
      "planning for trajectory step:  39\n",
      "action:  [ 3.0300038  -0.03299577  0.0430716  -0.45605642] reward:  -0.3601439812752419\n",
      "planning time:  0.21722793579101562\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.3244450349742825\n",
      "planning for trajectory step:  40\n",
      "action:  [ 1.5183343   0.02381925 -0.02897442 -0.4450202 ] reward:  -0.44870139841011336\n",
      "planning time:  0.23170804977416992\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.3421517744946073\n",
      "planning for trajectory step:  41\n",
      "action:  [ 2.3264062   0.02922909 -0.00867754  0.5194438 ] reward:  -0.6015962512272379\n",
      "planning time:  0.2336292266845703\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.360113516682968\n",
      "planning for trajectory step:  42\n",
      "action:  [ 0.22416826  0.04515278  0.04244932 -0.5721518 ] reward:  -0.6209215639294683\n",
      "planning time:  0.2196807861328125\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.37830177543690424\n",
      "planning for trajectory step:  43\n",
      "action:  [3.7072353  0.04943938 0.04410767 0.05576381] reward:  -0.6209364063184647\n",
      "planning time:  0.21664810180664062\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.39668749682467125\n",
      "planning for trajectory step:  44\n",
      "action:  [ 0.22143555 -0.04373165 -0.04771167  0.11406634] reward:  -0.6209279954914684\n",
      "planning time:  0.23414921760559082\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.41524113479581026\n",
      "planning for trajectory step:  45\n",
      "action:  [ 0.6041623  -0.03340285  0.04253785  0.5249858 ] reward:  -0.6272070226656375\n",
      "planning time:  0.23464679718017578\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.4339327268917206\n",
      "planning for trajectory step:  46\n",
      "action:  [ 0.30968186 -0.03753471  0.03539751  0.5575938 ] reward:  -0.6218677744577032\n",
      "planning time:  0.2630901336669922\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.45273196995622983\n",
      "planning for trajectory step:  47\n",
      "action:  [ 0.42065766 -0.02968616  0.03489191  0.51655394] reward:  -0.6218904147066465\n",
      "planning time:  0.2161266803741455\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.4716082958461666\n",
      "planning for trajectory step:  48\n",
      "action:  [ 1.1675453  -0.01074905  0.04155121 -0.55423146] reward:  -0.6218891860031779\n",
      "planning time:  0.2346663475036621\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.4905309471419318\n",
      "planning for trajectory step:  49\n",
      "action:  [ 0.09465267 -0.04263214 -0.02997559  0.5713292 ] reward:  -0.6218894765037435\n",
      "planning time:  0.2316575050354004\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5094690528580682\n",
      "planning for trajectory step:  50\n",
      "action:  [ 2.3070757  -0.0254597   0.02983893  0.52159566] reward:  -0.6218911645855868\n",
      "planning time:  0.2337784767150879\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5283917041538334\n",
      "planning for trajectory step:  51\n",
      "action:  [ 0.656988   -0.04338349  0.04889684 -0.20651418] reward:  -0.6218866811285382\n",
      "planning time:  0.23233675956726074\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5472680300437707\n",
      "planning for trajectory step:  52\n",
      "action:  [1.5984293  0.02997235 0.04237384 0.45968804] reward:  -0.6218984181199458\n",
      "planning time:  0.2358391284942627\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5660672731082802\n",
      "planning for trajectory step:  53\n",
      "action:  [ 0.96541685 -0.03287479 -0.03055148  0.53489447] reward:  -0.6218937246091141\n",
      "planning time:  0.23114967346191406\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5847588652041902\n",
      "planning for trajectory step:  54\n",
      "action:  [ 0.6404351  -0.04003959 -0.03295697 -0.5096242 ] reward:  -0.6218974269776935\n",
      "planning time:  0.21797800064086914\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.603312503175329\n",
      "planning for trajectory step:  55\n",
      "action:  [ 2.4690492  -0.00662357  0.02559104  0.13386382] reward:  -0.621892146365647\n",
      "planning time:  0.23281335830688477\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.621698224563096\n",
      "planning for trajectory step:  56\n",
      "action:  [ 0.21787903 -0.03941256 -0.02562918  0.31882113] reward:  -0.6218911003460741\n",
      "planning time:  0.2330172061920166\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.6398864833170326\n",
      "planning for trajectory step:  57\n",
      "action:  [ 0.03836374 -0.01420812 -0.00978605  0.18480062] reward:  -0.6218979834875252\n",
      "planning time:  0.23150944709777832\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.657848225505393\n",
      "planning for trajectory step:  58\n",
      "action:  [ 0.31355968 -0.0410422  -0.03789766  0.3711203 ] reward:  -0.6218871268898566\n",
      "planning time:  0.2330770492553711\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.6755549650257178\n",
      "planning for trajectory step:  59\n",
      "action:  [ 3.783038    0.03843498 -0.03364214  0.4242381 ] reward:  -0.6218858589351763\n",
      "planning time:  0.23357129096984863\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.6929788593154029\n",
      "planning for trajectory step:  60\n",
      "action:  [0.38155836 0.04004558 0.00075046 0.5060822 ] reward:  -0.622186002762004\n",
      "planning time:  0.21888136863708496\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.7100927850622721\n",
      "planning for trajectory step:  61\n",
      "action:  [ 3.9024625  -0.03980281 -0.03190782  0.5934825 ] reward:  -0.6306889846183039\n",
      "planning time:  0.25042295455932617\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.7268704139151478\n",
      "planning for trajectory step:  62\n",
      "action:  [ 0.9370936  -0.04061184  0.02505665  0.30060583] reward:  -0.6200575920413002\n",
      "planning time:  0.22871994972229004\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.7432862881944238\n",
      "planning for trajectory step:  63\n",
      "action:  [ 3.366186   -0.01893315 -0.04771796  0.48311567] reward:  -0.6350413588828541\n",
      "planning time:  0.2348020076751709\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.7593158966026328\n",
      "planning for trajectory step:  64\n",
      "action:  [ 3.8641071  -0.04027227 -0.0490011   0.40138307] reward:  -0.626761962547853\n",
      "planning time:  0.21892642974853516\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.7749357499350218\n",
      "planning for trajectory step:  65\n",
      "action:  [ 0.8679432  -0.04142275  0.02815663  0.54461384] reward:  -0.62268898852503\n",
      "planning time:  0.23217535018920898\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.7901234567901237\n",
      "planning for trajectory step:  66\n",
      "action:  [ 0.51761615  0.04058379 -0.01808921  0.55167854] reward:  -0.5579058218681119\n",
      "planning time:  0.23221230506896973\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8048577992803249\n",
      "planning for trajectory step:  67\n",
      "action:  [ 1.4919649  -0.03886126  0.04245452  0.4943395 ] reward:  -0.38059942541775676\n",
      "planning time:  0.2322075366973877\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8191188087424394\n",
      "planning for trajectory step:  68\n",
      "action:  [ 0.3920928  -0.04186863  0.04362038  0.3484903 ] reward:  -0.3410714790653\n",
      "planning time:  0.2322831153869629\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8328878414482771\n",
      "planning for trajectory step:  69\n",
      "action:  [ 1.737363   -0.01906962 -0.02269883  0.1490455 ] reward:  -0.3411392865949598\n",
      "planning time:  0.2155001163482666\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8461476543152218\n",
      "planning for trajectory step:  70\n",
      "action:  [ 0.01465542  0.02066487 -0.04522169 -0.48898607] reward:  -0.34114340413083305\n",
      "planning time:  0.2166290283203125\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8588824806167938\n",
      "planning for trajectory step:  71\n",
      "action:  [ 2.6151624e+00 -3.6773086e-02  4.4335297e-04 -3.5346073e-01] reward:  -0.34116790556065946\n",
      "planning time:  0.2310807704925537\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8710781056932277\n",
      "planning for trajectory step:  72\n",
      "action:  [0.23257838 0.00990115 0.03949022 0.5412724 ] reward:  -0.3411595674973339\n",
      "planning time:  0.2325451374053955\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8827219426620414\n",
      "planning for trajectory step:  73\n",
      "action:  [ 3.7857707  -0.03904083  0.04645218  0.44810557] reward:  -0.3411677918089626\n",
      "planning time:  0.24539422988891602\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8938031081286091\n",
      "planning for trajectory step:  74\n",
      "action:  [ 1.9326339e+00 -2.5313323e-02  2.6756217e-04 -5.6088674e-01] reward:  -0.3416879540525434\n",
      "planning time:  0.21913528442382812\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9043124978967261\n",
      "planning for trajectory step:  75\n",
      "action:  [ 1.7858579  -0.03128611  0.02600109 -0.40999517] reward:  -0.3604440700886027\n",
      "planning time:  0.23330903053283691\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9142428626791894\n",
      "planning for trajectory step:  76\n",
      "action:  [ 0.79729337 -0.00947279  0.01158726  0.590275  ] reward:  -0.3691047728338798\n",
      "planning time:  0.2506561279296875\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9235888838083635\n",
      "planning for trajectory step:  77\n",
      "action:  [ 3.7848475   0.03856989  0.01997851 -0.5969439 ] reward:  -0.36090145370006493\n",
      "planning time:  0.23201847076416016\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.932347248946753\n",
      "planning for trajectory step:  78\n",
      "action:  [ 0.04547528 -0.04256906 -0.03596747  0.4578073 ] reward:  -0.3598338941249933\n",
      "planning time:  0.23294281959533691\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.940516727797575\n",
      "planning for trajectory step:  79\n",
      "action:  [ 1.1616492  -0.02056537  0.01246037  0.55040985] reward:  -0.37138449426468373\n",
      "planning time:  0.2170090675354004\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9480982478153277\n",
      "planning for trajectory step:  80\n",
      "action:  [ 1.9406401  -0.00933677 -0.02094554 -0.54733133] reward:  -0.36016502101097114\n",
      "planning time:  0.2308042049407959\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9550949699163627\n",
      "planning for trajectory step:  81\n",
      "action:  [ 0.21388878 -0.03826291  0.04802966  0.34970292] reward:  -0.3603215869260881\n",
      "planning time:  0.22876405715942383\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.961512364189459\n",
      "planning for trajectory step:  82\n",
      "action:  [2.0363731  0.04430042 0.03795663 0.41458797] reward:  -0.36033236073485087\n",
      "planning time:  0.23362064361572266\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9673582856063918\n",
      "planning for trajectory step:  83\n",
      "action:  [ 1.7239817   0.03333876  0.0381883  -0.5300856 ] reward:  -0.36031676524888717\n",
      "planning time:  0.23381614685058594\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9726430497325027\n",
      "planning for trajectory step:  84\n",
      "action:  [ 0.58350927 -0.04786289  0.03668618  0.5475098 ] reward:  -0.36033256562267785\n",
      "planning time:  0.23383259773254395\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9773795084372736\n",
      "planning for trajectory step:  85\n",
      "action:  [ 0.93129116 -0.04306798  0.02999361 -0.56759673] reward:  -0.3603273940350066\n",
      "planning time:  0.24940085411071777\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9815831256048986\n",
      "planning for trajectory step:  86\n",
      "action:  [ 0.1603021  -0.03723369 -0.04450029 -0.5741994 ] reward:  -0.36031037282954714\n",
      "planning time:  0.23477959632873535\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9852720528448526\n",
      "planning for trajectory step:  87\n",
      "action:  [ 0.15157565 -0.04873753  0.00989451  0.5384873 ] reward:  -0.3603149863798378\n",
      "planning time:  0.2461695671081543\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9884672052024599\n",
      "planning for trajectory step:  88\n",
      "action:  [1.7682121  0.00957448 0.01611392 0.56375194] reward:  -0.36030352587465303\n",
      "planning time:  0.23163270950317383\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9911923368694775\n",
      "planning for trajectory step:  89\n",
      "action:  [ 3.8899503  -0.03642148 -0.03304469  0.5542666 ] reward:  -0.36031364557704143\n",
      "planning time:  0.23296618461608887\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9934741168946499\n",
      "planning for trajectory step:  90\n",
      "action:  [ 0.37895617 -0.00687459  0.04456561  0.46481937] reward:  -0.3603277338056498\n",
      "planning time:  0.2304079532623291\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9953422048942939\n",
      "planning for trajectory step:  91\n",
      "action:  [0.28948915 0.04654427 0.04652654 0.2686077 ] reward:  -0.3757635780506482\n",
      "planning time:  0.26492834091186523\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9968293267628683\n",
      "planning for trajectory step:  92\n",
      "action:  [ 0.27319905 -0.03269872  0.03764489  0.04092566] reward:  -0.35892796394781795\n",
      "planning time:  0.2336897850036621\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9979713503835264\n",
      "planning for trajectory step:  93\n",
      "action:  [ 2.2883048  -0.02100371  0.0340865  -0.32663193] reward:  -0.35908069849703916\n",
      "planning time:  0.21805286407470703\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9988073613387254\n",
      "planning for trajectory step:  94\n",
      "action:  [ 2.453636   -0.04071702  0.04663695 -0.55302477] reward:  -0.359084741291194\n",
      "planning time:  0.2183842658996582\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9993797386207541\n",
      "planning for trajectory step:  95\n",
      "action:  [ 3.021165    0.01578633 -0.04783867 -0.35822815] reward:  -0.3591185897661869\n",
      "planning time:  0.21900105476379395\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9997342303423391\n",
      "planning for trajectory step:  96\n",
      "action:  [ 1.5058714  -0.00870718  0.04785044  0.43078575] reward:  -0.35912199553814883\n",
      "planning time:  0.23392844200134277\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9999200294471979\n",
      "planning for trajectory step:  97\n",
      "action:  [ 3.7698336  -0.04626046  0.04069477 -0.55656093] reward:  -0.3591205556588514\n",
      "planning time:  0.21588754653930664\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9999898494206105\n",
      "planning for trajectory step:  98\n",
      "action:  [ 0.28658274 -0.04522614 -0.03972914  0.5950384 ] reward:  -0.3591059565689473\n",
      "planning time:  0.23223185539245605\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 1.0\n",
      "planning for trajectory step:  99\n",
      "action:  [ 3.8588006  -0.04133102  0.04465505  0.5601686 ] reward:  -0.37233768378525406\n",
      "planning time:  0.21741843223571777\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 1.0\n",
      "planning for trajectory step:  100\n",
      "action:  [2.0870757  0.0398824  0.03623464 0.56706154] reward:  -0.35936945728377984\n",
      "planning time:  0.2325437068939209\n",
      "setpoint updated\n",
      "setting setpoint to:  -2.719333828923398e-07 1.3245362727190578e-06 0.9999982057832423\n",
      "planning for trajectory step:  101\n",
      "action:  [ 3.0519428   0.04216862  0.03393096 -0.4555223 ] reward:  -0.3747467628038238\n",
      "planning time:  0.2196788787841797\n",
      "setpoint updated\n",
      "setting setpoint to:  -2.1326973642486238e-06 1.044615653911299e-05 0.9999858488085652\n",
      "planning for trajectory step:  102\n",
      "action:  [ 2.6197255  -0.04337144  0.04242663 -0.2546417 ] reward:  -0.34504613086585745\n",
      "planning time:  0.2340686321258545\n",
      "setpoint updated\n",
      "setting setpoint to:  -7.054821058702212e-06 3.47530711421912e-05 0.9999529179345232\n",
      "planning for trajectory step:  103\n",
      "action:  [ 0.02453352 -0.04592478  0.02538391 -0.24822263] reward:  -0.3324584912275991\n",
      "planning time:  0.23129773139953613\n",
      "setpoint updated\n",
      "setting setpoint to:  -1.6386616180580747e-05 8.119551485977874e-05 0.999889992922992\n",
      "planning for trajectory step:  104\n",
      "action:  [ 0.46730042 -0.04513337 -0.02185187  0.40857172] reward:  -0.33243481627408616\n",
      "planning time:  0.23016023635864258\n",
      "setpoint updated\n",
      "setting setpoint to:  -3.1355099684166555e-05 0.00015629462216361204 0.9997882324930554\n",
      "planning for trajectory step:  105\n",
      "action:  [3.8446121  0.04795835 0.03967068 0.40921974] reward:  -0.33238541547855005\n",
      "planning time:  0.2318434715270996\n",
      "setpoint updated\n",
      "setting setpoint to:  -5.3068916403956985e-05 0.0002661513024784345 0.9996393623748905\n",
      "planning for trajectory step:  106\n",
      "action:  [ 0.11410629  0.03933753 -0.03672748 -0.2970877 ] reward:  -0.3330064509578048\n",
      "planning time:  0.22099614143371582\n",
      "setpoint updated\n",
      "setting setpoint to:  -8.252126169492497e-05 0.0004164551154410927 0.9994356633636531\n",
      "planning for trajectory step:  107\n",
      "action:  [ 0.6717506  -0.04827775  0.03334662  0.47013855] reward:  -0.34830040281876545\n",
      "planning time:  0.23360300064086914\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.00012059280407277925 0.0006124931461596317 0.9991699593733643\n",
      "planning for trajectory step:  108\n",
      "action:  [0.3666349  0.03388259 0.03551589 0.18660676] reward:  -0.33379246848238703\n",
      "planning time:  0.21688508987426758\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.00016805460785422485 0.0008591588804723906 0.9988356054907952\n",
      "planning for trajectory step:  109\n",
      "action:  [1.8409371  0.0312091  0.04092201 0.4753727 ] reward:  -0.3338527855745516\n",
      "planning time:  0.23532676696777344\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.00022557105579722354 0.0011609610802070993 0.998426476029354\n",
      "planning for trajectory step:  110\n",
      "action:  [ 2.2895017   0.04689766 -0.04345207 -0.2382298 ] reward:  -0.3338491691564844\n",
      "planning time:  0.23487401008605957\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.00029370277174125403 0.0015220326584399727 0.9979369525829707\n",
      "planning for trajectory step:  111\n",
      "action:  [ 0.51938903 -0.03927279  0.03536946 -0.08449716] reward:  -0.3338784593715759\n",
      "planning time:  0.231306791305542\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.00037290954324757257 0.0019461395547548071 0.997361912079983\n",
      "planning for trajectory step:  112\n",
      "action:  [ 1.9733219  -0.01367174  0.04476495  0.15078534] reward:  -0.3338831347127565\n",
      "planning time:  0.23116254806518555\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0004635532442394734 0.0024366896105020783 0.9966967148370226\n",
      "planning for trajectory step:  113\n",
      "action:  [3.1035016  0.00877456 0.03836805 0.47414508] reward:  -0.3338644237870837\n",
      "planning time:  0.23270225524902344\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0005659007576425493 0.0029967414440580328 0.9959371926129004\n",
      "planning for trajectory step:  114\n",
      "action:  [ 2.9693332   0.02786773  0.04312345 -0.37783876] reward:  -0.33387803828175333\n",
      "planning time:  0.23088812828063965\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.000680126898024951 0.003629013326083788 0.9950796366624924\n",
      "planning for trajectory step:  115\n",
      "action:  [ 0.38644633 -0.04431115  0.02156822 -0.5924251 ] reward:  -0.3338994681799241\n",
      "planning time:  0.21753978729248047\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0008063173342376489 0.004335892054784423 0.9941207857906258\n",
      "planning for trajectory step:  116\n",
      "action:  [0.10377964 0.04655663 0.04721178 0.35065934] reward:  -0.3338981069433973\n",
      "planning time:  0.2301650047302246\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0009444715120546929 0.005119441831168079 0.9930578144059643\n",
      "planning for trajectory step:  117\n",
      "action:  [3.6187048  0.01910904 0.03405515 0.50143665] reward:  -0.33384871827539037\n",
      "planning time:  0.23175549507141113\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0010945055768134724 0.005981413134305054 0.991888320574894\n",
      "planning for trajectory step:  118\n",
      "action:  [ 3.8545668e+00 -3.5566690e-03  9.1989674e-03 -5.1291037e-01] reward:  -0.33422771167212995\n",
      "planning time:  0.4560563564300537\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0012562552960549774 0.006923251596586897 0.9906103140754094\n",
      "planning for trajectory step:  119\n",
      "action:  [ 0.05375455  0.03159648  0.04017119 -0.48599565] reward:  -0.412937689665762\n",
      "planning time:  0.24595355987548828\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0014294789821640585 0.007946106878985504 0.9892222044509987\n",
      "planning for trajectory step:  120\n",
      "action:  [ 2.310652   -0.04122149  0.04436962 -0.47862753] reward:  -0.5972619655149138\n",
      "planning time:  0.23186397552490234\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0016138604150096874 0.009050841546312215 0.9877227890645303\n",
      "planning for trajectory step:  121\n",
      "action:  [ 0.834084   -0.00920242  0.01776092  0.32083398] reward:  -0.6112591302239128\n",
      "planning time:  0.21661806106567383\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0018090117645852173 0.010238039942476912 0.9861112411521374\n",
      "planning for trajectory step:  122\n",
      "action:  [ 2.693343   -0.02712717  0.0040341  -0.5729295 ] reward:  -0.6036270097499051\n",
      "planning time:  0.2297513484954834\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.002014476513648644 0.011508017065747105 0.9843870978771051\n",
      "planning for trajectory step:  123\n",
      "action:  [ 0.35291108  0.04922897 -0.01879651  0.54766   ] reward:  -0.601114204303023\n",
      "planning time:  0.23572564125061035\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0022297323803628626 0.012860827444007033 0.9825502483837555\n",
      "planning for trajectory step:  124\n",
      "action:  [0.46567518 0.04434686 0.00809644 0.520375  ] reward:  -0.6011692297571894\n",
      "planning time:  0.24887800216674805\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0024541942409359355 0.014296274010016777 0.9806009218513326\n",
      "planning for trajectory step:  125\n",
      "action:  [ 3.792571    0.00668115 -0.03029954 -0.54423654] reward:  -0.6012200700539055\n",
      "planning time:  0.24867510795593262\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0026872170522613454 0.015813916976671324 0.9785396755478899\n",
      "planning for trajectory step:  126\n",
      "action:  [ 3.146341    0.02114707 -0.04901367  0.5673529 ] reward:  -0.6015349278103441\n",
      "planning time:  0.23468685150146484\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0029280987745582605 0.017413082712259685 0.9763673828841749\n",
      "planning for trajectory step:  127\n",
      "action:  [ 3.7730982  -0.0149665   0.04693594  0.596176  ] reward:  -0.6150716096807769\n",
      "planning time:  0.2298111915588379\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.003176083294011789 0.019092872615723993 0.974085221467515\n",
      "planning for trajectory step:  128\n",
      "action:  [3.5330157  0.04360044 0.04442451 0.5496795 ] reward:  -0.6263174209691772\n",
      "planning time:  0.23047804832458496\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0034303633454132463 0.02085217199191857 0.9716946611557035\n",
      "planning for trajectory step:  129\n",
      "action:  [ 0.2836319   0.04174763 -0.04372888  0.5204428 ] reward:  -0.6305276705641699\n",
      "planning time:  0.2478470802307129\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.003690083434800414 0.022689658926869062 0.9691974521108854\n",
      "planning for trajectory step:  130\n",
      "action:  [ 1.3318331   0.01753637 -0.03232544  0.48137408] reward:  -0.7134406646670397\n",
      "planning time:  0.23184514045715332\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.003954342762097794 0.024603813163031497 0.9665956128534426\n",
      "planning for trajectory step:  131\n",
      "action:  [ 2.8526478  -0.03207338 -0.04643651  0.5640091 ] reward:  -0.7046039944538872\n",
      "planning time:  0.2499253749847412\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.004222198143756878 0.026592924974551426 0.9638914183158809\n",
      "planning for trajectory step:  132\n",
      "action:  [ 3.5139027  -0.0183154  -0.04886356 -0.34140527] reward:  -0.5117881547188824\n",
      "planning time:  0.23618364334106445\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.004492666935396402 0.028655104042522955 0.961087387896714\n",
      "planning for trajectory step:  133\n",
      "action:  [ 1.1897118   0.01937845 -0.04242649  0.5485127 ] reward:  -0.5317514471156505\n",
      "planning time:  0.23200535774230957\n",
      "setpoint updated\n",
      "setting setpoint to:  -0.0047647299544426105 0.030788288330247937 0.9581862735143508\n",
      "planning for trajectory step:  134\n",
      "Crash detected!\n",
      "Crash detected!\n",
      "action:  [ 2.7104428   0.04389742 -0.04473386  0.46196744] reward:  -0.46409300849349683\n",
      "planning time:  0.21791410446166992\n",
      "resetting environment, and starting trial : 10\n",
      "Connected!\n",
      "4 DoF [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "No propeller. Is it a magic box?\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 1.0150579389358555e-05\n",
      "Number of stored transitions:  10000\n",
      "Training model\n",
      "training loss:  -41.102954776495096\n",
      "validation loss:  0.0054159825667738914\n",
      "training time:  52.846999168395996\n",
      "Model trained\n",
      "planning for trajectory step:  1\n",
      "action:  [ 1.2070508   0.00305417 -0.00314444 -0.18143667] reward:  -0.315900039152819\n",
      "planning time:  0.19708538055419922\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 7.997055280231069e-05\n",
      "planning for trajectory step:  2\n",
      "action:  [ 3.0322986  -0.01399816 -0.01779505  0.20341356] reward:  -0.3159086998459906\n",
      "planning time:  0.20052313804626465\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.00026576965766001126\n",
      "planning for trajectory step:  3\n",
      "action:  [ 0.97416496  0.03979644  0.02117878 -0.29309362] reward:  -0.31593579998296184\n",
      "planning time:  0.18484735488891602\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0006202613792457937\n",
      "planning for trajectory step:  4\n",
      "action:  [ 3.817112    0.0167627   0.0177298  -0.09467942] reward:  -0.3159566641088768\n",
      "planning time:  0.19976019859313965\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0011926386612764923\n",
      "planning for trajectory step:  5\n",
      "action:  [ 3.2436652  -0.00936544 -0.02094059  0.2513526 ] reward:  -0.3159181423370474\n",
      "planning time:  0.2162024974822998\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.002028649616473763\n",
      "planning for trajectory step:  6\n",
      "action:  [ 2.1988726e+00  1.5553198e-02  1.9139088e-03 -4.7924486e-01] reward:  -0.335245022729076\n",
      "planning time:  0.18570613861083984\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.003170673237135412\n",
      "planning for trajectory step:  7\n",
      "action:  [ 0.635241   -0.01943994 -0.02280865 -0.17500536] reward:  -0.3600159712610437\n",
      "planning time:  0.18497323989868164\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.004657795105706712\n",
      "planning for trajectory step:  8\n",
      "action:  [ 3.6399615   0.01899225 -0.04424139 -0.24416514] reward:  -0.356235705600639\n",
      "planning time:  0.16910195350646973\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.006525883105351723\n",
      "planning for trajectory step:  9\n",
      "action:  [ 2.7996418   0.03447933 -0.00918672  0.5393948 ] reward:  -0.3021937932304036\n",
      "planning time:  0.18934416770935059\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.008807663130524624\n",
      "planning for trajectory step:  10\n",
      "action:  [3.2293108  0.0407083  0.03514716 0.5131761 ] reward:  -0.3167747782017379\n",
      "planning time:  0.20149850845336914\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.011532794797541032\n",
      "planning for trajectory step:  11\n",
      "action:  [ 3.8978434  -0.04731538  0.01665929 -0.11188281] reward:  -0.33649630849048345\n",
      "planning time:  0.18694686889648438\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.014727947155149303\n",
      "planning for trajectory step:  12\n",
      "action:  [ 2.7050664   0.04538602 -0.04725861 -0.409985  ] reward:  -0.332384360618731\n",
      "planning time:  0.1853480339050293\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.018416874395101913\n",
      "planning for trajectory step:  13\n",
      "action:  [ 3.154122   -0.02322907  0.00556348  0.09822713] reward:  -0.38339758306703997\n",
      "planning time:  0.18543100357055664\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.022620491562726712\n",
      "planning for trajectory step:  14\n",
      "Crash detected!\n",
      "Crash detected!\n",
      "action:  [ 2.0842013   0.04891218 -0.03740648  0.14583296] reward:  -0.5605958226629401\n",
      "planning time:  0.20220160484313965\n",
      "resetting environment, and starting trial : 11\n",
      "Connected!\n",
      "4 DoF [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "No propeller. Is it a magic box?\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 1.0150579389358555e-05\n",
      "Number of stored transitions:  10000\n",
      "Training model\n",
      "training loss:  -41.26902238474596\n",
      "validation loss:  0.0035541444085538387\n",
      "training time:  47.895580530166626\n",
      "Model trained\n",
      "planning for trajectory step:  1\n",
      "action:  [ 1.6656036  -0.00760862  0.00175281  0.18652155] reward:  -0.31590003914941683\n",
      "planning time:  0.18050909042358398\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 7.997055280231069e-05\n",
      "planning for trajectory step:  2\n",
      "action:  [ 3.336985    0.01212781 -0.00933589 -0.04922827] reward:  -0.31589949615659485\n",
      "planning time:  0.18399930000305176\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.00026576965766001126\n",
      "planning for trajectory step:  3\n",
      "action:  [ 3.2857168   0.007544   -0.03986346  0.26477835] reward:  -0.31592804942452946\n",
      "planning time:  0.20219683647155762\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0006202613792457937\n",
      "planning for trajectory step:  4\n",
      "action:  [ 3.4244940e+00 -2.8327459e-03  1.3378968e-02 -1.6812834e-01] reward:  -0.31758671213078205\n",
      "planning time:  0.18805789947509766\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0011926386612764923\n",
      "planning for trajectory step:  5\n",
      "action:  [ 3.0064862   0.00579783  0.01048888 -0.01872661] reward:  -0.32060330633977485\n",
      "planning time:  0.20183777809143066\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.002028649616473763\n",
      "planning for trajectory step:  6\n",
      "action:  [ 0.67301315  0.04558737 -0.03614571  0.42617857] reward:  -0.3261688333913516\n",
      "planning time:  0.18522882461547852\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.003170673237135412\n",
      "planning for trajectory step:  7\n",
      "action:  [ 3.50299573e+00 -1.39809679e-03  2.58940961e-02  1.17497124e-01] reward:  -0.31986559631418204\n",
      "planning time:  0.18674039840698242\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.004657795105706712\n",
      "planning for trajectory step:  8\n",
      "action:  [ 2.7336776  -0.03846547 -0.01898752 -0.16489877] reward:  -0.29689414484900345\n",
      "planning time:  0.1860208511352539\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.006525883105351723\n",
      "planning for trajectory step:  9\n",
      "action:  [ 0.08399358  0.04342324  0.04388154 -0.37621918] reward:  -0.30596581247781107\n",
      "planning time:  0.18551945686340332\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.008807663130524624\n",
      "planning for trajectory step:  10\n",
      "action:  [2.5303643e+00 1.0873792e-02 1.4025051e-03 7.0026278e-02] reward:  -0.31456053816635104\n",
      "planning time:  0.16973018646240234\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.011532794797541032\n",
      "planning for trajectory step:  11\n",
      "action:  [ 1.8291987   0.00215832 -0.04103149  0.12992689] reward:  -0.3019438981591839\n",
      "planning time:  0.18445777893066406\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.014727947155149303\n",
      "planning for trajectory step:  12\n",
      "action:  [ 3.1699514  -0.04116636 -0.0481075   0.08398623] reward:  -0.30253287192962164\n",
      "planning time:  0.1850595474243164\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.018416874395101913\n",
      "planning for trajectory step:  13\n",
      "action:  [ 3.7857249   0.01097799 -0.03816837 -0.24962893] reward:  -0.30255987527223815\n",
      "planning time:  0.1865100860595703\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.022620491562726712\n",
      "planning for trajectory step:  14\n",
      "action:  [ 3.8086238  -0.03536316  0.03093867  0.39451015] reward:  -0.3027664996539685\n",
      "planning time:  0.18404555320739746\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.027356950267498275\n",
      "planning for trajectory step:  15\n",
      "action:  [ 0.1840338   0.04911107 -0.04534381 -0.2009459 ] reward:  -0.3157817404849399\n",
      "planning time:  0.2019331455230713\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.03264171439360926\n",
      "planning for trajectory step:  16\n",
      "action:  [ 3.3934758e+00 -2.4568499e-03  3.7893936e-02  2.0428775e-02] reward:  -0.3649996873532058\n",
      "planning time:  0.1843726634979248\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.03848763581054164\n",
      "planning for trajectory step:  17\n",
      "action:  [ 0.4291833  -0.04886379  0.02810957  0.3630025 ] reward:  -0.31168417248039343\n",
      "planning time:  0.22241544723510742\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.044905030083638116\n",
      "planning for trajectory step:  18\n",
      "action:  [ 3.2881474   0.00982624 -0.04211175  0.28577745] reward:  -0.31952493245151925\n",
      "planning time:  0.18541550636291504\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.05190175218467344\n",
      "planning for trajectory step:  19\n",
      "action:  [3.5869484  0.04333396 0.04936845 0.547999  ] reward:  -0.3131727269764965\n",
      "planning time:  0.21664047241210938\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.05948327220242562\n",
      "planning for trajectory step:  20\n",
      "action:  [ 3.7305412  -0.00501917  0.03643156 -0.2960195 ] reward:  -0.3195052721019409\n",
      "planning time:  0.20113110542297363\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0676527510532474\n",
      "planning for trajectory step:  21\n",
      "action:  [ 3.7124271e+00  2.7469444e-04 -3.0014958e-02 -1.7995317e-01] reward:  -0.351704964563267\n",
      "planning time:  0.19814395904541016\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.07641111619163748\n",
      "planning for trajectory step:  22\n",
      "action:  [3.7550085  0.04869406 0.04813395 0.55937755] reward:  -0.4484275095367276\n",
      "planning time:  0.20119810104370117\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0857571373208119\n",
      "planning for trajectory step:  23\n",
      "action:  [ 0.01875917  0.04824996 -0.03314827  0.52691096] reward:  -0.5862231342107243\n",
      "planning time:  0.18622541427612305\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.09568750210327515\n",
      "planning for trajectory step:  24\n",
      "action:  [ 2.6067069  -0.04966446 -0.03952245  0.5436561 ] reward:  -0.6477382183835954\n",
      "planning time:  0.2019972801208496\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.10619689187139202\n",
      "planning for trajectory step:  25\n",
      "action:  [ 3.763488   -0.02321801 -0.04510263 -0.24932192] reward:  -0.5909121002014318\n",
      "planning time:  0.1849520206451416\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.11727805733795829\n",
      "planning for trajectory step:  26\n",
      "Crash detected!\n",
      "Crash detected!\n",
      "action:  [ 3.4570909   0.01083464 -0.04796136  0.03008307] reward:  -0.44362922506725294\n",
      "planning time:  0.18649506568908691\n",
      "resetting environment, and starting trial : 12\n",
      "Connected!\n",
      "4 DoF [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "No propeller. Is it a magic box?\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 1.0150579389358555e-05\n",
      "Number of stored transitions:  10000\n",
      "Training model\n",
      "training loss:  -43.57231202861607\n",
      "validation loss:  0.03672017157077789\n",
      "training time:  47.372469902038574\n",
      "Model trained\n",
      "planning for trajectory step:  1\n",
      "action:  [ 2.0723333   0.01023464 -0.01601606 -0.05544572] reward:  -0.315900039152819\n",
      "planning time:  0.1786348819732666\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 7.997055280231069e-05\n",
      "planning for trajectory step:  2\n",
      "action:  [ 0.69711614  0.02879449  0.00536527 -0.29753652] reward:  -0.31591561316693767\n",
      "planning time:  0.18710637092590332\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.00026576965766001126\n",
      "planning for trajectory step:  3\n",
      "action:  [ 2.9781182e+00 -7.0520989e-03 -2.0067424e-03 -2.9919773e-01] reward:  -0.31593567336750306\n",
      "planning time:  0.2025434970855713\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0006202613792457937\n",
      "planning for trajectory step:  4\n",
      "action:  [ 3.4130850e+00  3.3954016e-03  1.1698256e-03 -3.6216494e-01] reward:  -0.31591231951780185\n",
      "planning time:  0.19690155982971191\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0011926386612764923\n",
      "planning for trajectory step:  5\n",
      "action:  [ 1.8407913  -0.02295416  0.0096942  -0.3736166 ] reward:  -0.3159548551535734\n",
      "planning time:  0.18747472763061523\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.002028649616473763\n",
      "planning for trajectory step:  6\n",
      "action:  [ 3.2231445   0.0168567  -0.01096212  0.26386842] reward:  -0.31743582436785434\n",
      "planning time:  0.18619465827941895\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.003170673237135412\n",
      "planning for trajectory step:  7\n",
      "action:  [ 3.2995222  -0.01711359 -0.02479567 -0.3496292 ] reward:  -0.31595722068442017\n",
      "planning time:  0.18211579322814941\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.004657795105706712\n",
      "planning for trajectory step:  8\n",
      "action:  [ 0.6276861   0.03922772  0.01764401 -0.20923981] reward:  -0.3161321296757189\n",
      "planning time:  0.20131349563598633\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.006525883105351723\n",
      "planning for trajectory step:  9\n",
      "action:  [ 3.3344193  -0.025516   -0.00848841  0.10564791] reward:  -0.31669513960217793\n",
      "planning time:  0.20099639892578125\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.008807663130524624\n",
      "planning for trajectory step:  10\n",
      "action:  [ 3.4252830e+00 -2.6702036e-03  4.2392273e-02  4.5042223e-01] reward:  -0.3146808992889479\n",
      "planning time:  0.1856369972229004\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.011532794797541032\n",
      "planning for trajectory step:  11\n",
      "action:  [ 3.8577898  -0.00529453  0.00864074  0.02271516] reward:  -0.3166785554381435\n",
      "planning time:  0.18625426292419434\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.014727947155149303\n",
      "planning for trajectory step:  12\n",
      "action:  [ 0.2061387   0.04286809 -0.02412399 -0.48209578] reward:  -0.32605416591475844\n",
      "planning time:  0.20169425010681152\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.018416874395101913\n",
      "planning for trajectory step:  13\n",
      "action:  [ 0.7041865   0.02719326 -0.04726895  0.54801637] reward:  -0.3648190405732471\n",
      "planning time:  0.18564963340759277\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.022620491562726712\n",
      "planning for trajectory step:  14\n",
      "action:  [ 3.7682726e+00  3.2074910e-04 -2.8470004e-02 -8.5878953e-02] reward:  -0.35778614525664765\n",
      "planning time:  0.20318388938903809\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.027356950267498275\n",
      "planning for trajectory step:  15\n",
      "action:  [3.6160512e+00 1.8718090e-03 1.9641520e-02 1.9665214e-01] reward:  -0.35947622353924796\n",
      "planning time:  0.18569159507751465\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.03264171439360926\n",
      "planning for trajectory step:  16\n",
      "action:  [ 1.6211089   0.0089963   0.02362153 -0.4728766 ] reward:  -0.36907105936887336\n",
      "planning time:  0.20235300064086914\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.03848763581054164\n",
      "planning for trajectory step:  17\n",
      "action:  [ 3.2796593  -0.04131423 -0.02898568 -0.3441124 ] reward:  -0.4170198605080375\n",
      "planning time:  0.18802928924560547\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.044905030083638116\n",
      "planning for trajectory step:  18\n",
      "action:  [ 3.40606     0.04653783 -0.03161621  0.302383  ] reward:  -0.409077062833986\n",
      "planning time:  0.18810176849365234\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.05190175218467344\n",
      "planning for trajectory step:  19\n",
      "action:  [2.0010176  0.03505249 0.0460454  0.40253586] reward:  -0.3680801328299114\n",
      "planning time:  0.18633770942687988\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.05948327220242562\n",
      "planning for trajectory step:  20\n",
      "action:  [ 2.931387   -0.00748432 -0.02455243 -0.29248154] reward:  -0.36273296660231413\n",
      "planning time:  0.20270490646362305\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0676527510532474\n",
      "planning for trajectory step:  21\n",
      "action:  [ 0.29579315 -0.04487148  0.01289034 -0.440202  ] reward:  -0.3630422634501818\n",
      "planning time:  0.1878952980041504\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.07641111619163748\n",
      "planning for trajectory step:  22\n",
      "action:  [0.00750619 0.04952381 0.04864556 0.53865397] reward:  -0.3630520812065739\n",
      "planning time:  0.2037944793701172\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0857571373208119\n",
      "planning for trajectory step:  23\n",
      "action:  [ 3.68266    -0.02409571 -0.01084079  0.5669282 ] reward:  -0.36302726469022895\n",
      "planning time:  0.1830005645751953\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.09568750210327515\n",
      "planning for trajectory step:  24\n",
      "action:  [ 1.072756   -0.0299141   0.02858379 -0.5362567 ] reward:  -0.36302322207479015\n",
      "planning time:  0.20082640647888184\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.10619689187139202\n",
      "planning for trajectory step:  25\n",
      "action:  [ 0.5648405   0.04120909  0.03187003 -0.2620981 ] reward:  -0.3709856742999321\n",
      "planning time:  0.18469858169555664\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.11727805733795829\n",
      "planning for trajectory step:  26\n",
      "action:  [ 0.7245269  -0.03056279 -0.02774606  0.26160243] reward:  -0.3623015824827866\n",
      "planning time:  0.18650531768798828\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.12892189430677242\n",
      "planning for trajectory step:  27\n",
      "action:  [1.5223879  0.02753559 0.00318225 0.10758411] reward:  -0.36265437544066714\n",
      "planning time:  0.18510675430297852\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.14111751938320677\n",
      "planning for trajectory step:  28\n",
      "action:  [ 3.8996396e+00  3.9138712e-02 -2.4638097e-03  5.6090617e-01] reward:  -0.36267994770276407\n",
      "planning time:  0.20364952087402344\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.15385234568477898\n",
      "planning for trajectory step:  29\n",
      "action:  [ 2.11051    -0.00675838  0.04221013  0.35777843] reward:  -0.36330524975231243\n",
      "planning time:  0.18515276908874512\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.1671121585517233\n",
      "planning for trajectory step:  30\n",
      "action:  [ 0.5475388   0.03301884  0.02538453 -0.47247106] reward:  -0.37497804964015924\n",
      "planning time:  0.2019507884979248\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.1808811912575617\n",
      "planning for trajectory step:  31\n",
      "action:  [ 0.0862113  -0.0398652   0.02042256 -0.0499162 ] reward:  -0.37783969274204804\n",
      "planning time:  0.1705617904663086\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.1951422007196756\n",
      "planning for trajectory step:  32\n",
      "action:  [ 1.090338   -0.02652462 -0.04894418 -0.46225724] reward:  -0.3593311932052156\n",
      "planning time:  0.186018705368042\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.2098765432098766\n",
      "planning for trajectory step:  33\n",
      "action:  [ 2.8279414   0.00487944 -0.0494678  -0.19485779] reward:  -0.3597258869190674\n",
      "planning time:  0.18468689918518066\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.22506425006497865\n",
      "planning for trajectory step:  34\n",
      "action:  [ 3.861548    0.03638794 -0.04971883 -0.4728508 ] reward:  -0.35978317725068365\n",
      "planning time:  0.2024097442626953\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.24068410339736837\n",
      "planning for trajectory step:  35\n",
      "action:  [ 0.37191212 -0.03327963 -0.02950108 -0.01223907] reward:  -0.3598145454750346\n",
      "planning time:  0.23364615440368652\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.25671371180557717\n",
      "planning for trajectory step:  36\n",
      "action:  [ 3.290836   -0.04167752  0.02739811  0.29490557] reward:  -0.3784039926071855\n",
      "planning time:  0.1872234344482422\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.2731295860848524\n",
      "planning for trajectory step:  37\n",
      "action:  [ 2.4707735  -0.04215755 -0.02217512 -0.40307593] reward:  -0.35630590231305337\n",
      "planning time:  0.18581557273864746\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.2899072149377282\n",
      "planning for trajectory step:  38\n",
      "action:  [ 0.0878107  -0.03801502 -0.04075672 -0.43746343] reward:  -0.35830803628956637\n",
      "planning time:  0.18556451797485352\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.3070211406845973\n",
      "planning for trajectory step:  39\n",
      "action:  [1.3365884  0.04868315 0.04622283 0.42488462] reward:  -0.35808113229733907\n",
      "planning time:  0.20226263999938965\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.3244450349742825\n",
      "planning for trajectory step:  40\n",
      "action:  [ 3.2299330e+00  1.1259434e-03 -2.4718419e-02  2.3285270e-01] reward:  -0.3581716546684453\n",
      "planning time:  0.1852550506591797\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.3421517744946073\n",
      "planning for trajectory step:  41\n",
      "action:  [ 0.16778144  0.03748456  0.04441737 -0.29723984] reward:  -0.35820121023776175\n",
      "planning time:  0.218186616897583\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.360113516682968\n",
      "planning for trajectory step:  42\n",
      "action:  [ 0.14176208  0.04234124  0.03456227 -0.46487814] reward:  -0.35812823897537494\n",
      "planning time:  0.18611621856689453\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.37830177543690424\n",
      "planning for trajectory step:  43\n",
      "action:  [0.72365916 0.03765401 0.02264946 0.3559341 ] reward:  -0.3580417561462879\n",
      "planning time:  0.1866753101348877\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.39668749682467125\n",
      "planning for trajectory step:  44\n",
      "action:  [ 3.810857   -0.04367543 -0.00636471  0.42639625] reward:  -0.3580491197579489\n",
      "planning time:  0.18571090698242188\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.41524113479581026\n",
      "planning for trajectory step:  45\n",
      "action:  [ 3.8355489e+00  1.2171489e-02 -9.1497804e-04  3.6726540e-01] reward:  -0.35805415355407005\n",
      "planning time:  0.23011159896850586\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.4339327268917206\n",
      "planning for trajectory step:  46\n",
      "action:  [0.28378257 0.04103357 0.01717035 0.18314894] reward:  -0.3764925330238424\n",
      "planning time:  0.18755578994750977\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.45273196995622983\n",
      "planning for trajectory step:  47\n",
      "action:  [ 2.2542768  -0.04922595 -0.04103069  0.06926713] reward:  -0.43590016218325556\n",
      "planning time:  0.20291614532470703\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.4716082958461666\n",
      "planning for trajectory step:  48\n",
      "action:  [ 2.330139   -0.03368355 -0.0472008   0.2270305 ] reward:  -0.3846759462291917\n",
      "planning time:  0.18881678581237793\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.4905309471419318\n",
      "planning for trajectory step:  49\n",
      "action:  [ 3.3897717  -0.02088923 -0.02749774  0.46624327] reward:  -0.3715164517977759\n",
      "planning time:  0.20334386825561523\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5094690528580682\n",
      "planning for trajectory step:  50\n",
      "action:  [ 1.2618603   0.0416806   0.03961177 -0.5340188 ] reward:  -0.3720616188726445\n",
      "planning time:  0.20437884330749512\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5283917041538334\n",
      "planning for trajectory step:  51\n",
      "action:  [ 3.4021373   0.04356376 -0.02474451  0.5602926 ] reward:  -0.37340835032939235\n",
      "planning time:  0.16987156867980957\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5472680300437707\n",
      "planning for trajectory step:  52\n",
      "action:  [ 0.21233323 -0.04041941 -0.03350613 -0.29618555] reward:  -0.3716658069076964\n",
      "planning time:  0.18436837196350098\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5660672731082802\n",
      "planning for trajectory step:  53\n",
      "action:  [3.8099346  0.04288838 0.0442715  0.5765958 ] reward:  -0.37205938929720384\n",
      "planning time:  0.1865711212158203\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5847588652041902\n",
      "planning for trajectory step:  54\n",
      "action:  [ 2.9487085  -0.02168375 -0.02047812 -0.05663146] reward:  -0.37194169373726504\n",
      "planning time:  0.2000110149383545\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.603312503175329\n",
      "planning for trajectory step:  55\n",
      "action:  [ 2.0824523   0.01393779 -0.00907493 -0.5061452 ] reward:  -0.3931587308401345\n",
      "planning time:  0.18633794784545898\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.621698224563096\n",
      "planning for trajectory step:  56\n",
      "action:  [ 1.9877391  -0.04834002 -0.04124591  0.29235962] reward:  -0.4341545742175957\n",
      "planning time:  0.18732261657714844\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.6398864833170326\n",
      "planning for trajectory step:  57\n",
      "action:  [ 3.6900749e+00 -1.8400837e-03 -4.1732159e-02 -2.7941942e-01] reward:  -0.5417829354461221\n",
      "planning time:  0.18763017654418945\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.657848225505393\n",
      "planning for trajectory step:  58\n",
      "action:  [ 0.47929844  0.04562281 -0.03520267 -0.5065959 ] reward:  -0.5883622133960407\n",
      "planning time:  0.1856403350830078\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.6755549650257178\n",
      "planning for trajectory step:  59\n",
      "action:  [ 0.2251756  -0.01876712 -0.03845616  0.57485104] reward:  -0.6356684608418776\n",
      "planning time:  0.18504047393798828\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.6929788593154029\n",
      "planning for trajectory step:  60\n",
      "Crash detected!\n",
      "Crash detected!\n",
      "action:  [ 2.0239265  -0.02122435  0.03866742 -0.43333778] reward:  -0.6672469601051715\n",
      "planning time:  0.20097947120666504\n",
      "resetting environment, and starting trial : 13\n",
      "Connected!\n",
      "4 DoF [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "No propeller. Is it a magic box?\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 1.0150579389358555e-05\n",
      "Number of stored transitions:  10000\n",
      "Training model\n",
      "training loss:  -43.67477020161264\n",
      "validation loss:  0.022248711436986923\n",
      "training time:  46.76439952850342\n",
      "Model trained\n",
      "planning for trajectory step:  1\n",
      "action:  [1.1595827  0.0312047  0.00815058 0.02311035] reward:  -0.31590003914941683\n",
      "planning time:  0.18680906295776367\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 7.997055280231069e-05\n",
      "planning for trajectory step:  2\n",
      "action:  [ 1.2412542  -0.03073807  0.01815743 -0.41739905] reward:  -0.3159074803959587\n",
      "planning time:  0.2032792568206787\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.00026576965766001126\n",
      "planning for trajectory step:  3\n",
      "action:  [ 0.5020307   0.04479272  0.00353974 -0.12294209] reward:  -0.3159196242840048\n",
      "planning time:  0.19902348518371582\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0006202613792457937\n",
      "planning for trajectory step:  4\n",
      "action:  [ 2.8791666e+00  2.8851980e-02  1.8135457e-03 -1.7630599e-02] reward:  -0.31591562803870504\n",
      "planning time:  0.18867778778076172\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0011926386612764923\n",
      "planning for trajectory step:  5\n",
      "action:  [ 2.5840058   0.04504467  0.01613097 -0.34700346] reward:  -0.31590775042377517\n",
      "planning time:  0.18319320678710938\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.002028649616473763\n",
      "planning for trajectory step:  6\n",
      "action:  [ 1.2695183e+00  4.5257431e-02 -3.6023602e-02  9.6473680e-04] reward:  -0.3159513619120074\n",
      "planning time:  0.18731164932250977\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.003170673237135412\n",
      "planning for trajectory step:  7\n",
      "action:  [ 3.620917    0.00762368 -0.01141375  0.18185636] reward:  -0.31593706484786377\n",
      "planning time:  0.1712043285369873\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.004657795105706712\n",
      "planning for trajectory step:  8\n",
      "action:  [ 3.6389003   0.04685928 -0.01942924  0.2963279 ] reward:  -0.3159229946583176\n",
      "planning time:  0.1854710578918457\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.006525883105351723\n",
      "planning for trajectory step:  9\n",
      "action:  [ 0.42573196 -0.04074633 -0.02929935 -0.00263599] reward:  -0.32355316860319033\n",
      "planning time:  0.20164895057678223\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.008807663130524624\n",
      "planning for trajectory step:  10\n",
      "action:  [ 3.8509011   0.00985162 -0.03985714  0.57501775] reward:  -0.34834922980486943\n",
      "planning time:  0.1865098476409912\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.011532794797541032\n",
      "planning for trajectory step:  11\n",
      "action:  [3.2073932  0.01337992 0.01303361 0.00998694] reward:  -0.3165666317658709\n",
      "planning time:  0.2017972469329834\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.014727947155149303\n",
      "planning for trajectory step:  12\n",
      "action:  [ 0.33152848  0.04543734 -0.01529315 -0.4791553 ] reward:  -0.36438100362342446\n",
      "planning time:  0.1892836093902588\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.018416874395101913\n",
      "planning for trajectory step:  13\n",
      "action:  [ 3.2828119  -0.04226954  0.02658111 -0.36759433] reward:  -0.46816527468970237\n",
      "planning time:  0.1848750114440918\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.022620491562726712\n",
      "planning for trajectory step:  14\n",
      "action:  [ 2.7960148  -0.04859033 -0.03672729  0.30116397] reward:  -0.48968572670522686\n",
      "planning time:  0.21717238426208496\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.027356950267498275\n",
      "planning for trajectory step:  15\n",
      "action:  [ 0.07152805  0.03017845  0.04878235 -0.40606374] reward:  -0.5318197669945908\n",
      "planning time:  0.21547579765319824\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.03264171439360926\n",
      "planning for trajectory step:  16\n",
      "action:  [ 1.1609145   0.01482235 -0.01990232  0.25131953] reward:  -0.5416427651219027\n",
      "planning time:  0.2176814079284668\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.03848763581054164\n",
      "planning for trajectory step:  17\n",
      "action:  [ 1.0257279   0.04369855 -0.04039721  0.44503045] reward:  -0.4946276016635657\n",
      "planning time:  0.20194435119628906\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.044905030083638116\n",
      "planning for trajectory step:  18\n",
      "action:  [1.8104838  0.03269364 0.04583111 0.00688628] reward:  -0.494632556014432\n",
      "planning time:  0.23077774047851562\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.05190175218467344\n",
      "planning for trajectory step:  19\n",
      "action:  [ 0.28616118  0.0225371  -0.04763851 -0.55073214] reward:  -0.48595388659401745\n",
      "planning time:  0.2198619842529297\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.05948327220242562\n",
      "planning for trajectory step:  20\n",
      "action:  [ 0.73719513  0.0431702   0.03666733 -0.12375404] reward:  -0.5155375400517657\n",
      "planning time:  0.21465635299682617\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0676527510532474\n",
      "planning for trajectory step:  21\n",
      "action:  [ 3.3664486   0.04447113 -0.04827638 -0.07762917] reward:  -0.4711547690388266\n",
      "planning time:  0.1986691951751709\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.07641111619163748\n",
      "planning for trajectory step:  22\n",
      "action:  [ 3.4152327   0.03420395 -0.03989435 -0.5859051 ] reward:  -0.47026019479595416\n",
      "planning time:  0.20254898071289062\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0857571373208119\n",
      "planning for trajectory step:  23\n",
      "action:  [ 0.9252376   0.00764248 -0.03570643  0.52543795] reward:  -0.4279661780096337\n",
      "planning time:  0.18704533576965332\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.09568750210327515\n",
      "planning for trajectory step:  24\n",
      "action:  [ 1.415911   -0.04817107 -0.04924224 -0.06494414] reward:  -0.4266165903751171\n",
      "planning time:  0.18732237815856934\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.10619689187139202\n",
      "planning for trajectory step:  25\n",
      "action:  [ 1.8447398  -0.0398911  -0.04869867 -0.08189391] reward:  -0.44195928381739613\n",
      "planning time:  0.18686175346374512\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.11727805733795829\n",
      "planning for trajectory step:  26\n",
      "action:  [3.8745704  0.03046288 0.03299621 0.12637994] reward:  -0.430384132862262\n",
      "planning time:  0.20324182510375977\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.12892189430677242\n",
      "planning for trajectory step:  27\n",
      "action:  [ 0.9609816  -0.04217469 -0.03001985 -0.5966739 ] reward:  -0.41393342116328125\n",
      "planning time:  0.19970035552978516\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.14111751938320677\n",
      "planning for trajectory step:  28\n",
      "action:  [ 0.3226879   0.00591849 -0.04552402 -0.50340027] reward:  -0.46374158729595727\n",
      "planning time:  0.1832587718963623\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.15385234568477898\n",
      "planning for trajectory step:  29\n",
      "action:  [ 0.07625606 -0.03750956  0.04092653  0.5608783 ] reward:  -0.5124755116532966\n",
      "planning time:  0.2024681568145752\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.1671121585517233\n",
      "planning for trajectory step:  30\n",
      "action:  [ 3.9847658  -0.02318381  0.02132028 -0.3550293 ] reward:  -0.4749402890393672\n",
      "planning time:  0.1837766170501709\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.1808811912575617\n",
      "planning for trajectory step:  31\n",
      "action:  [ 3.3884413  -0.01082213 -0.00383608  0.49650246] reward:  -0.4760680495294211\n",
      "planning time:  0.18710803985595703\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.1951422007196756\n",
      "planning for trajectory step:  32\n",
      "action:  [1.6604806  0.03402947 0.01086612 0.5299158 ] reward:  -0.492004657761216\n",
      "planning time:  0.18783140182495117\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.2098765432098766\n",
      "planning for trajectory step:  33\n",
      "action:  [ 3.2111423   0.00557043 -0.03366793  0.5074525 ] reward:  -0.6264518544956\n",
      "planning time:  0.2014143466949463\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.22506425006497865\n",
      "planning for trajectory step:  34\n",
      "action:  [ 1.7695953   0.03413323  0.04469754 -0.51616937] reward:  -0.791389228215776\n",
      "planning time:  0.19851016998291016\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.24068410339736837\n",
      "planning for trajectory step:  35\n",
      "action:  [ 0.03233657  0.01651309 -0.03854389  0.25426698] reward:  -0.9387510835171708\n",
      "planning time:  0.18654394149780273\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.25671371180557717\n",
      "planning for trajectory step:  36\n",
      "action:  [ 3.3182278   0.02380209 -0.04743861 -0.24011384] reward:  -0.9037287458070662\n",
      "planning time:  0.18468999862670898\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.2731295860848524\n",
      "planning for trajectory step:  37\n",
      "action:  [ 0.18863367  0.0430553  -0.04697093 -0.12781717] reward:  -0.8868666908889169\n",
      "planning time:  0.20050811767578125\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.2899072149377282\n",
      "planning for trajectory step:  38\n",
      "action:  [ 0.26920477  0.04789974  0.03247092 -0.34169194] reward:  -0.8765156989264632\n",
      "planning time:  0.17224502563476562\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.3070211406845973\n",
      "planning for trajectory step:  39\n",
      "action:  [ 0.45528904 -0.04835202 -0.03621671 -0.4693183 ] reward:  -0.8653215367576361\n",
      "planning time:  0.20142173767089844\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.3244450349742825\n",
      "planning for trajectory step:  40\n",
      "action:  [ 3.4742095  -0.03698906  0.01915786  0.49062428] reward:  -0.8662774347796017\n",
      "planning time:  0.1833808422088623\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.3421517744946073\n",
      "planning for trajectory step:  41\n",
      "action:  [ 3.156873   -0.02492686 -0.03966444  0.56329054] reward:  -0.886063784095396\n",
      "planning time:  0.18569517135620117\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.360113516682968\n",
      "planning for trajectory step:  42\n",
      "action:  [ 3.772362    0.04098704 -0.04653326  0.4881185 ] reward:  -0.9192723662558618\n",
      "planning time:  0.18284845352172852\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.37830177543690424\n",
      "planning for trajectory step:  43\n",
      "action:  [3.709953   0.02874128 0.02043931 0.5443991 ] reward:  -0.8613400251392808\n",
      "planning time:  0.1862633228302002\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.39668749682467125\n",
      "planning for trajectory step:  44\n",
      "action:  [ 2.1322472   0.04540715  0.04734965 -0.54526204] reward:  -0.6538278475299946\n",
      "planning time:  0.1851482391357422\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.41524113479581026\n",
      "planning for trajectory step:  45\n",
      "Crash detected!\n",
      "Crash detected!\n",
      "action:  [ 3.896449    0.03178492 -0.0207605  -0.1406655 ] reward:  -0.5939951907655021\n",
      "planning time:  0.20057439804077148\n",
      "resetting environment, and starting trial : 14\n",
      "Connected!\n",
      "4 DoF [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "No propeller. Is it a magic box?\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 1.0150579389358555e-05\n",
      "Number of stored transitions:  10000\n",
      "Training model\n",
      "training loss:  -44.054214259922105\n",
      "validation loss:  0.021796464920043945\n",
      "training time:  63.01815414428711\n",
      "Model trained\n",
      "planning for trajectory step:  1\n",
      "action:  [0.6763409  0.02734292 0.00532946 0.19266386] reward:  -0.315900039152819\n",
      "planning time:  0.34442639350891113\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 7.997055280231069e-05\n",
      "planning for trajectory step:  2\n",
      "action:  [ 3.3710527e+00  1.2270626e-02  4.1414527e-03 -3.2907664e-03] reward:  -0.31589345565453497\n",
      "planning time:  0.3101003170013428\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.00026576965766001126\n",
      "planning for trajectory step:  3\n",
      "action:  [ 1.0126326  -0.01233679  0.01638538  0.03478235] reward:  -0.31601837508955916\n",
      "planning time:  0.26149892807006836\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0006202613792457937\n",
      "planning for trajectory step:  4\n",
      "action:  [ 1.080245    0.03687898  0.03686325 -0.44912893] reward:  -0.3210657627706517\n",
      "planning time:  0.23434996604919434\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0011926386612764923\n",
      "planning for trajectory step:  5\n",
      "action:  [ 3.2689474   0.02728661 -0.03578016 -0.41064596] reward:  -0.3149335836358488\n",
      "planning time:  0.26091837882995605\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.002028649616473763\n",
      "planning for trajectory step:  6\n",
      "action:  [ 3.3167582e+00 -2.3103198e-03 -1.2456458e-02 -1.4218576e-01] reward:  -0.31511488559522904\n",
      "planning time:  0.2318878173828125\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.003170673237135412\n",
      "planning for trajectory step:  7\n",
      "action:  [ 1.2931964   0.00750308 -0.0252209  -0.4050236 ] reward:  -0.31579686533752943\n",
      "planning time:  0.20419692993164062\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.004657795105706712\n",
      "planning for trajectory step:  8\n",
      "action:  [ 3.320676   -0.00584891  0.02319616 -0.3886741 ] reward:  -0.31316665377800074\n",
      "planning time:  0.20314288139343262\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.006525883105351723\n",
      "planning for trajectory step:  9\n",
      "action:  [3.600434   0.02527549 0.01464045 0.50351894] reward:  -0.30909364818731483\n",
      "planning time:  0.21977496147155762\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.008807663130524624\n",
      "planning for trajectory step:  10\n",
      "action:  [ 3.9383214e+00 -2.7141620e-03 -4.3531947e-02  1.6095960e-01] reward:  -0.3120221657789713\n",
      "planning time:  0.18829655647277832\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.011532794797541032\n",
      "planning for trajectory step:  11\n",
      "action:  [ 3.2316618e+00 -1.2624017e-02 -2.0931277e-03  3.0394146e-01] reward:  -0.32901983441903543\n",
      "planning time:  0.20198607444763184\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.014727947155149303\n",
      "planning for trajectory step:  12\n",
      "action:  [0.63467497 0.03088112 0.04107169 0.21937744] reward:  -0.40834454565733425\n",
      "planning time:  0.199066162109375\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.018416874395101913\n",
      "planning for trajectory step:  13\n",
      "action:  [ 1.1394908   0.02713331  0.0470802  -0.36014396] reward:  -0.5613154271744893\n",
      "planning time:  0.20420265197753906\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.022620491562726712\n",
      "planning for trajectory step:  14\n",
      "action:  [3.609794   0.00994391 0.04417787 0.25469992] reward:  -0.5872850559834625\n",
      "planning time:  0.21866250038146973\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.027356950267498275\n",
      "planning for trajectory step:  15\n",
      "action:  [ 1.8045502   0.01119342 -0.02379234 -0.5980747 ] reward:  -0.5853398267136954\n",
      "planning time:  0.20355606079101562\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.03264171439360926\n",
      "planning for trajectory step:  16\n",
      "action:  [3.7201722  0.04424435 0.03025202 0.2867468 ] reward:  -0.5927531169746881\n",
      "planning time:  0.1998882293701172\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.03848763581054164\n",
      "planning for trajectory step:  17\n",
      "action:  [ 0.37332398  0.04102622 -0.0375666  -0.03330471] reward:  -0.5854427842964589\n",
      "planning time:  0.20178961753845215\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.044905030083638116\n",
      "planning for trajectory step:  18\n",
      "action:  [ 1.1614385   0.04564344 -0.01904803 -0.3057506 ] reward:  -0.5849345622975405\n",
      "planning time:  0.18536162376403809\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.05190175218467344\n",
      "planning for trajectory step:  19\n",
      "action:  [ 3.8832166   0.04853005  0.04791496 -0.5621351 ] reward:  -0.5845057840509957\n",
      "planning time:  0.20298337936401367\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.05948327220242562\n",
      "planning for trajectory step:  20\n",
      "action:  [ 2.0007093   0.02503236  0.03681665 -0.21896607] reward:  -0.591279606405306\n",
      "planning time:  0.18517637252807617\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0676527510532474\n",
      "planning for trajectory step:  21\n",
      "action:  [ 2.7381945  -0.02104032  0.0416273   0.59453493] reward:  -0.6187346764511098\n",
      "planning time:  0.1885972023010254\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.07641111619163748\n",
      "planning for trajectory step:  22\n",
      "action:  [2.6871767  0.01324378 0.00479307 0.47182876] reward:  -0.6408155944891877\n",
      "planning time:  0.18619585037231445\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0857571373208119\n",
      "planning for trajectory step:  23\n",
      "action:  [ 3.7041755  -0.03868265 -0.04546955  0.5857456 ] reward:  -0.597799968300421\n",
      "planning time:  0.20396661758422852\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.09568750210327515\n",
      "planning for trajectory step:  24\n",
      "action:  [ 1.5995753  -0.04252061 -0.03257968 -0.22330688] reward:  -0.5250638553917784\n",
      "planning time:  0.18789267539978027\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.10619689187139202\n",
      "planning for trajectory step:  25\n",
      "action:  [ 0.0340459   0.04428524 -0.04180865 -0.5497722 ] reward:  -0.5562927715994169\n",
      "planning time:  0.20179510116577148\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.11727805733795829\n",
      "planning for trajectory step:  26\n",
      "action:  [0.14504021 0.04445405 0.0125028  0.32489944] reward:  -0.5645423373984293\n",
      "planning time:  0.19879865646362305\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.12892189430677242\n",
      "planning for trajectory step:  27\n",
      "action:  [3.230957   0.02658431 0.01191342 0.30894163] reward:  -0.5507898710125777\n",
      "planning time:  0.20273065567016602\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.14111751938320677\n",
      "planning for trajectory step:  28\n",
      "action:  [ 0.6655365   0.03929589 -0.04732939 -0.4578936 ] reward:  -0.5796188086767453\n",
      "planning time:  0.18439030647277832\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.15385234568477898\n",
      "planning for trajectory step:  29\n",
      "action:  [ 3.4611235   0.04530341 -0.03263991 -0.5526552 ] reward:  -0.623437329508099\n",
      "planning time:  0.20385503768920898\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.1671121585517233\n",
      "planning for trajectory step:  30\n",
      "action:  [ 3.7398996   0.04707636 -0.03944338  0.5294907 ] reward:  -0.6599984002479389\n",
      "planning time:  0.21353745460510254\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.1808811912575617\n",
      "planning for trajectory step:  31\n",
      "action:  [ 3.7843509  -0.0422911  -0.03707493 -0.39745247] reward:  -0.6398880812651452\n",
      "planning time:  0.1987462043762207\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.1951422007196756\n",
      "planning for trajectory step:  32\n",
      "action:  [ 0.19861564  0.0133494   0.04559107 -0.28041887] reward:  -0.7288505681407967\n",
      "planning time:  0.19934749603271484\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.2098765432098766\n",
      "planning for trajectory step:  33\n",
      "action:  [ 1.1739858  -0.00612967  0.04110459  0.05884081] reward:  -0.971166871312024\n",
      "planning time:  0.20621776580810547\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.22506425006497865\n",
      "planning for trajectory step:  34\n",
      "action:  [ 2.7368386  -0.04565868  0.04764519  0.49400008] reward:  -1.2079691768388734\n",
      "planning time:  0.20372605323791504\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.24068410339736837\n",
      "planning for trajectory step:  35\n",
      "action:  [ 0.36866975  0.04786362 -0.03882021  0.49581507] reward:  -1.3144069543877681\n",
      "planning time:  0.21644043922424316\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.25671371180557717\n",
      "planning for trajectory step:  36\n",
      "action:  [ 0.16121909 -0.04286827  0.01166494 -0.55212927] reward:  -1.4171520158125084\n",
      "planning time:  0.20090579986572266\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.2731295860848524\n",
      "planning for trajectory step:  37\n",
      "action:  [ 0.23332906 -0.01033006  0.01311133  0.57765865] reward:  -1.5918134347826378\n",
      "planning time:  0.21375679969787598\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.2899072149377282\n",
      "planning for trajectory step:  38\n",
      "Crash detected!\n",
      "Crash detected!\n",
      "action:  [ 0.0984296  -0.02121072  0.03582588 -0.4140394 ] reward:  -1.6326131360436011\n",
      "planning time:  0.2160019874572754\n",
      "resetting environment, and starting trial : 15\n",
      "Connected!\n",
      "4 DoF [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "No propeller. Is it a magic box?\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 1.0150579389358555e-05\n",
      "Number of stored transitions:  10000\n",
      "Training model\n",
      "training loss:  -43.577417796090145\n",
      "validation loss:  0.013330812565982342\n",
      "training time:  58.6339225769043\n",
      "Model trained\n",
      "planning for trajectory step:  1\n",
      "action:  [ 2.9400702  -0.01882357  0.03714883  0.04853813] reward:  -0.315900039152819\n",
      "planning time:  0.27191615104675293\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 7.997055280231069e-05\n",
      "planning for trajectory step:  2\n",
      "action:  [ 2.497734    0.01190403 -0.02467699 -0.1977637 ] reward:  -0.31589944591917757\n",
      "planning time:  0.23417091369628906\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.00026576965766001126\n",
      "planning for trajectory step:  3\n",
      "action:  [ 1.3092594   0.00482497 -0.01261354 -0.25004253] reward:  -0.315950903296436\n",
      "planning time:  0.2514162063598633\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0006202613792457937\n",
      "planning for trajectory step:  4\n",
      "action:  [ 2.1485682   0.02796169  0.01136715 -0.39278725] reward:  -0.315947660126244\n",
      "planning time:  0.27491021156311035\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0011926386612764923\n",
      "planning for trajectory step:  5\n",
      "action:  [ 1.4347571   0.01762822  0.023327   -0.39281383] reward:  -0.315923585394416\n",
      "planning time:  0.23171019554138184\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.002028649616473763\n",
      "planning for trajectory step:  6\n",
      "action:  [3.4683907  0.00756849 0.0157805  0.3433978 ] reward:  -0.31593280290984366\n",
      "planning time:  0.23494982719421387\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.003170673237135412\n",
      "planning for trajectory step:  7\n",
      "action:  [ 3.2365174   0.02259432 -0.01963503  0.12103508] reward:  -0.3159255050287172\n",
      "planning time:  0.2609841823577881\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.004657795105706712\n",
      "planning for trajectory step:  8\n",
      "action:  [ 2.5954032e+00  2.7265580e-02 -1.6364863e-03 -4.4972491e-01] reward:  -0.32616195206371884\n",
      "planning time:  0.2648890018463135\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.006525883105351723\n",
      "planning for trajectory step:  9\n",
      "action:  [ 3.135375    0.01562689  0.03163159 -0.30134758] reward:  -0.3423656861278772\n",
      "planning time:  0.24759459495544434\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.008807663130524624\n",
      "planning for trajectory step:  10\n",
      "action:  [ 2.7224753   0.01772284  0.02882132 -0.39753816] reward:  -0.3218886021857483\n",
      "planning time:  0.23209238052368164\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.011532794797541032\n",
      "planning for trajectory step:  11\n",
      "action:  [ 0.97840333  0.0073044   0.02994967 -0.16686374] reward:  -0.2849154305557705\n",
      "planning time:  0.24973773956298828\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.014727947155149303\n",
      "planning for trajectory step:  12\n",
      "action:  [ 0.23442385 -0.03710468  0.04244652 -0.38558918] reward:  -0.28187241521847906\n",
      "planning time:  0.24586057662963867\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.018416874395101913\n",
      "planning for trajectory step:  13\n",
      "action:  [ 3.3412104e+00 -1.6910157e-03 -6.7064702e-03  1.3414407e-02] reward:  -0.2785271900344619\n",
      "planning time:  0.24559640884399414\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.022620491562726712\n",
      "planning for trajectory step:  14\n",
      "action:  [ 3.9195085  -0.00816886  0.03964234 -0.16842087] reward:  -0.2785648712890275\n",
      "planning time:  0.21609878540039062\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.027356950267498275\n",
      "planning for trajectory step:  15\n",
      "action:  [ 3.2248914e+00  5.5030878e-03 -2.4733595e-03  1.0791127e-01] reward:  -0.28271701006777145\n",
      "planning time:  0.2790238857269287\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.03264171439360926\n",
      "planning for trajectory step:  16\n",
      "action:  [ 2.6491666   0.01287004 -0.01331879  0.02421225] reward:  -0.3376394272354053\n",
      "planning time:  0.2813107967376709\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.03848763581054164\n",
      "planning for trajectory step:  17\n",
      "action:  [0.10329707 0.02870514 0.0441758  0.5832618 ] reward:  -0.4943276617401925\n",
      "planning time:  0.2345583438873291\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.044905030083638116\n",
      "planning for trajectory step:  18\n",
      "action:  [ 3.195913   -0.04886797  0.04572845  0.53892934] reward:  -0.5788340557822899\n",
      "planning time:  0.2502014636993408\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.05190175218467344\n",
      "planning for trajectory step:  19\n",
      "action:  [ 1.9539368   0.00478131 -0.03539507  0.56984085] reward:  -0.4000773997891249\n",
      "planning time:  0.23199963569641113\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.05948327220242562\n",
      "planning for trajectory step:  20\n",
      "action:  [ 0.39043212 -0.04808955  0.00855867 -0.36791316] reward:  -0.4236322925986749\n",
      "planning time:  0.24808597564697266\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0676527510532474\n",
      "planning for trajectory step:  21\n",
      "action:  [ 3.9757354  -0.0181088   0.04664158  0.3251622 ] reward:  -0.42780682460819025\n",
      "planning time:  0.21861624717712402\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.07641111619163748\n",
      "planning for trajectory step:  22\n",
      "action:  [ 2.1139016  -0.03429428  0.01058127 -0.4615525 ] reward:  -0.4291229376318687\n",
      "planning time:  0.21678709983825684\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0857571373208119\n",
      "planning for trajectory step:  23\n",
      "action:  [ 0.09712993  0.03195369 -0.03349881 -0.55736035] reward:  -0.469480180851782\n",
      "planning time:  0.27692174911499023\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.09568750210327515\n",
      "planning for trajectory step:  24\n",
      "action:  [ 1.0540713   0.04976417 -0.02505495 -0.45031098] reward:  -0.6346734674817394\n",
      "planning time:  0.23132848739624023\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.10619689187139202\n",
      "planning for trajectory step:  25\n",
      "action:  [ 3.840822    0.04159184  0.02590405 -0.52103394] reward:  -0.7523855592279884\n",
      "planning time:  0.2500801086425781\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.11727805733795829\n",
      "planning for trajectory step:  26\n",
      "action:  [ 3.6756754   0.0338486   0.0485757  -0.40293723] reward:  -0.7537972102063369\n",
      "planning time:  0.2319931983947754\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.12892189430677242\n",
      "planning for trajectory step:  27\n",
      "action:  [0.4259352  0.04786572 0.03559891 0.56524897] reward:  -0.7509185751795996\n",
      "planning time:  0.2338860034942627\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.14111751938320677\n",
      "planning for trajectory step:  28\n",
      "action:  [ 0.38095295 -0.03613031 -0.04636576  0.592919  ] reward:  -0.6737460340638544\n",
      "planning time:  0.24603939056396484\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.15385234568477898\n",
      "planning for trajectory step:  29\n",
      "action:  [2.288662   0.02747612 0.04514262 0.53451955] reward:  -0.42411966301311227\n",
      "planning time:  0.2445065975189209\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.1671121585517233\n",
      "planning for trajectory step:  30\n",
      "action:  [ 0.15430485  0.0447727  -0.04912967 -0.57907957] reward:  -0.33568382978636574\n",
      "planning time:  0.23451852798461914\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.1808811912575617\n",
      "planning for trajectory step:  31\n",
      "action:  [ 0.83847106 -0.04969533 -0.03293057  0.54664904] reward:  -0.3215376970162562\n",
      "planning time:  0.2494487762451172\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.1951422007196756\n",
      "planning for trajectory step:  32\n",
      "action:  [ 0.13815592  0.04445091 -0.03892169  0.49896628] reward:  -0.319856674653132\n",
      "planning time:  0.23084712028503418\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.2098765432098766\n",
      "planning for trajectory step:  33\n",
      "action:  [ 0.11759531 -0.03878466  0.03967002  0.2972551 ] reward:  -0.31989773573259633\n",
      "planning time:  0.24650216102600098\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.22506425006497865\n",
      "planning for trajectory step:  34\n",
      "action:  [ 2.2317603   0.04244947  0.03449106 -0.5716664 ] reward:  -0.319990430550341\n",
      "planning time:  0.23444437980651855\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.24068410339736837\n",
      "planning for trajectory step:  35\n",
      "action:  [ 2.9859443  -0.02643128 -0.03721485 -0.36023155] reward:  -0.32000618394272834\n",
      "planning time:  0.2349398136138916\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.25671371180557717\n",
      "planning for trajectory step:  36\n",
      "action:  [ 0.95139503  0.03981122 -0.02272513 -0.1997228 ] reward:  -0.3200530338701128\n",
      "planning time:  0.24513840675354004\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.2731295860848524\n",
      "planning for trajectory step:  37\n",
      "action:  [ 2.8666534  -0.01847879  0.04772343 -0.58152694] reward:  -0.32004681281243785\n",
      "planning time:  0.23178601264953613\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.2899072149377282\n",
      "planning for trajectory step:  38\n",
      "action:  [ 3.966346   -0.03616694 -0.01471309  0.5748827 ] reward:  -0.3200241546555305\n",
      "planning time:  0.23252606391906738\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.3070211406845973\n",
      "planning for trajectory step:  39\n",
      "action:  [ 3.942394   -0.03173676 -0.01364101  0.516289  ] reward:  -0.32006018463788544\n",
      "planning time:  0.23017549514770508\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.3244450349742825\n",
      "planning for trajectory step:  40\n",
      "action:  [ 1.2402215   0.02546934 -0.03496746  0.44574562] reward:  -0.34607110806521596\n",
      "planning time:  0.2336137294769287\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.3421517744946073\n",
      "planning for trajectory step:  41\n",
      "action:  [ 0.23861049 -0.04586399  0.03034133  0.5082405 ] reward:  -0.43234307607827077\n",
      "planning time:  0.2326359748840332\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.360113516682968\n",
      "planning for trajectory step:  42\n",
      "action:  [ 0.86454755  0.03210906 -0.04189532  0.32653126] reward:  -0.45507553162928477\n",
      "planning time:  0.2497091293334961\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.37830177543690424\n",
      "planning for trajectory step:  43\n",
      "action:  [ 2.2599707   0.03466351 -0.03252106 -0.32566512] reward:  -0.26323647235617537\n",
      "planning time:  0.26300644874572754\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.39668749682467125\n",
      "planning for trajectory step:  44\n",
      "action:  [ 0.05006288  0.04799877 -0.04768866 -0.5245011 ] reward:  -0.2652583605034422\n",
      "planning time:  0.2808568477630615\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.41524113479581026\n",
      "planning for trajectory step:  45\n",
      "action:  [ 1.1139362   0.03967176 -0.03809903 -0.10876777] reward:  -0.2653448211556225\n",
      "planning time:  0.5892386436462402\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.4339327268917206\n",
      "planning for trajectory step:  46\n",
      "action:  [ 1.5269649   0.03391701  0.04676809 -0.01107666] reward:  -0.26530265531959957\n",
      "planning time:  0.23168420791625977\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.45273196995622983\n",
      "planning for trajectory step:  47\n",
      "action:  [3.8784034e+00 9.9436250e-03 1.1045928e-03 4.1504189e-01] reward:  -0.26531498945451004\n",
      "planning time:  0.2478504180908203\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.4716082958461666\n",
      "planning for trajectory step:  48\n",
      "action:  [3.8678634  0.02765222 0.0437851  0.47393426] reward:  -0.26532535858936823\n",
      "planning time:  0.24842333793640137\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.4905309471419318\n",
      "planning for trajectory step:  49\n",
      "action:  [ 3.203208e+00  3.414777e-02 -8.559711e-04 -5.521628e-01] reward:  -0.2924937110210594\n",
      "planning time:  0.2498624324798584\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5094690528580682\n",
      "planning for trajectory step:  50\n",
      "action:  [ 0.3278066   0.04488066 -0.03563034 -0.1465859 ] reward:  -0.4174067898420352\n",
      "planning time:  0.23359894752502441\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5283917041538334\n",
      "planning for trajectory step:  51\n",
      "action:  [ 2.897756   -0.03086078 -0.04707536  0.10081602] reward:  -0.6217360896778867\n",
      "planning time:  0.2640879154205322\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5472680300437707\n",
      "planning for trajectory step:  52\n",
      "action:  [ 0.09462167  0.01868622 -0.03470527 -0.28252023] reward:  -0.515650465991792\n",
      "planning time:  0.24899721145629883\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5660672731082802\n",
      "planning for trajectory step:  53\n",
      "action:  [ 0.1975263  -0.01628705  0.03827618 -0.02970027] reward:  -0.3629235936180565\n",
      "planning time:  0.23385310173034668\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5847588652041902\n",
      "planning for trajectory step:  54\n",
      "action:  [2.0186086  0.01573175 0.04859328 0.14119245] reward:  -0.3616208711940972\n",
      "planning time:  0.230452299118042\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.603312503175329\n",
      "planning for trajectory step:  55\n",
      "action:  [ 0.83992463 -0.04512365  0.04007339 -0.20758851] reward:  -0.36160933098926684\n",
      "planning time:  0.2661921977996826\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.621698224563096\n",
      "planning for trajectory step:  56\n",
      "action:  [ 2.4329615  -0.00486297 -0.02330646  0.35810813] reward:  -0.3616395555891787\n",
      "planning time:  0.26371073722839355\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.6398864833170326\n",
      "planning for trajectory step:  57\n",
      "action:  [ 0.04562726  0.04851397 -0.02194897 -0.5377345 ] reward:  -0.3616401136318249\n",
      "planning time:  0.2312026023864746\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.657848225505393\n",
      "planning for trajectory step:  58\n",
      "action:  [ 1.8107835  -0.04228602 -0.04546253 -0.58584285] reward:  -0.3616554036728966\n",
      "planning time:  0.24771690368652344\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.6755549650257178\n",
      "planning for trajectory step:  59\n",
      "action:  [1.0093416e+00 3.8583525e-02 7.4079144e-04 5.8819807e-01] reward:  -0.3616283875214486\n",
      "planning time:  0.24819326400756836\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.6929788593154029\n",
      "planning for trajectory step:  60\n",
      "action:  [ 0.8288077  -0.03008346 -0.04017601 -0.5526785 ] reward:  -0.3616384883262246\n",
      "planning time:  0.2504239082336426\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.7100927850622721\n",
      "planning for trajectory step:  61\n",
      "action:  [ 0.25023648  0.00780049 -0.02365854 -0.5783356 ] reward:  -0.3616302356603143\n",
      "planning time:  0.23160147666931152\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.7268704139151478\n",
      "planning for trajectory step:  62\n",
      "action:  [0.05279058 0.00539007 0.04773327 0.5848043 ] reward:  -0.3616287048703738\n",
      "planning time:  0.23364663124084473\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.7432862881944238\n",
      "planning for trajectory step:  63\n",
      "action:  [ 1.3898333   0.0478282  -0.04009335 -0.17074229] reward:  -0.3616176877327183\n",
      "planning time:  0.24655389785766602\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.7593158966026328\n",
      "planning for trajectory step:  64\n",
      "action:  [ 0.36731088  0.04643919 -0.04302376 -0.108269  ] reward:  -0.36162617974533046\n",
      "planning time:  0.22936344146728516\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.7749357499350218\n",
      "planning for trajectory step:  65\n",
      "action:  [ 3.7130475  -0.02656646  0.01590115  0.28558806] reward:  -0.36163019609227337\n",
      "planning time:  0.2557370662689209\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.7901234567901237\n",
      "planning for trajectory step:  66\n",
      "action:  [ 1.039984    0.01962315 -0.04396843 -0.02957953] reward:  -0.36206427495251015\n",
      "planning time:  0.22650551795959473\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8048577992803249\n",
      "planning for trajectory step:  67\n",
      "action:  [ 3.3256688   0.03962192  0.03545608 -0.24268728] reward:  -0.3763925417506319\n",
      "planning time:  0.24607443809509277\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8191188087424394\n",
      "planning for trajectory step:  68\n",
      "action:  [ 3.2850707  -0.02724081 -0.00819532  0.58709466] reward:  -0.3645394207795649\n",
      "planning time:  0.23111343383789062\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8328878414482771\n",
      "planning for trajectory step:  69\n",
      "action:  [ 3.420551    0.04709255 -0.03069798 -0.2699269 ] reward:  -0.36858418700809487\n",
      "planning time:  0.23209142684936523\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8461476543152218\n",
      "planning for trajectory step:  70\n",
      "action:  [ 0.42405534 -0.02728156 -0.02807832 -0.16391294] reward:  -0.3658719302778653\n",
      "planning time:  0.21954631805419922\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8588824806167938\n",
      "planning for trajectory step:  71\n",
      "action:  [ 1.267855    0.02743153 -0.04524357  0.2010527 ] reward:  -0.3160777103427092\n",
      "planning time:  0.2148449420928955\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8710781056932277\n",
      "planning for trajectory step:  72\n",
      "action:  [ 2.8520496  -0.04413001  0.02478957 -0.4387747 ] reward:  -0.26291834229664135\n",
      "planning time:  0.20476603507995605\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8827219426620414\n",
      "planning for trajectory step:  73\n",
      "action:  [ 0.19495057  0.0147788  -0.00851627 -0.52181137] reward:  -0.26917111939638183\n",
      "planning time:  0.20449542999267578\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8938031081286091\n",
      "planning for trajectory step:  74\n",
      "action:  [ 1.4674515   0.03382999 -0.04839437 -0.47674048] reward:  -0.2736865470310094\n",
      "planning time:  0.21834874153137207\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9043124978967261\n",
      "planning for trajectory step:  75\n",
      "action:  [ 0.30350563  0.03335079 -0.01037095 -0.49035078] reward:  -0.26552371216455184\n",
      "planning time:  0.24669766426086426\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9142428626791894\n",
      "planning for trajectory step:  76\n",
      "action:  [ 3.1903248   0.04394583 -0.0461249  -0.52835816] reward:  -0.2879484059778851\n",
      "planning time:  0.21941709518432617\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9235888838083635\n",
      "planning for trajectory step:  77\n",
      "action:  [ 3.387429   -0.02352291 -0.02436963  0.03029476] reward:  -0.2699639254143618\n",
      "planning time:  0.2322676181793213\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.932347248946753\n",
      "planning for trajectory step:  78\n",
      "action:  [ 1.5547184   0.04131003  0.01180326 -0.4331551 ] reward:  -0.2957696364565517\n",
      "planning time:  0.2313687801361084\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.940516727797575\n",
      "planning for trajectory step:  79\n",
      "action:  [ 3.3380535 -0.0234825 -0.0473776 -0.5553202] reward:  -0.3364023531448923\n",
      "planning time:  0.20299625396728516\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9480982478153277\n",
      "planning for trajectory step:  80\n",
      "action:  [ 3.0475461  -0.03709913  0.02024917  0.56690973] reward:  -0.3931840249057232\n",
      "planning time:  0.2178633213043213\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9550949699163627\n",
      "planning for trajectory step:  81\n",
      "action:  [ 0.06714365 -0.00882384 -0.03482275 -0.46100852] reward:  -0.4264716366871081\n",
      "planning time:  0.20297551155090332\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.961512364189459\n",
      "planning for trajectory step:  82\n",
      "action:  [ 0.9771768  -0.00257179  0.00914405  0.34236848] reward:  -0.4481593890102527\n",
      "planning time:  0.20273375511169434\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9673582856063918\n",
      "planning for trajectory step:  83\n",
      "action:  [ 1.6994665  -0.02415331 -0.01871201  0.31064376] reward:  -0.4064680455283991\n",
      "planning time:  0.23108172416687012\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9726430497325027\n",
      "planning for trajectory step:  84\n",
      "action:  [ 3.8895483   0.02327084 -0.04427703  0.46272767] reward:  -0.3989833916551262\n",
      "planning time:  0.25086188316345215\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9773795084372736\n",
      "planning for trajectory step:  85\n",
      "action:  [ 0.9959746  -0.04907842 -0.01033979  0.50804913] reward:  -0.4210692570526326\n",
      "planning time:  0.21620869636535645\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9815831256048986\n",
      "planning for trajectory step:  86\n",
      "action:  [3.526214   0.0108035  0.04356527 0.47878113] reward:  -0.45013596537865586\n",
      "planning time:  0.20377016067504883\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9852720528448526\n",
      "planning for trajectory step:  87\n",
      "action:  [0.26671046 0.03128112 0.03800842 0.5694523 ] reward:  -0.49806675152632074\n",
      "planning time:  0.21802711486816406\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9884672052024599\n",
      "planning for trajectory step:  88\n",
      "action:  [ 0.6021909  -0.03389045  0.04681442 -0.5101601 ] reward:  -0.6135440698683741\n",
      "planning time:  0.20151782035827637\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9911923368694775\n",
      "planning for trajectory step:  89\n",
      "action:  [ 3.9532442  -0.04177504 -0.03423044  0.4544661 ] reward:  -0.6318317952382794\n",
      "planning time:  0.19966340065002441\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9934741168946499\n",
      "planning for trajectory step:  90\n",
      "action:  [ 2.2512727   0.04876542 -0.01357228 -0.37739235] reward:  -0.6666636659767694\n",
      "planning time:  0.21613121032714844\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9953422048942939\n",
      "planning for trajectory step:  91\n",
      "action:  [ 3.5021608  -0.04885517  0.0298969  -0.32720318] reward:  -0.9349778833294023\n",
      "planning time:  0.2039179801940918\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9968293267628683\n",
      "planning for trajectory step:  92\n",
      "action:  [ 3.1831496  -0.03829104  0.03909452 -0.37713838] reward:  -1.2969007554934389\n",
      "planning time:  0.2177598476409912\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9979713503835264\n",
      "planning for trajectory step:  93\n",
      "action:  [ 0.70304585 -0.04102976 -0.01636013 -0.5928322 ] reward:  -1.7511963698391522\n",
      "planning time:  0.22955036163330078\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9988073613387254\n",
      "planning for trajectory step:  94\n",
      "Crash detected!\n",
      "Crash detected!\n",
      "action:  [0.4487492  0.04602245 0.04922856 0.52394855] reward:  -2.0652189848037814\n",
      "planning time:  0.21546506881713867\n",
      "resetting environment, and starting trial : 16\n",
      "Connected!\n",
      "4 DoF [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "No propeller. Is it a magic box?\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 1.0150579389358555e-05\n",
      "Number of stored transitions:  10000\n",
      "Training model\n",
      "training loss:  -43.716738348839264\n",
      "validation loss:  0.022094741463661194\n",
      "training time:  54.278189182281494\n",
      "Model trained\n",
      "planning for trajectory step:  1\n",
      "action:  [ 1.3038095e+00 -3.1768566e-03  1.2294501e-03  3.7345462e-02] reward:  -0.315900039152819\n",
      "planning time:  0.22014331817626953\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 7.997055280231069e-05\n",
      "planning for trajectory step:  2\n",
      "action:  [ 1.9537739   0.01130064  0.03472169 -0.30681825] reward:  -0.3159131336022355\n",
      "planning time:  0.21775388717651367\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.00026576965766001126\n",
      "planning for trajectory step:  3\n",
      "action:  [ 1.5149305   0.02331001  0.00713619 -0.29979655] reward:  -0.3159322795263467\n",
      "planning time:  0.28156566619873047\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0006202613792457937\n",
      "planning for trajectory step:  4\n",
      "action:  [ 3.3643093  -0.00745419  0.00408615  0.1232985 ] reward:  -0.31593163741037883\n",
      "planning time:  0.3146092891693115\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0011926386612764923\n",
      "planning for trajectory step:  5\n",
      "action:  [ 3.5547373  -0.01265646  0.0383664  -0.02179791] reward:  -0.316056081220855\n",
      "planning time:  0.29384517669677734\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.002028649616473763\n",
      "planning for trajectory step:  6\n",
      "action:  [ 3.3195794  -0.00537178 -0.02423115  0.09933113] reward:  -0.3243096925910473\n",
      "planning time:  0.26294779777526855\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.003170673237135412\n",
      "planning for trajectory step:  7\n",
      "action:  [ 3.5625257e+00 -2.8606530e-03 -4.8425365e-02  4.2936805e-01] reward:  -0.3596627114240958\n",
      "planning time:  0.6048250198364258\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.004657795105706712\n",
      "planning for trajectory step:  8\n",
      "action:  [ 2.7273374  -0.02282175  0.01410427  0.3493435 ] reward:  -0.5963037987403969\n",
      "planning time:  0.464158296585083\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.006525883105351723\n",
      "planning for trajectory step:  9\n",
      "action:  [ 2.3986928   0.00501899 -0.0182169   0.39287406] reward:  -0.8214797861192848\n",
      "planning time:  0.44605517387390137\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.008807663130524624\n",
      "planning for trajectory step:  10\n",
      "action:  [3.5195057  0.02560485 0.02635964 0.57554704] reward:  -1.5966441378703662\n",
      "planning time:  0.2303314208984375\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.011532794797541032\n",
      "planning for trajectory step:  11\n",
      "action:  [3.4955242  0.04020324 0.04378229 0.4949651 ] reward:  -2.2357455853651254\n",
      "planning time:  0.24701595306396484\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.014727947155149303\n",
      "planning for trajectory step:  12\n",
      "action:  [ 0.54882437  0.04711575 -0.04525844 -0.33042157] reward:  -2.8094861481055937\n",
      "planning time:  0.802945613861084\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.018416874395101913\n",
      "planning for trajectory step:  13\n",
      "Crash detected!\n",
      "Crash detected!\n",
      "action:  [ 1.468281    0.04765101 -0.04661975  0.51197004] reward:  -4.25877021189299\n",
      "planning time:  0.24858880043029785\n",
      "resetting environment, and starting trial : 17\n",
      "Connected!\n",
      "4 DoF [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "No propeller. Is it a magic box?\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 1.0150579389358555e-05\n",
      "Number of stored transitions:  10000\n",
      "Training model\n",
      "training loss:  -43.09629832018142\n",
      "validation loss:  0.024505583569407463\n",
      "training time:  55.91996932029724\n",
      "Model trained\n",
      "planning for trajectory step:  1\n",
      "action:  [ 1.2348161   0.02908482  0.01740681 -0.00608905] reward:  -0.31590003914941683\n",
      "planning time:  0.22214007377624512\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 7.997055280231069e-05\n",
      "planning for trajectory step:  2\n",
      "action:  [ 2.2426317   0.01340154 -0.01662106 -0.3262813 ] reward:  -0.3159091868834555\n",
      "planning time:  0.2186143398284912\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.00026576965766001126\n",
      "planning for trajectory step:  3\n",
      "action:  [ 0.6795693   0.03969125  0.01520921 -0.29386294] reward:  -0.31592164365748937\n",
      "planning time:  0.21722173690795898\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0006202613792457937\n",
      "planning for trajectory step:  4\n",
      "action:  [ 3.2238362  -0.00819045  0.01719949  0.09645516] reward:  -0.31594033277373573\n",
      "planning time:  0.21479129791259766\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0011926386612764923\n",
      "planning for trajectory step:  5\n",
      "action:  [ 1.60444     0.041809    0.03453326 -0.47063595] reward:  -0.31593323588681493\n",
      "planning time:  0.19916081428527832\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.002028649616473763\n",
      "planning for trajectory step:  6\n",
      "action:  [ 2.05301     0.04506277  0.02933911 -0.536096  ] reward:  -0.31585792291219056\n",
      "planning time:  0.21918749809265137\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.003170673237135412\n",
      "planning for trajectory step:  7\n",
      "action:  [ 3.3849041  -0.00916752 -0.0344492   0.40258303] reward:  -0.315875580611313\n",
      "planning time:  0.21569013595581055\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.004657795105706712\n",
      "planning for trajectory step:  8\n",
      "action:  [3.2370877  0.04502806 0.02755343 0.23548517] reward:  -0.31588503155152586\n",
      "planning time:  0.21589994430541992\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.006525883105351723\n",
      "planning for trajectory step:  9\n",
      "action:  [ 1.6792085   0.0306816   0.03879847 -0.47659788] reward:  -0.3200847923876228\n",
      "planning time:  0.21421241760253906\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.008807663130524624\n",
      "planning for trajectory step:  10\n",
      "action:  [ 3.303379   -0.00676689 -0.01606139  0.22408068] reward:  -0.32389489175498426\n",
      "planning time:  0.21654200553894043\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.011532794797541032\n",
      "planning for trajectory step:  11\n",
      "action:  [ 2.1302893   0.02761339  0.01289459 -0.3133682 ] reward:  -0.3164745211188618\n",
      "planning time:  0.23129677772521973\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.014727947155149303\n",
      "planning for trajectory step:  12\n",
      "action:  [ 3.6253994  -0.04725184  0.04236118  0.578247  ] reward:  -0.31791848894355945\n",
      "planning time:  0.2299189567565918\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.018416874395101913\n",
      "planning for trajectory step:  13\n",
      "action:  [ 3.3852978   0.04794892 -0.02040078  0.34158504] reward:  -0.3119691103395397\n",
      "planning time:  0.24714326858520508\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.022620491562726712\n",
      "planning for trajectory step:  14\n",
      "action:  [ 3.9223504e+00 -1.0969375e-03 -6.9082417e-03  4.8497269e-01] reward:  -0.32462018188936737\n",
      "planning time:  0.23239564895629883\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.027356950267498275\n",
      "planning for trajectory step:  15\n",
      "action:  [ 1.7554729   0.04517738 -0.00645737 -0.42752397] reward:  -0.375847209625534\n",
      "planning time:  0.23368597030639648\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.03264171439360926\n",
      "planning for trajectory step:  16\n",
      "action:  [ 3.6659293  -0.02080743  0.02416489  0.34502423] reward:  -0.5497679324310375\n",
      "planning time:  0.2642033100128174\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.03848763581054164\n",
      "planning for trajectory step:  17\n",
      "Crash detected!\n",
      "Crash detected!\n",
      "action:  [ 0.2908167  -0.03813848  0.0271709   0.3455575 ] reward:  -0.6805592754414251\n",
      "planning time:  0.23114824295043945\n",
      "resetting environment, and starting trial : 18\n",
      "Connected!\n",
      "4 DoF [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "No propeller. Is it a magic box?\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 1.0150579389358555e-05\n",
      "Number of stored transitions:  10000\n",
      "Training model\n",
      "training loss:  -43.803320110244236\n",
      "validation loss:  0.002898962004110217\n",
      "training time:  67.36193776130676\n",
      "Model trained\n",
      "planning for trajectory step:  1\n",
      "action:  [ 1.1474893e+00 -9.4526401e-03 -5.1819562e-04 -3.4687850e-01] reward:  -0.31590003914941683\n",
      "planning time:  0.24138283729553223\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 7.997055280231069e-05\n",
      "planning for trajectory step:  2\n",
      "action:  [ 1.8536500e+00  3.6828898e-02 -1.4806731e-03 -1.1975013e-01] reward:  -0.31590880052726183\n",
      "planning time:  0.23335909843444824\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.00026576965766001126\n",
      "planning for trajectory step:  3\n",
      "action:  [ 3.2823064e+00 -5.4113183e-04 -6.3477973e-03  3.2096276e-01] reward:  -0.3159207176399853\n",
      "planning time:  0.24692201614379883\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0006202613792457937\n",
      "planning for trajectory step:  4\n",
      "action:  [ 1.3315502   0.02479414 -0.01267705 -0.23701961] reward:  -0.3159323231666626\n",
      "planning time:  0.23340630531311035\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0011926386612764923\n",
      "planning for trajectory step:  5\n",
      "action:  [ 1.9715298   0.00428207  0.02293606 -0.44917443] reward:  -0.3156203904370572\n",
      "planning time:  0.2335350513458252\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.002028649616473763\n",
      "planning for trajectory step:  6\n",
      "action:  [ 3.4444418   0.02582743  0.03752211 -0.20940487] reward:  -0.3159582522909469\n",
      "planning time:  0.23463869094848633\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.003170673237135412\n",
      "planning for trajectory step:  7\n",
      "action:  [ 2.9032989   0.02746055  0.02864585 -0.44160312] reward:  -0.31597611795384584\n",
      "planning time:  0.24817252159118652\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.004657795105706712\n",
      "planning for trajectory step:  8\n",
      "action:  [3.4492178  0.00778826 0.00894484 0.03449045] reward:  -0.3218493477960718\n",
      "planning time:  0.2507491111755371\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.006525883105351723\n",
      "planning for trajectory step:  9\n",
      "action:  [ 3.2582026e+00 -2.2173769e-04 -2.0589314e-02  9.5105611e-02] reward:  -0.3165904081942521\n",
      "planning time:  0.22104787826538086\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.008807663130524624\n",
      "planning for trajectory step:  10\n",
      "action:  [ 2.2525985e+00  1.3846128e-02 -1.7260138e-03 -3.7427881e-01] reward:  -0.2938377848207092\n",
      "planning time:  0.23443889617919922\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.011532794797541032\n",
      "planning for trajectory step:  11\n",
      "action:  [ 1.1339489   0.01657308  0.03626261 -0.24293612] reward:  -0.26450792075258356\n",
      "planning time:  0.24799346923828125\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.014727947155149303\n",
      "planning for trajectory step:  12\n",
      "Crash detected!\n",
      "Crash detected!\n",
      "action:  [ 3.5800867  -0.03338171  0.03627033  0.54401904] reward:  -0.3063275609853304\n",
      "planning time:  0.21689343452453613\n",
      "resetting environment, and starting trial : 19\n",
      "Connected!\n",
      "4 DoF [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "No propeller. Is it a magic box?\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 1.0150579389358555e-05\n",
      "Number of stored transitions:  10000\n",
      "Training model\n",
      "training loss:  -43.9251454628554\n",
      "validation loss:  0.026040297001600266\n",
      "training time:  65.54918479919434\n",
      "Model trained\n",
      "planning for trajectory step:  1\n",
      "action:  [2.7395613  0.01861827 0.00476155 0.36800978] reward:  -0.315900039152819\n",
      "planning time:  0.24493694305419922\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 7.997055280231069e-05\n",
      "planning for trajectory step:  2\n",
      "action:  [ 1.1788437e+00 -1.8051315e-02  1.1650058e-03 -7.3401630e-02] reward:  -0.3159188271026884\n",
      "planning time:  0.2509167194366455\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.00026576965766001126\n",
      "planning for trajectory step:  3\n",
      "action:  [ 0.5338073  -0.03977341 -0.00700903 -0.13317554] reward:  -0.31593552799341723\n",
      "planning time:  0.3561556339263916\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0006202613792457937\n",
      "planning for trajectory step:  4\n",
      "action:  [ 1.2130202   0.01750937 -0.01547327 -0.3587458 ] reward:  -0.31591617375904424\n",
      "planning time:  0.3113706111907959\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0011926386612764923\n",
      "planning for trajectory step:  5\n",
      "action:  [ 3.8597276e+00 -2.9137067e-02 -3.6435874e-04  3.1834364e-01] reward:  -0.3159144067342631\n",
      "planning time:  0.2615966796875\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.002028649616473763\n",
      "planning for trajectory step:  6\n",
      "action:  [ 2.7092113  -0.03947414 -0.04062539  0.5237988 ] reward:  -0.3159206553979008\n",
      "planning time:  0.3271951675415039\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.003170673237135412\n",
      "planning for trajectory step:  7\n",
      "action:  [ 3.307461   -0.00592092 -0.01729205  0.1561467 ] reward:  -0.3691680402214555\n",
      "planning time:  0.26231956481933594\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.004657795105706712\n",
      "planning for trajectory step:  8\n",
      "action:  [ 3.5469444  -0.02524595  0.00659106  0.31123975] reward:  -0.4574664416311988\n",
      "planning time:  0.2948305606842041\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.006525883105351723\n",
      "planning for trajectory step:  9\n",
      "action:  [ 1.4026492  -0.01113446 -0.03784531  0.5442072 ] reward:  -0.5893422926305178\n",
      "planning time:  0.2608368396759033\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.008807663130524624\n",
      "planning for trajectory step:  10\n",
      "action:  [ 2.0437725   0.03897451 -0.02298729 -0.35941115] reward:  -0.7651294728405948\n",
      "planning time:  0.29465770721435547\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.011532794797541032\n",
      "planning for trajectory step:  11\n",
      "action:  [0.871664   0.00891794 0.01607766 0.28582907] reward:  -0.8363448151906203\n",
      "planning time:  0.26247692108154297\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.014727947155149303\n",
      "planning for trajectory step:  12\n",
      "action:  [ 0.3660032   0.03492711 -0.04912028 -0.53781855] reward:  -0.7434257970542995\n",
      "planning time:  0.26302599906921387\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.018416874395101913\n",
      "planning for trajectory step:  13\n",
      "action:  [ 0.085959   -0.04259205 -0.03760028  0.47152346] reward:  -0.7704782848472588\n",
      "planning time:  0.3261086940765381\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.022620491562726712\n",
      "planning for trajectory step:  14\n",
      "action:  [ 0.04033814 -0.04527785 -0.04145583 -0.52689934] reward:  -0.7440233500407768\n",
      "planning time:  0.2767910957336426\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.027356950267498275\n",
      "planning for trajectory step:  15\n",
      "action:  [ 3.546389    0.01591294  0.03445894 -0.47999266] reward:  -0.7556770370072716\n",
      "planning time:  0.39840030670166016\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.03264171439360926\n",
      "planning for trajectory step:  16\n",
      "action:  [ 2.7560205e+00  1.6712490e-03 -3.0015377e-02 -4.0534824e-01] reward:  -0.7510182925270318\n",
      "planning time:  0.32196784019470215\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.03848763581054164\n",
      "planning for trajectory step:  17\n",
      "action:  [ 0.02689687  0.04989845 -0.04753803 -0.37495878] reward:  -0.7445315933578649\n",
      "planning time:  0.26436877250671387\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.044905030083638116\n",
      "planning for trajectory step:  18\n",
      "action:  [ 0.5778237   0.01086367 -0.02971712 -0.4697576 ] reward:  -0.6737163783677783\n",
      "planning time:  0.26229405403137207\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.05190175218467344\n",
      "planning for trajectory step:  19\n",
      "action:  [ 3.2274904   0.04975901 -0.03921475  0.36411417] reward:  -0.5705029638957116\n",
      "planning time:  0.2161087989807129\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.05948327220242562\n",
      "planning for trajectory step:  20\n",
      "action:  [ 3.2299757  -0.0447528  -0.02099206  0.28682587] reward:  -0.584078872993596\n",
      "planning time:  0.26211047172546387\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0676527510532474\n",
      "planning for trajectory step:  21\n",
      "action:  [ 0.14361879 -0.04927041 -0.04766307 -0.31290606] reward:  -0.5914088257145187\n",
      "planning time:  0.21867775917053223\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.07641111619163748\n",
      "planning for trajectory step:  22\n",
      "action:  [ 3.9101357  -0.01607771 -0.01077912 -0.26261988] reward:  -0.5434792121707869\n",
      "planning time:  0.2456371784210205\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0857571373208119\n",
      "planning for trajectory step:  23\n",
      "action:  [ 1.0474454  -0.03737994 -0.04175443 -0.5605496 ] reward:  -0.41605984352300823\n",
      "planning time:  0.2624380588531494\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.09568750210327515\n",
      "planning for trajectory step:  24\n",
      "action:  [ 1.0031979   0.04810438 -0.03241699 -0.48254344] reward:  -0.48859873188481334\n",
      "planning time:  0.30925679206848145\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.10619689187139202\n",
      "planning for trajectory step:  25\n",
      "Crash detected!\n",
      "Crash detected!\n",
      "action:  [ 2.6910899  -0.04702247 -0.04248035  0.5621355 ] reward:  -0.7746550534707781\n",
      "planning time:  0.2613193988800049\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABH8AAAFWCAYAAADwsDHbAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAdLElEQVR4nO3dT1Iiaf4H4G9NdISr0RR3E1M9MXgDLU/QcAOtPkHjvhcSriZ6ZeANoE/Qyg3kBlVyA7IifrUWU2flZvgtOqSbKv+AQpG8/TwRLEh4JcvXIj/xyX9vRqPRKAAAAABI0t+WvQIAAAAALI7yBwAAACBhyh8AAACAhCl/AAAAABKm/AEAAABImPIHAAAAIGHKHwAAAICEKX8AAAAAEqb8AQAAAEjYdy8Z1Ol0YjAYRKvVmur9eZ5Hq9WK7e3tiIjIsiwajcZLPhoAYCXJTwDAskxd/twHkIiIs7OzqcNHnuexu7sbnz59iizLIiKi2WzG6elpHB0dzb7GAAArQn4CAMrgzWg0Gs06aHd3N2q12lR7rg4PDyPLson3FkURm5ub8YKPBgBYSfITALAsC7/mz9nZ2fhw5Xv3e7B6vd6iPx4AYOXITwDAPC20/CmKIoqiiGq1+tVrWZZFv99f5McDAKwc+QkAmLcXXfB5WnmeP/papVKJq6urJ8ff3d3F3d3d+Pn//ve/GA6HsbW1FW/evJnbegIA8zUajeK///1v/OMf/4i//c3NRWchPwHAX9Mi89NCy5/nFEXx5OsnJyfxyy+/fJuVAQDm7vPnz/HPf/5z2auRFPkJANK2iPy00PLn/tz0hwyHw2fHHx8fx88//zx+fnNzE99//318/vw51tfX57GKAMAC3N7extu3b+Pvf//7sldl5chPAPDXtMj8tNDyp1KpRMTDe6iKongy3ERErK2txdra2lfL19fXhRcAWAFOM5qd/AQAf22LyE8LPQk/y7LIsuzRvVT1en2RHw8AsHLkJwBg3hZ+Bcb379/HYDCYWHZ/IcNarbbojwcAWDnyEwAwTy8qf+5vQfrQ8nq9PnEL0mazGd1ud+J97XY72u32Sz4aAGAlyU8AwLJMfc2foiji5OQkiqKIPM/j7OwsIiK2t7fj6OgoIn6/COHHjx8nDlOuVqtxfn4ezWYz9vb2Is/z2NraikajMed/CgBAuchPAEAZvBmNRqNlr8S0bm9vY2NjI25ublywEABKzDa7PMwFAKyGRW6zF37NHwAAAACWR/kDAAAAkDDlDwAAAEDClD8AAAAACVP+AAAAACRM+QMAAACQMOUPAAAAQMKUPwAAAAAJU/4AAAAAJEz5AwAAAJAw5Q8AAABAwpQ/AAAAAAlT/gAAAAAkTPkDAAAAkDDlDwAAAEDClD8AAAAACVP+AAAAACRM+QMAAACQMOUPAAAAQMKUPwAAAAAJU/4AAAAAJEz5AwAAAJAw5Q8AAABAwpQ/AAAAAAlT/gAAAAAkTPkDAAAAkDDlDwAAAEDClD8AAAAACVP+AAAAACRM+QMAAACQMOUPAAAAQMKUPwAAAAAJU/4AAAAAJEz5AwAAAJAw5Q8AAABAwpQ/AAAAAAlT/gAAAAAkTPkDAAAAkDDlDwAAAEDClD8AAAAACVP+AAAAACRM+QMAAACQMOUPAAAAQMKUPwAAAAAJ+27WAXmeR6vViu3t7YiIyLIsGo3Gs+P6/X70er2IiLi6uoqtra04Ojqa9eMBAFaO/AQALNNM5U+e57G7uxufPn2KLMsiIqLZbMbp6emTQSTP8+j1ehPv6ff7cXBwEOfn5y9bcwCAFSA/AQDLNtNpX61WKxqNxji4REQcHx9Hs9l8dtz+/v7Esp2dnSiKYpaPBwBYOfITALBsM5U/Z2dn48OV790HmftDkh8yHA6j1Wo9uBwAIGXyEwCwbFOXP0VRRFEUUa1Wv3oty7Lo9/uPjj08PIxOpxMHBwfjvVWnp6dxeHg4+xoDAKwI+QkAKIOpy588zx99rVKpxNXV1aOv12q1aLVa0e12Y3NzMw4ODqJWqz17ocO7u7u4vb2deAAArAr5CQAog5nv9vWY584/39/fjw8fPkSe59HtdiMi4tdff504//1LJycn8csvv8xrFQEASkV+AgC+hamP/HkqZDx37nm/349msxnn5+dxeXk53ou1u7v75Ljj4+O4ubkZPz5//jzt6gIALJ38BACUwdTlT6VSiYiH91AVRfFkuPnpp58mbkl6dHQUg8EghsNhdDqdR8etra3F+vr6xAMAYFXITwBAGcx05E+WZY/uparX6w8uz/N8HHz+rFqtxvHxcVxeXk67CgAAK0V+AgDKYKZbvb9//z4Gg8HEsvsLGdZqtQfHVKvVRy92mGXZs4cuAwCsMvkJAFi2mcqfZrM5vtjgvXa7He12e/y8KIqo1+sTty7d39+P09PTiXFFUcTFxcWzd6wAAFhl8hMAsGwz3e2rWq3G+fl5NJvN2NvbizzPY2trayKADIfD+Pjx48Thza1WKzqdThweHo7Pbd/a2po4jx0AIEXyEwCwbG9Go9Fo2Ssxrdvb29jY2IibmxsXLwSAErPNLg9zAQCrYZHb7JlO+wIAAABgtSh/AAAAABKm/AEAAABImPIHAAAAIGHKHwAAAICEKX8AAAAAEqb8AQAAAEiY8gcAAAAgYcofAAAAgIQpfwAAAAASpvwBAAAASJjyBwAAACBhyh8AAACAhCl/AAAAABKm/AEAAABImPIHAAAAIGHKHwAAAICEKX8AAAAAEqb8AQAAAEiY8gcAAAAgYcofAAAAgIQpfwAAAAASpvwBAAAASJjyBwAAACBhyh8AAACAhCl/AAAAABKm/AEAAABImPIHAAAAIGHKHwAAAICEKX8AAAAAEqb8AQAAAEiY8gcAAAAgYcofAAAAgIQpfwAAAAASpvwBAAAASJjyBwAAACBhyh8AAACAhCl/AAAAABKm/AEAAABImPIHAAAAIGHKHwAAAICEKX8AAAAAEqb8AQAAAEiY8gcAAAAgYd/NOiDP82i1WrG9vR0REVmWRaPRmHpsu92Ora2tuLq6ir29vdjf3591FQAAVor8BAAs05vRaDSa9s15nsfu7m58+vQpsiyLiIhmsxlbW1txdHT05NherxftdjvOz88jIqIoivjhhx/i8vJy6pW9vb2NjY2NuLm5ifX19anHAQDflm32H+QnAGAai9xmz1T+HB4eRpZl0Wq1xsuKoojNzc146scURRH//ve/J0JPr9eLg4ODuL6+nnplhRcAWA222X+QnwCAaSxymz3TNX/Ozs7Ghyvf+3MYeczJyUm8e/du/N6IiFqtNlNwAQBYRfITALBsU5c/RVFEURRRrVa/ei3Lsuj3+4+O7Xa7Ua/XI+L3kPPUewEAUiE/AQBlMHX5k+f5o69VKpW4urp6dmyn04l3795FRES9Xn82xNzd3cXt7e3EAwBgVchPAEAZzO1W70VRPLj8PrhcXFxEo9GILMtiZ2cnms1m/PDDD0/+zJOTk9jY2Bg/3r59O6/VBQBYOvkJAPgWpi5//ny++ZeGw+Gz43d2diae12q1KIoiOp3Oo2OOj4/j5uZm/Pj8+fO0qwsAsHTyEwBQBt9N+8ZKpRIRD++hKori0XBzP+7LCx3ee+pWpWtra7G2tjbtKgIAlIr8BACUwUxH/mRZ9uheqvsLEj427rHDmh8LNQAAq05+AgDKYKZr/rx//z4Gg8HEsvtz0mu12pPjPnz4MLHsPsw8NQ4AYNXJTwDAss1U/jSbzeh2uxPL2u12tNvt8fOiKL66E0Wr1Yp+vz9xx4tmsxn7+/tfncsOAJAS+QkAWLapr/kTEVGtVuP8/DyazWbs7e1FnuextbUVjUZj/J7hcBgfP36cOLw5y7K4vLyMZrM5Prd9e3t7IvQAAKRIfgIAlu3NaDQaLXslpnV7exsbGxtxc3MT6+vry14dAOARttnlYS4AYDUscps902lfAAAAAKwW5Q8AAABAwpQ/AAAAAAlT/gAAAAAkTPkDAAAAkDDlDwAAAEDClD8AAAAACVP+AAAAACRM+QMAAACQMOUPAAAAQMKUPwAAAAAJU/4AAAAAJEz5AwAAAJAw5Q8AAABAwpQ/AAAAAAlT/gAAAAAkTPkDAAAAkDDlDwAAAEDClD8AAAAACVP+AAAAACRM+QMAAACQMOUPAAAAQMKUPwAAAAAJU/4AAAAAJEz5AwAAAJAw5Q8AAABAwpQ/AAAAAAlT/gAAAAAkTPkDAAAAkDDlDwAAAEDClD8AAAAACVP+AAAAACRM+QMAAACQMOUPAAAAQMKUPwAAAAAJU/4AAAAAJEz5AwAAAJAw5Q8AAABAwpQ/AAAAAAlT/gAAAAAkTPkDAAAAkDDlDwAAAEDClD8AAAAACVP+AAAAACTsu1kH5HkerVYrtre3IyIiy7JoNBozf3C9Xo+Li4uZxwEArBr5CQBYppnKnzzPY3d3Nz59+hRZlkVERLPZjNPT0zg6Opr655yenkav15tpRQEAVpH8BAAs20ynfbVarWg0GuPgEhFxfHwczWZz6p+R53l8+PBhlo8FAFhZ8hMAsGwzlT9nZ2fjw5Xv3QeZafdEdbvd+PHHH2f5WACAlSU/AQDLNnX5UxRFFEUR1Wr1q9eyLIt+v//sz+h2u7G/vz/bGgIArCj5CQAog6nLnzzPH32tUqnE1dXVk+OLoojhcPhg+HnM3d1d3N7eTjwAAFaF/AQAlMHcbvVeFMWTr3c6nZnvanFychIbGxvjx9u3b1+xhgAA5SI/AQDfwtTlz58vUvil4XD45Nherxe1Wm3qlbp3fHwcNzc348fnz59n/hkAAMsiPwEAZTB1+VOpVCLi4T1URVE8GW76/X7s7OzMvHJra2uxvr4+8QAAWBXyEwBQBt9N+8YsyyLLskf3UtXr9QeXdzqdGAwGE7czvb+4YbPZjK2trTg6OpplnQEAVoL8BACUwdTlT0TE+/fvYzAYTCy7v5DhY4clP3SeeqfTiV6vF61Wa5aPBwBYOfITALBsM13wudlsRrfbnVjWbrej3W6PnxdFEfV6/clblz53cUMAgFTITwDAss105E+1Wo3z8/NoNpuxt7cXeZ7H1tbWxN6p4XAYHz9+fPDw5jzPo91ujwPQwcFB1Ov1me9iAQCwKuQnAGDZ3oxGo9GyV2Jat7e3sbGxETc3Ny5eCAAlZptdHuYCAFbDIrfZM532BQAAAMBqUf4AAAAAJEz5AwAAAJAw5Q8AAABAwpQ/AAAAAAlT/gAAAAAkTPkDAAAAkDDlDwAAAEDClD8AAAAACVP+AAAAACRM+QMAAACQMOUPAAAAQMKUPwAAAAAJU/4AAAAAJEz5AwAAAJAw5Q8AAABAwpQ/AAAAAAlT/gAAAAAkTPkDAAAAkDDlDwAAAEDClD8AAAAACVP+AAAAACRM+QMAAACQMOUPAAAAQMKUPwAAAAAJU/4AAAAAJEz5AwAAAJAw5Q8AAABAwpQ/AAAAAAlT/gAAAAAkTPkDAAAAkDDlDwAAAEDClD8AAAAACVP+AAAAACRM+QMAAACQMOUPAAAAQMKUPwAAAAAJU/4AAAAAJEz5AwAAAJAw5Q8AAABAwpQ/AAAAAAlT/gAAAAAkTPkDAAAAkLDvZh2Q53m0Wq3Y3t6OiIgsy6LRaDw7rtfrxcXFRRRFEXmex8HBwVTjAABWnfwEACzTTOVPnuexu7sbnz59iizLIiKi2WzG6elpHB0dPTqu1+tFv9+PVqsVERFFUcTu7m5cXl5Gu91++doDAJSc/AQALNub0Wg0mvbNh4eHkWXZOIRE/B5ENjc346kfc3BwEOfn5xPLOp1OHB4exmAwiGq1OtXn397exsbGRtzc3MT6+vq0qw0AfGO22X+QnwCAaSxymz3TNX/Ozs7Ghyvfu9+D1ev1Hh3X7Xaj2WxOLHv37t2z4wAAVp38BAAs29TlT1EUURTFg3uZsiyLfr//6Nj9/f2vQg8AQOrkJwCgDKa+5k+e54++VqlU4urq6tHXvzxkOSLi48ePERFRq9UeHXd3dxd3d3fj57e3t9OsKgBAKchPAEAZzO1W70VRzPT+VqsVrVbryfPVT05OYmNjY/x4+/btK9cSAKA85CcA4FuYuvy5Pzf9IcPhcKYPPTg4iFqt9uQdLiIijo+P4+bmZvz4/PnzTJ8DALBM8hMAUAZTn/ZVqVQi4uE9VEVRPBlu/qzT6USlUpnqFqVra2uxtrY27SoCAJSK/AQAlMFMR/5kWfboXqp6vf7sz+h2u1EUxURwmfVwZwCAVSE/AQBlMNM1f96/fx+DwWBi2f2FDJ+68GBERL/fj+FwOHGoclEUblUKACRNfgIAlm2m8qfZbEa3251Y1m63v9oTVa/XJ25dmud5nJycRKVSiW63O340m80nL1gIALDq5CcAYNnejEaj0SwD+v1+/Pbbb7G3tzfea/XnvVF5nsfu7m6cn5+P92Ztbm4+enjyLB9/e3sbGxsbcXNzE+vr67OsNgDwDdlmT5KfAIDnLHKbPXP5s0zCCwCsBtvs8jAXALAaFrnNnum0LwAAAABWi/IHAAAAIGHKHwAAAICEKX8AAAAAEqb8AQAAAEiY8gcAAAAgYcofAAAAgIQpfwAAAAASpvwBAAAASJjyBwAAACBhyh8AAACAhCl/AAAAABKm/AEAAABImPIHAAAAIGHKHwAAAICEKX8AAAAAEqb8AQAAAEiY8gcAAAAgYcofAAAAgIQpfwAAAAASpvwBAAAASJjyBwAAACBhyh8AAACAhCl/AAAAABKm/AEAAABImPIHAAAAIGHKHwAAAICEKX8AAAAAEqb8AQAAAEiY8gcAAAAgYcofAAAAgIQpfwAAAAASpvwBAAAASJjyBwAAACBhyh8AAACAhCl/AAAAABKm/AEAAABImPIHAAAAIGHKHwAAAICEKX8AAAAAEqb8AQAAAEiY8gcAAAAgYcofAAAAgIQpfwAAAAAS9t2sA/I8j1arFdvb2xERkWVZNBqNhY0DAFh18hMAsExvRqPRaNo353keu7u78enTp8iyLCIims1mbG1txdHR0dzHfen29jY2Njbi5uYm1tfXpx4HAHxbttl/kJ8AgGkscps9U/lzeHgYWZZFq9UaLyuKIjY3N+OpH/PScV8SXgBgNdhm/0F+AgCmscht9kzX/Dk7Oxsfdnzvfk9Ur9eb+zgAgFUnPwEAyzZ1+VMURRRFEdVq9avXsiyLfr8/13EAAKtOfgIAymDqCz7nef7oa5VKJa6uruY6LiLi7u4u7u7uxs9vbm4i4vdDoQCA8rrfVs9yelKK5CcAYFqLzE8z3+3rMUVRzH3cyclJ/PLLL18tf/v27Ys+CwD4tq6urmJjY2PZq1Fa8hMA8KVF5Kepy5/7c8wfMhwO5z4uIuL4+Dh+/vnn8fOiKOJf//pX/N///Z8guWS3t7fx9u3b+Pz5s4tHLpF5KA9zUR7mohxubm7i+++/j0qlsuxVWSr5iXu+m8rDXJSHuSgH81Aei8xPU5c/9x/+0J6moigeDSkvHRcRsba2Fmtra18t39jY8EdZEuvr6+aiBMxDeZiL8jAX5fC3v810b4nkyE98yXdTeZiL8jAX5WAeymMR+Wnqn5hlWWRZ9ujepnq9PtdxAACrTn4CAMpgpjrp/fv3MRgMJpbdX5CwVqvNfRwAwKqTnwCAZZup/Gk2m9HtdieWtdvtaLfb4+dFUUS9Xp+4Bek046axtrYW//nPfx48lJlvy1yUg3koD3NRHuaiHMzDH+QnIsxDmZiL8jAX5WAeymORc/FmNOM9xPr9fvz222+xt7c33vt0dHQ0fj3P89jd3Y3z8/OJvVLPjQMASJX8BAAs08zlDwAAAACr4699Cw4AAACAxCl/AAAAABKm/AEAAABI2HfLXoF7eZ5Hq9WK7e3tiIjIsiwajcbCxvGwl/4+e71eXFxcRFEUked5HBwcmIdXmtffdr1ej4uLi3mv3l/Ka+Yiz/Not9uxtbUVV1dXsbe3F/v7+4tc3WS9dB76/X70er2IiLi6uoqtrS0XzJ2DTqcTg8EgWq3WVO+3vV4M+akc5KfykJ/KQ34qB/mpXJaan0YlMBgMRlmWja6vr8fLjo6ORq1WayHjeNhLf58XFxcT77m+vh5Vq9VRo9FY1Komb15/261Wa1SS/+Yr6zVzcXFxMdrf3x8/v76+Hu3s7CxiNZP3mu3El++5vLycmBemNxgMRo1GY9RoNEZZlo2Ojo6mHmd7PX/yUznIT+UhP5WH/FQO8lM5lCU/leJbrdFofPULuL6+fvZL96XjeNhLf58PfQm02+1RRIwGg8Fc1/GvYh5/24PBYLS/v+//wyu9dC6ur6+/+rK+uLgYZVm2iNVM3mu2Ew99D9Vqtbmu31/Rzs7O1OHF9nox5KdykJ/KQ34qD/mpHOSn8llmfirFNX/Ozs7GhzHdy7IsImJ8qNk8x/Gwl/4+u91uNJvNiWXv3r17dhyPm8ffdrfbjR9//HHeq/aX89K5ODk5iXfv3o3fGxFRq9Xi+vp6EauZvJfOw3A4fPCw2uFwONf142m214shP5WD/FQe8lN5yE/lID+ttnlvr5de/hRFEUVRRLVa/eq1LMui3+/PdRwPe83vc39//6s/Sl5uHn/b3W7XedFz8Jq56Ha7Ua/XI+L3L2ffSS/3mnk4PDyMTqcTBwcHURRFREScnp7G4eHholaXL9heL4b8VA7yU3nIT+UhP5WD/LTaFrG9Xnr5k+f5o69VKpW4urqa6zge9prf5/n5+VcXnfr48WNE/N7UM5vX/m0XRRHD4fDBLwpm85q5uB/b6XTGe3Lr9boQ8wKvmYdarRatViu63W5sbm7GwcFB1Go1F1T9hmyvF0N+Kgf5qTzkp/KQn8pBflpti9hel+ZuX4+5bxq/1TgeNuvvs9VqRavVsgFdgOfmotPpuBL/N/LYXNx/WV9cXIzvFLKzsxPNZjN++OEHhy7P2XP/J/b39+PDhw+R53l0u92IiPj1118nDilneWyvF0N+Kgf5qTzkp/KQn8pBflptL9leL/3In6f+eJ46p/Cl43jYPH+f982wDejLvGYuer2evYVz9Nr/Fzs7OxPPa7VaFEURnU7ntav2l/Kaeej3+9FsNuP8/DwuLy/He7F2d3fnvJY8xvZ6MeSncpCfykN+Kg/5qRzkp9W2iO310sufSqUSEQ83V0VRPPqPfuk4Hjav32en04lKpRLtdnuOa/fX8pq56Pf7X20webnXfj89di2Hy8vLuazfX8Vr/k/89NNPcX5+Pn5+dHQUg8EghsOhEPmN2F4vhvxUDvJTechP5SE/lYP8tNoWsb1e+mlfWZZFlmWPtlf3F/ya1zgeNo/fZ7fbjaIoJoKLIDm7l85Fp9OJwWAwceeQ+/Ojm81mbG1t2Zs4o9d+Pz12OKYLfM7mpfOQ5/l4w/ln1Wo1jo+PhchvxPZ6MeSncpCfykN+Kg/5qRzkp9W2iO310sufiIj379/HYDCYWHZ/zudTh2C+dBwPe83vs9/vx3A4nNg4FkURvV7PXRNe4CVz8dAF2DqdTvR6vQdv1ch0XvP99OHDh4ll92HG99PsXjIP1Wr10YvlZVnm0OVvyPZ6MeSncpCfykN+Kg/5qRzkp9U29+31qAQGg8GoWq1OLDs6Ohq12+3x8+vr61GtVhtdXl7ONI7pvWYe9vf3R+fn5xOPRqMx8T6m99K5+FKr1RqV5L/5ynrpXFxfX4+q1epoMBiMlzUajdH+/v7iVzpBL52Ho6OjUavVmhh3fX1tHuagWq2OGo3GV8ttr78d+akc5KfykJ/KQ34qB/mpfJaZn96MRqPR6zup1+v3+/Hbb7/F3t7euM36816QPM9jd3c3zs/PJ1qu58Yxm5fMw+bm5qOHZ5bkz2slvfT/xP1r7XY7ut1u5Hke+/v7Ua/X3Z7xhV46F0VRRLPZHB+679Dx13npPHQ6nbi8vDQPc1AURZycnIwvvJllWbx//z62t7fHv1Pb629LfioH+ak85KfykJ/KQX5avrLkp9KUPwAAAADM39Lv9gUAAADA4ih/AAAAABKm/AEAAABImPIHAAAAIGHKHwAAAICEKX8AAAAAEqb8AQAAAEiY8gcAAAAgYcofAAAAgIQpfwAAAAASpvwBAAAASJjyBwAAACBh/w/QXOAC70suXAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1400x375 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_scores = []\n",
    "train_time = []\n",
    "plan_time = []\n",
    "\n",
    "def train_callback(_model, _total_calls, _epoch, tr_loss, val_score, _best_val):\n",
    "    train_losses.append(tr_loss)\n",
    "    val_scores.append(val_score.mean().item())   # this returns val score per ensemble model\n",
    "\n",
    "\n",
    "# Create a trainer for the model\n",
    "model_trainer = models.ModelTrainer(dynamics_model, optim_lr=5e-5, weight_decay=5e-5)\n",
    "env.initialize_target_trajectory(traj = \"random trajectory\", num_waypoints=3) \n",
    "# Create visualization objects\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 3.75), gridspec_kw={\"width_ratios\": [1, 1]})\n",
    "ax_text = axs[0].text(300, 50, \"\")\n",
    "    \n",
    "# the states will encapsulate the desired trajectory \\\n",
    "\n",
    "# Main PETS loop\n",
    "all_rewards = [0]\n",
    "trajectory_length, total_time, pos_traj, orient_traj = env.initialize_target_trajectory(traj = \"random trajectory\", std_position_change=0.5, num_waypoints=3, num_hover_points=2, time_step_duration=5)\n",
    "#     trajectory parameters, default arguments used for random trajectory\n",
    "#     start_pos=[0.0, 0.0, 1.0], start_yaw=0.0, \n",
    "#     start_vel=[0.0, 0.0, 0.0], start_yaw_rate=0.0,\n",
    "#     std_position_change=0.2,\n",
    "#     std_orientation_change=0.1,\n",
    "#     std_velocity_change=0.05,\n",
    "#     std_angular_velocity_change=0.0,\n",
    "#     std_acceleration_change=0.0,\n",
    "#     std_angular_acceleration_change=0.0,\n",
    "#     num_waypoints=20, \n",
    "#     num_hover_points=3,\n",
    "#     time_step_duration=10\n",
    "\n",
    "model_env.set_desired_trajectory(total_time, pos_traj, orient_traj)\n",
    "model_env.trajectory_step = 0\n",
    "# logger._write_to_log(\"\\nStarting main experiment loop...\")\n",
    "\n",
    "for trial in range(num_trials):\n",
    "    \n",
    "    print(\"resetting environment, and starting trial :\", trial)\n",
    "    obs, _ = env.reset()    \n",
    "    agent.reset()\n",
    "    \n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    total_reward = 0.0\n",
    "    steps_trial = 0\n",
    "\n",
    "    traj_step = 0 \n",
    "    model_env.trajectory_step = 0\n",
    "    env.trajectory_step = 0\n",
    "    last_setpoint_set = time.time()\n",
    "    time.sleep(0.5)\n",
    "    # env.pause_simulation()\n",
    "\n",
    "    # update_axes(axs, env.render(), ax_text, trial, steps_trial, all_rewards)\n",
    "    while not (terminated or truncated or (model_env.trajectory_step >= trajectory_length-1)):\n",
    "\n",
    "        if (time.time() - last_setpoint_set > total_time/trajectory_length) :\n",
    "            # env.resume_simulation()\n",
    "            model_env.trajectory_step += 1\n",
    "            last_setpoint_set = time.time()\n",
    "            print(\"setpoint updated\")\n",
    "            env.update_setpoint(model_env.trajectory_step)\n",
    "            \n",
    "            \n",
    "\n",
    "            # env.pause_simulation()\n",
    "\n",
    "        \n",
    "        # --------------- Model Training -----------------\n",
    "        if steps_trial == 0:\n",
    "            print(\"Number of stored transitions: \", replay_buffer.num_stored)\n",
    "            dynamics_model.update_normalizer(replay_buffer.get_all())  # update normalizer stats\n",
    "            \n",
    "            dataset_train, dataset_val = common_util.get_basic_buffer_iterators(\n",
    "                replay_buffer,\n",
    "                batch_size=cfg.overrides.model_batch_size,\n",
    "                val_ratio=cfg.overrides.validation_ratio,\n",
    "                ensemble_size=ensemble_size,\n",
    "                shuffle_each_epoch=True,\n",
    "                bootstrap_permutes=False,  # build bootstrap dataset using sampling with replacement \n",
    "            )\n",
    "            print(\"Training model\")\n",
    "            training_start_time = time.time()\n",
    "            model_trainer.train(\n",
    "                dataset_train, \n",
    "                dataset_val=dataset_val, \n",
    "                num_epochs=50, \n",
    "                patience=50, \n",
    "                callback=train_callback,\n",
    "                silent=True)\n",
    "            print(\"training loss: \", train_losses[-1])\n",
    "            print(\"validation loss: \", val_scores[-1])\n",
    "            print(\"training time: \", time.time() - training_start_time)\n",
    "            print(\"Model trained\")\n",
    "\n",
    "        # --- Doing env step using the agent and adding to model dataset ---\n",
    "        print(\"planning for trajectory step: \", model_env.trajectory_step)\n",
    "        planning_start_time = time.time()\n",
    "        next_obs, reward, terminated, truncated, _ = common_util.step_env_and_add_to_buffer(\n",
    "            env, obs, agent, {}, replay_buffer)\n",
    "        print(\"planning time: \", time.time() - planning_start_time)\n",
    "        \n",
    "            \n",
    "        # update_axes(\n",
    "        #     axs, env.render(), ax_text, trial, steps_trial, all_rewards)\n",
    "        \n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "        steps_trial += 1\n",
    "\n",
    "    env.end_simulation()\n",
    "        \n",
    "    # if steps_trial == trial_length:\n",
    "    #         break\n",
    "    \n",
    "    all_rewards.append(total_reward)\n",
    "\n",
    "# update_axes(axs, env.render(), ax_text, trial, steps_trial, all_rewards, force_update=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2dd428bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, -61.43014625271793, -39.3959555937571, -10.05214803830876, -10.231261296408805, -4.559030443513064, -15.945444964017872, -33.51818468697467, -9.854794340326542, -67.55281007049238, -56.59064877946464, -4.86295869128979, -9.372479357226307, -22.566700157247617, -24.073583247858597, -23.98017983061252, -40.56704539805338, -14.582135243021535, -6.048359377357873, -3.714329663714123, -14.064610929510014]\n"
     ]
    }
   ],
   "source": [
    "print(all_rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modquad_exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
