{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c18d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import omegaconf\n",
    "from experiment_logger import (\n",
    "    create_modquad_experiment_logger,\n",
    "    log_modquad_experiment_params,\n",
    "    log_modquad_environment_info,\n",
    "    log_modquad_model_info,\n",
    "    log_modquad_agent_info\n",
    ")\n",
    "\n",
    "import modquad_copp_env as modquad_env\n",
    "import meta_learning_base as mlb\n",
    "# import mbrl.env.reward_fns as reward_fns\n",
    "# import mbrl.env.termination_fns as termination_fns\n",
    "import mbrl.models as models\n",
    "import mbrl.planning as planning\n",
    "import mbrl.util.common as common_util\n",
    "import mbrl.util as util\n",
    "import modquad_utils \n",
    "import time\n",
    "from models import modquad_ModelEnv\n",
    "# import models. as mq_model\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "mpl.rcParams.update({\"font.size\": 16})\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "seed = 0\n",
    "env = modquad_env.ModQuadEnv()\n",
    "rng = np.random.default_rng(seed=0)\n",
    "generator = torch.Generator(device=device)\n",
    "generator.manual_seed(seed)\n",
    "obs_shape = env.observation_space.shape\n",
    "act_shape = env.action_space.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e5a23d",
   "metadata": {},
   "source": [
    "Get initial replay buffer. Either run simulation now or load existing flight data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdde150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to test if gym environment simulation is working\n",
    "replay_buffer_sim = env.run_gym_simulation_and_collect_data(cut_at = 540)\n",
    "import pickle\n",
    "with open('540_sim_data.pkl', 'wb') as f:\n",
    "    pickle.dump(replay_buffer_sim, f)\n",
    "# env.end_simulation()\n",
    "# env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a2799a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_length = 200\n",
    "num_trials = 10\n",
    "ensemble_size = 5\n",
    "\n",
    "# Everything with \"???\" indicates an option with a missing value.\n",
    "# Our utility functions will fill in these details using the \n",
    "# environment information\n",
    "cfg_dict = {\n",
    "    # dynamics model configuration\n",
    "    \"dynamics_model\": {\n",
    "        \"_target_\": \"mbrl.models.GaussianMLP\",\n",
    "        \"device\": device,\n",
    "        \"num_layers\": 6,\n",
    "        \"ensemble_size\": ensemble_size,\n",
    "        \"hid_size\": 400,\n",
    "        \"in_size\": \"???\",\n",
    "        \"out_size\": \"???\",\n",
    "        \"deterministic\": False,\n",
    "        \"propagation_method\": \"fixed_model\",\n",
    "        # can also configure activation function for GaussianMLP\n",
    "        \"activation_fn_cfg\": {\n",
    "            \"_target_\": \"torch.nn.LeakyReLU\",\n",
    "            \"negative_slope\": 0.01\n",
    "        }\n",
    "    },\n",
    "    # options for training the dynamics model\n",
    "    \"algorithm\": {\n",
    "        \"dataset_size\": 20000,\n",
    "        \"learned_rewards\": False,\n",
    "        \"target_is_delta\": True,\n",
    "        \"normalize\": True,\n",
    "    },\n",
    "    # these are experiment specific options\n",
    "    \"overrides\": {\n",
    "        \"trial_length\": trial_length,\n",
    "        \"num_steps\": num_trials * trial_length,\n",
    "        \"model_batch_size\": 64,\n",
    "        \"validation_ratio\": 0.05\n",
    "    }\n",
    "}\n",
    "cfg = omegaconf.OmegaConf.create(cfg_dict)\n",
    "\n",
    "# Create a 1-D dynamics model for this environment\n",
    "dynamics_model = common_util.create_one_dim_tr_model(cfg, obs_shape, act_shape)\n",
    "\n",
    "# Create a gym-like environment to encapsulate the model\n",
    "model_env = modquad_ModelEnv.modquad_ModelEnv(env, dynamics_model, generator=generator)\n",
    "\n",
    "replay_buffer = common_util.create_replay_buffer(cfg, obs_shape, act_shape, rng=rng)\n",
    "\n",
    "for tr in replay_buffer_sim:\n",
    "    obs, action, next_obs, reward, terminate, truncated = tr\n",
    "    replay_buffer.add(obs, action, next_obs, reward, terminate, truncated)\n",
    "print(\"# samples stored\", replay_buffer.num_stored)\n",
    "\n",
    "agent_cfg = omegaconf.OmegaConf.create({\n",
    "    # this class evaluates many trajectories and picks the best one\n",
    "    \"_target_\": \"mbrl.planning.TrajectoryOptimizerAgent\",\n",
    "    \"planning_horizon\": 15,\n",
    "    \"replan_freq\": 1,\n",
    "    \"verbose\": False,\n",
    "    \"action_lb\": \"???\",\n",
    "    \"action_ub\": \"???\",\n",
    "    # this is the optimizer to generate and choose a trajectory\n",
    "    \"optimizer_cfg\": {\n",
    "        \"_target_\": \"mbrl.planning.CEMOptimizer\",\n",
    "        \"num_iterations\": 5,\n",
    "        \"elite_ratio\": 0.1,\n",
    "        \"population_size\": 500,\n",
    "        \"alpha\": 0.1,\n",
    "        \"device\": device,\n",
    "        \"lower_bound\": \"???\",\n",
    "        \"upper_bound\": \"???\",\n",
    "        \"return_mean_elites\": True,\n",
    "        \"clipped_normal\": False\n",
    "    }\n",
    "})\n",
    "\n",
    "agent = planning.create_trajectory_optim_agent_for_model(\n",
    "    model_env,\n",
    "    agent_cfg,\n",
    "    num_particles=20\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modquad_exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
