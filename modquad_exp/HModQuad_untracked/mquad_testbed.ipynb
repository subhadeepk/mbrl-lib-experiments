{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71c18d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Connecting to CoppeliaSim...\n",
      "Connected!\n",
      "4 DoF [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "No propeller. Is it a magic box?\n"
     ]
    }
   ],
   "source": [
    "from IPython import display\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import omegaconf\n",
    "from experiment_logger import (\n",
    "    create_modquad_experiment_logger,\n",
    "    log_modquad_experiment_params,\n",
    "    log_modquad_environment_info,\n",
    "    log_modquad_model_info,\n",
    "    log_modquad_agent_info\n",
    ")\n",
    "\n",
    "import modquad_copp_env as modquad_env\n",
    "import meta_learning_base as mlb\n",
    "# import mbrl.env.reward_fns as reward_fns\n",
    "# import mbrl.env.termination_fns as termination_fns\n",
    "import mbrl.models as models\n",
    "import mbrl.planning as planning\n",
    "import mbrl.util.common as common_util\n",
    "import mbrl.util as util\n",
    "import modquad_utils \n",
    "import time\n",
    "from models import modquad_ModelEnv\n",
    "# import models. as mq_model\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "mpl.rcParams.update({\"font.size\": 16})\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "seed = 0\n",
    "env = modquad_env.ModQuadEnv()\n",
    "rng = np.random.default_rng(seed=0)\n",
    "generator = torch.Generator(device=device)\n",
    "generator.manual_seed(seed)\n",
    "obs_shape = env.observation_space.shape\n",
    "act_shape = env.action_space.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e5a23d",
   "metadata": {},
   "source": [
    "Get initial replay buffer. Either run simulation now or load existing flight data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdde150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to test if gym environment simulation is working\n",
    "import pickle\n",
    "\n",
    "# replay_buffer_sim = env.run_gym_simulation_and_collect_data(cut_at = 540)\n",
    "# with open('540_sim_data.pkl', 'wb') as f:\n",
    "#     pickle.dump(replay_buffer_sim, f)\n",
    "\n",
    "with open('540_sim_data.pkl', 'rb') as f:\n",
    "    replay_buffer_sim = pickle.load(f)\n",
    "    \n",
    "# env.end_simulation()\n",
    "# env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b0d96f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16617, 4)\n",
      "Means: [ 3.23930072e+00  7.75771227e-04 -1.19661915e-05 -1.77362437e-01]\n",
      "Variances: [8.22598750e-03 5.94578935e-05 4.61922572e-05 3.14238911e-02]\n",
      "Standard Deviations: [0.09069723 0.00771089 0.00679649 0.17726785]\n",
      "Minimums: [-2.97384988 -0.0322127  -0.04703512 -0.57251428]\n",
      "Maximums: [3.32227485e+00 4.76690636e-02 4.60323018e-02 3.87281920e-05]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "replay_buffer_sim\n",
    "all_actions = []\n",
    "for transition in replay_buffer_sim:\n",
    "    obs, action, next_obs, reward, terminate, truncated = transition\n",
    "    all_actions.append(action)\n",
    "\n",
    "all_actions = np.array(all_actions)\n",
    "print(all_actions.shape)\n",
    "\n",
    "means = np.mean(all_actions, axis=0)\n",
    "variances = np.var(all_actions, axis=0)\n",
    "std_devs = np.std(all_actions, axis=0)\n",
    "mins = np.min(all_actions, axis=0)\n",
    "maxs = np.max(all_actions, axis=0)\n",
    "\n",
    "print(\"Means:\", means)\n",
    "print(\"Variances:\", variances)\n",
    "print(\"Standard Deviations:\", std_devs)\n",
    "print(\"Minimums:\", mins)\n",
    "print(\"Maximums:\", maxs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a2799a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# samples stored 10000\n"
     ]
    }
   ],
   "source": [
    "trial_length = 200\n",
    "num_trials = 10\n",
    "ensemble_size = 5\n",
    "\n",
    "# Everything with \"???\" indicates an option with a missing value.\n",
    "# Our utility functions will fill in these details using the \n",
    "# environment information\n",
    "cfg_dict = {\n",
    "    # dynamics model configuration\n",
    "    \"dynamics_model\": {\n",
    "        \"_target_\": \"mbrl.models.GaussianMLP\",\n",
    "        \"device\": device,\n",
    "        \"num_layers\": 3,\n",
    "        \"ensemble_size\": ensemble_size,\n",
    "        \"hid_size\": 200,\n",
    "        \"in_size\": \"???\",\n",
    "        \"out_size\": \"???\",\n",
    "        \"deterministic\": False,\n",
    "        \"propagation_method\": \"fixed_model\",\n",
    "        # can also configure activation function for GaussianMLP\n",
    "        \"activation_fn_cfg\": {\n",
    "            \"_target_\": \"torch.nn.LeakyReLU\",\n",
    "            \"negative_slope\": 0.01\n",
    "        }\n",
    "    },\n",
    "    # options for training the dynamics model\n",
    "    \"algorithm\": {\n",
    "        \"dataset_size\": 10000,\n",
    "        \"learned_rewards\": False,\n",
    "        \"target_is_delta\": True,\n",
    "        \"normalize\": True,\n",
    "    },\n",
    "    # these are experiment specific options\n",
    "    \"overrides\": {\n",
    "        \"trial_length\": trial_length,\n",
    "        \"num_steps\": num_trials * trial_length,\n",
    "        \"model_batch_size\": 64,\n",
    "        \"validation_ratio\": 0.05\n",
    "    }\n",
    "}\n",
    "cfg = omegaconf.OmegaConf.create(cfg_dict)\n",
    "\n",
    "# Create a 1-D dynamics model for this environment\n",
    "dynamics_model = common_util.create_one_dim_tr_model(cfg, obs_shape, act_shape)\n",
    "\n",
    "# Create a gym-like environment to encapsulate the model\n",
    "model_env = modquad_ModelEnv.modquad_ModelEnv(env, dynamics_model, generator=generator)\n",
    "\n",
    "replay_buffer = common_util.create_replay_buffer(cfg, obs_shape, act_shape, rng=rng)\n",
    "\n",
    "for tr in replay_buffer_sim:\n",
    "    obs, action, next_obs, reward, terminate, truncated = tr\n",
    "    replay_buffer.add(obs, action, next_obs, reward, terminate, truncated)\n",
    "print(\"# samples stored\", replay_buffer.num_stored)\n",
    "\n",
    "agent_cfg = omegaconf.OmegaConf.create({\n",
    "    # this class evaluates many trajectories and picks the best one\n",
    "    \"_target_\": \"mbrl.planning.TrajectoryOptimizerAgent\",\n",
    "    \"planning_horizon\": 15,\n",
    "    \"replan_freq\": 1,\n",
    "    \"verbose\": False,\n",
    "    \"action_lb\": \"???\",\n",
    "    \"action_ub\": \"???\",\n",
    "    # this is the optimizer to generate and choose a trajectory\n",
    "    \"optimizer_cfg\": {\n",
    "        \"_target_\": \"mbrl.planning.CEMOptimizer\",\n",
    "        \"num_iterations\": 5,\n",
    "        \"elite_ratio\": 0.1,\n",
    "        \"population_size\": 50,\n",
    "        \"alpha\": 0.1,\n",
    "        \"device\": device,\n",
    "        \"lower_bound\": \"???\",\n",
    "        \"upper_bound\": \"???\",\n",
    "        \"return_mean_elites\": True,\n",
    "        \"clipped_normal\": False\n",
    "    }\n",
    "})\n",
    "\n",
    "agent = planning.create_trajectory_optim_agent_for_model(\n",
    "    model_env,\n",
    "    agent_cfg,\n",
    "    num_particles=20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d59496",
   "metadata": {},
   "source": [
    "Main experiment block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6ce8faf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected!\n",
      "4 DoF [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "No propeller. Is it a magic box?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 5.59521955e-04,  2.99863845e-01,  9.93689895e-02, -1.62705561e-04,\n",
       "        -1.39460681e-05,  6.56652381e-04, -3.60691637e-08, -1.70620389e-08,\n",
       "         1.04601611e-07,  6.79286849e-09,  1.04879405e-07,  2.00711838e-07],\n",
       "       dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.end_simulation()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8c8c7d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0. 12. 24. 36. 48. 60.]\n",
      "[ 0.    6.25 12.5  18.75 25.  ]\n",
      "resetting environment, and starting trial : 0\n",
      "Connected!\n",
      "4 DoF [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "No propeller. Is it a magic box?\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 1.0150579389358555e-05\n",
      "Number of stored transitions:  10000\n",
      "Training model\n",
      "training loss:  -46.811901521362714\n",
      "validation loss:  1.1029876077373046e-05\n",
      "training time:  53.14830017089844\n",
      "Model trained\n",
      "planning for trajectory step:  1\n",
      "action:  [ 0.8406206   0.0350685   0.02301273 -0.36466676] reward:  0\n",
      "planning time:  0.3738288879394531\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 7.997055280231069e-05\n",
      "planning for trajectory step:  2\n",
      "action:  [-1.2039168   0.04058039  0.00844365 -0.40519747] reward:  0\n",
      "planning time:  0.40412330627441406\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.00026576965766001126\n",
      "planning for trajectory step:  3\n",
      "action:  [-1.2809017   0.04492109 -0.03721632 -0.22367483] reward:  0\n",
      "planning time:  0.3608372211456299\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0006202613792457937\n",
      "planning for trajectory step:  4\n",
      "action:  [-1.3414869   0.04192575 -0.01711314 -0.247618  ] reward:  0\n",
      "planning time:  0.377103328704834\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0011926386612764923\n",
      "planning for trajectory step:  5\n",
      "action:  [-1.1894299   0.04544435 -0.02957712 -0.14085245] reward:  0\n",
      "planning time:  0.36831235885620117\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.002028649616473763\n",
      "planning for trajectory step:  6\n",
      "action:  [-2.065269    0.04364294 -0.02632974 -0.29044887] reward:  0\n",
      "planning time:  0.3547556400299072\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.003170673237135412\n",
      "planning for trajectory step:  7\n",
      "action:  [-2.5649729   0.04504644 -0.02370191 -0.366255  ] reward:  0\n",
      "planning time:  0.37360334396362305\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.004657795105706712\n",
      "planning for trajectory step:  8\n",
      "action:  [-1.815721    0.0467786  -0.04404807 -0.07746276] reward:  0\n",
      "planning time:  0.3769044876098633\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.006525883105351723\n",
      "planning for trajectory step:  9\n",
      "action:  [-2.1298594   0.03905437 -0.03880669 -0.15349185] reward:  0\n",
      "planning time:  0.3896193504333496\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.008807663130524624\n",
      "planning for trajectory step:  10\n",
      "action:  [-2.3433137   0.04136137 -0.02314644 -0.34257364] reward:  0\n",
      "planning time:  0.36022424697875977\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.011532794797541032\n",
      "planning for trajectory step:  11\n",
      "action:  [-0.8188959   0.0425984  -0.0352063  -0.16009566] reward:  0\n",
      "planning time:  0.3741612434387207\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.014727947155149303\n",
      "planning for trajectory step:  12\n",
      "action:  [-2.640793    0.04382085 -0.0339633  -0.17236601] reward:  0\n",
      "planning time:  0.3720672130584717\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.018416874395101913\n",
      "planning for trajectory step:  13\n",
      "action:  [-1.9317513   0.04161396 -0.03170393 -0.24300882] reward:  0\n",
      "planning time:  0.38989925384521484\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.022620491562726712\n",
      "planning for trajectory step:  14\n",
      "action:  [-2.3692927   0.04168943 -0.00959563 -0.30881986] reward:  0\n",
      "planning time:  0.36983776092529297\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.027356950267498275\n",
      "planning for trajectory step:  15\n",
      "action:  [-2.6890152   0.04247468 -0.04296283 -0.42290574] reward:  0\n",
      "planning time:  0.3608255386352539\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.03264171439360926\n",
      "planning for trajectory step:  16\n",
      "action:  [-2.5762932   0.03642657 -0.04262945 -0.45647106] reward:  0\n",
      "planning time:  0.3722407817840576\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.03848763581054164\n",
      "planning for trajectory step:  17\n",
      "action:  [-2.2509034   0.03869274 -0.04341898 -0.4697928 ] reward:  0\n",
      "planning time:  0.40618038177490234\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.044905030083638116\n",
      "planning for trajectory step:  18\n",
      "action:  [-0.5246856   0.03997559  0.00734366 -0.25112075] reward:  0\n",
      "planning time:  0.35516834259033203\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.05190175218467344\n",
      "planning for trajectory step:  19\n",
      "action:  [-1.7456242   0.04215851  0.02802906 -0.54063463] reward:  0\n",
      "planning time:  0.37379884719848633\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.05948327220242562\n",
      "planning for trajectory step:  20\n",
      "action:  [-1.7430198   0.04459716  0.02571807 -0.5291628 ] reward:  0\n",
      "planning time:  0.36107468605041504\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0676527510532474\n",
      "planning for trajectory step:  21\n",
      "action:  [-1.5280336   0.04345274  0.02891921 -0.5312438 ] reward:  0\n",
      "planning time:  0.3755922317504883\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.07641111619163748\n",
      "planning for trajectory step:  22\n",
      "action:  [-1.4367933   0.04605795 -0.02421697 -0.24473873] reward:  0\n",
      "planning time:  0.37547969818115234\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0857571373208119\n",
      "planning for trajectory step:  23\n",
      "action:  [-1.257917    0.04597139 -0.03470191 -0.07843269] reward:  0\n",
      "planning time:  0.37480926513671875\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.09568750210327515\n",
      "planning for trajectory step:  24\n",
      "action:  [-1.9007212   0.04484064 -0.03455839 -0.08132012] reward:  0\n",
      "planning time:  0.40166544914245605\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.10619689187139202\n",
      "planning for trajectory step:  25\n",
      "action:  [-2.089364    0.04598499 -0.03846289 -0.5013652 ] reward:  0\n",
      "planning time:  0.37090063095092773\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.11727805733795829\n",
      "planning for trajectory step:  26\n",
      "action:  [-2.0311      0.04646938 -0.04078963 -0.05905812] reward:  0\n",
      "planning time:  0.3683044910430908\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.12892189430677242\n",
      "planning for trajectory step:  27\n",
      "action:  [-1.1221578   0.04060527 -0.03290188 -0.16535825] reward:  0\n",
      "planning time:  0.3702676296234131\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.14111751938320677\n",
      "planning for trajectory step:  28\n",
      "action:  [-1.7418034   0.04584236 -0.04323104 -0.5260179 ] reward:  0\n",
      "planning time:  0.3599107265472412\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.15385234568477898\n",
      "planning for trajectory step:  29\n",
      "action:  [-1.9533843   0.04610245  0.02784    -0.44790828] reward:  0\n",
      "planning time:  0.37071895599365234\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.1671121585517233\n",
      "planning for trajectory step:  30\n",
      "action:  [-2.746185    0.01160386 -0.03629692 -0.5020123 ] reward:  0\n",
      "planning time:  0.38912415504455566\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.1808811912575617\n",
      "planning for trajectory step:  31\n",
      "action:  [-1.6573408   0.04356687 -0.04386845 -0.06829663] reward:  0\n",
      "planning time:  0.4337484836578369\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.1951422007196756\n",
      "planning for trajectory step:  32\n",
      "action:  [-1.4727095   0.04692641 -0.00151079 -0.45026153] reward:  0\n",
      "planning time:  0.41446518898010254\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.2098765432098766\n",
      "planning for trajectory step:  33\n",
      "action:  [-1.7094537   0.04426602 -0.04408151 -0.18982266] reward:  0\n",
      "planning time:  0.40540003776550293\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.22506425006497865\n",
      "planning for trajectory step:  34\n",
      "action:  [-2.4413416   0.04138217 -0.04068984 -0.11984304] reward:  0\n",
      "planning time:  0.3721342086791992\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.24068410339736837\n",
      "planning for trajectory step:  35\n",
      "action:  [-1.3502403   0.04550857 -0.04153047 -0.04878122] reward:  0\n",
      "planning time:  0.37828612327575684\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.25671371180557717\n",
      "planning for trajectory step:  36\n",
      "action:  [-2.705834    0.04162923 -0.03471296 -0.28771973] reward:  0\n",
      "planning time:  0.38858819007873535\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.2731295860848524\n",
      "planning for trajectory step:  37\n",
      "action:  [-2.3281672   0.04036521 -0.03498116 -0.37759793] reward:  0\n",
      "planning time:  0.38431429862976074\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.2899072149377282\n",
      "planning for trajectory step:  38\n",
      "action:  [-2.644131    0.04695536  0.00730254 -0.5655947 ] reward:  0\n",
      "planning time:  0.49756288528442383\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.3070211406845973\n",
      "planning for trajectory step:  39\n",
      "action:  [-2.0709739   0.042956   -0.03543368 -0.51497847] reward:  0\n",
      "planning time:  0.3858041763305664\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.3244450349742825\n",
      "planning for trajectory step:  40\n",
      "action:  [-1.6862457   0.04169041 -0.03807925 -0.07226296] reward:  0\n",
      "planning time:  0.37081241607666016\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.3421517744946073\n",
      "planning for trajectory step:  41\n",
      "action:  [-2.5456877   0.04619826 -0.03496771 -0.48187366] reward:  0\n",
      "planning time:  0.41986846923828125\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.360113516682968\n",
      "planning for trajectory step:  42\n",
      "action:  [-2.3802178  -0.00470636 -0.02096926 -0.4997188 ] reward:  0\n",
      "planning time:  0.4031655788421631\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.37830177543690424\n",
      "planning for trajectory step:  43\n",
      "action:  [-2.0954518   0.04238488 -0.02112321 -0.5705018 ] reward:  0\n",
      "planning time:  0.37090301513671875\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.39668749682467125\n",
      "planning for trajectory step:  44\n",
      "action:  [-2.548386    0.04377462  0.03742459 -0.5611072 ] reward:  0\n",
      "planning time:  0.40392637252807617\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.41524113479581026\n",
      "planning for trajectory step:  45\n",
      "action:  [-2.1794698   0.04605637 -0.03059601 -0.21886711] reward:  0\n",
      "planning time:  0.3614766597747803\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.4339327268917206\n",
      "planning for trajectory step:  46\n",
      "action:  [-2.1187801   0.04691317 -0.03794725 -0.13443534] reward:  0\n",
      "planning time:  0.37724781036376953\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.45273196995622983\n",
      "planning for trajectory step:  47\n",
      "action:  [-1.9151976   0.04259758  0.01183857 -0.5288414 ] reward:  0\n",
      "planning time:  0.36163878440856934\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.4716082958461666\n",
      "planning for trajectory step:  48\n",
      "action:  [-1.5915077   0.04394925  0.01778796 -0.49721053] reward:  0\n",
      "planning time:  0.38803982734680176\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.4905309471419318\n",
      "planning for trajectory step:  49\n",
      "action:  [-1.3539666   0.04559306  0.02343472 -0.4286881 ] reward:  0\n",
      "planning time:  0.3725888729095459\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5094690528580682\n",
      "planning for trajectory step:  50\n",
      "action:  [-1.6116906   0.0430718  -0.03754853 -0.36902988] reward:  0\n",
      "planning time:  0.3739488124847412\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5283917041538334\n",
      "planning for trajectory step:  51\n",
      "action:  [-1.7477585   0.04711583 -0.03892307 -0.56597203] reward:  0\n",
      "planning time:  0.376253604888916\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5472680300437707\n",
      "planning for trajectory step:  52\n",
      "action:  [-1.8255335   0.03885221 -0.03713389 -0.1690475 ] reward:  0\n",
      "planning time:  0.4004249572753906\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5660672731082802\n",
      "planning for trajectory step:  53\n",
      "action:  [-1.5800917   0.04367151  0.01337314 -0.47742826] reward:  0\n",
      "planning time:  0.3733034133911133\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5847588652041902\n",
      "planning for trajectory step:  54\n",
      "action:  [-0.824961    0.04556646  0.02661957 -0.52077603] reward:  0\n",
      "planning time:  0.37769246101379395\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.603312503175329\n",
      "planning for trajectory step:  55\n",
      "action:  [-1.9486078   0.04690821 -0.03377743 -0.05336038] reward:  0\n",
      "planning time:  0.3705010414123535\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.621698224563096\n",
      "planning for trajectory step:  56\n",
      "action:  [-0.57564926  0.04618181  0.01923653 -0.49835178] reward:  0\n",
      "planning time:  0.38892531394958496\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.6398864833170326\n",
      "planning for trajectory step:  57\n",
      "action:  [-0.79191196  0.04561889 -0.03852231 -0.05236858] reward:  0\n",
      "planning time:  0.40364646911621094\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.657848225505393\n",
      "planning for trajectory step:  58\n",
      "action:  [-2.2645466   0.04624561 -0.02410433 -0.18374479] reward:  0\n",
      "planning time:  0.3734135627746582\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.6755549650257178\n",
      "planning for trajectory step:  59\n",
      "action:  [-1.3404561   0.04169744  0.02090256 -0.43629614] reward:  0\n",
      "planning time:  0.37131452560424805\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.6929788593154029\n",
      "planning for trajectory step:  60\n",
      "action:  [-1.3463591   0.0459367  -0.03378473 -0.1490389 ] reward:  0\n",
      "planning time:  0.3772158622741699\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.7100927850622721\n",
      "planning for trajectory step:  61\n",
      "action:  [-1.884216    0.04234039  0.02025273 -0.4390723 ] reward:  0\n",
      "planning time:  0.38969850540161133\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.7268704139151478\n",
      "planning for trajectory step:  62\n",
      "action:  [ 0.6765493   0.04520902 -0.00413839 -0.12766838] reward:  0\n",
      "planning time:  0.35791778564453125\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.7432862881944238\n",
      "planning for trajectory step:  63\n",
      "action:  [-2.317712    0.04705269 -0.00581387 -0.5011027 ] reward:  0\n",
      "planning time:  0.3848259449005127\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.7593158966026328\n",
      "planning for trajectory step:  64\n",
      "action:  [-1.9728112   0.04488048  0.00984693 -0.42582476] reward:  0\n",
      "planning time:  0.40415048599243164\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.7749357499350218\n",
      "planning for trajectory step:  65\n",
      "action:  [-1.0689892   0.04778768  0.02234303 -0.46197295] reward:  0\n",
      "planning time:  0.43634605407714844\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.7901234567901237\n",
      "planning for trajectory step:  66\n",
      "action:  [-0.71314394  0.04628099 -0.00281132 -0.46897542] reward:  0\n",
      "planning time:  0.41507744789123535\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8048577992803249\n",
      "planning for trajectory step:  67\n",
      "action:  [-1.1696454   0.04612792 -0.04203578 -0.04393045] reward:  0\n",
      "planning time:  0.43387866020202637\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8191188087424394\n",
      "planning for trajectory step:  68\n",
      "action:  [-2.316279    0.04358933 -0.01552034 -0.24760284] reward:  0\n",
      "planning time:  0.392650842666626\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8328878414482771\n",
      "planning for trajectory step:  69\n",
      "action:  [-2.5312884   0.04573644 -0.03199558 -0.1423204 ] reward:  0\n",
      "planning time:  0.3548765182495117\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8461476543152218\n",
      "planning for trajectory step:  70\n",
      "action:  [-2.0076618   0.04682424 -0.03805681 -0.07449685] reward:  0\n",
      "planning time:  0.37421512603759766\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8588824806167938\n",
      "planning for trajectory step:  71\n",
      "action:  [-1.9602498   0.04253905 -0.02853264 -0.16282515] reward:  0\n",
      "planning time:  0.37360286712646484\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8710781056932277\n",
      "planning for trajectory step:  72\n",
      "action:  [-1.0428988   0.0203646  -0.0321267  -0.15378529] reward:  0\n",
      "planning time:  0.40164780616760254\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8827219426620414\n",
      "planning for trajectory step:  73\n",
      "action:  [-1.1978011   0.04565082  0.02803984 -0.53196114] reward:  0\n",
      "planning time:  0.3912172317504883\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8938031081286091\n",
      "planning for trajectory step:  74\n",
      "action:  [-1.708669    0.04380597  0.02319602 -0.5647665 ] reward:  0\n",
      "planning time:  0.37365221977233887\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9043124978967261\n",
      "planning for trajectory step:  75\n",
      "action:  [-1.1123549   0.04435582  0.01898099 -0.5121892 ] reward:  0\n",
      "planning time:  0.38926196098327637\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9142428626791894\n",
      "planning for trajectory step:  76\n",
      "action:  [-1.8669336   0.04723772  0.02790084 -0.54165286] reward:  0\n",
      "planning time:  0.3753035068511963\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9235888838083635\n",
      "planning for trajectory step:  77\n",
      "action:  [-1.7618707   0.04445376 -0.04104695 -0.12286963] reward:  0\n",
      "planning time:  0.36002373695373535\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.932347248946753\n",
      "planning for trajectory step:  78\n",
      "action:  [-2.1527236   0.04592058 -0.0402973  -0.18205926] reward:  0\n",
      "planning time:  0.37065601348876953\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.940516727797575\n",
      "planning for trajectory step:  79\n",
      "action:  [-2.0787427   0.04421314 -0.03184885 -0.21760853] reward:  0\n",
      "planning time:  0.3749868869781494\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9480982478153277\n",
      "planning for trajectory step:  80\n",
      "action:  [-0.845819    0.04320289  0.03090313 -0.56307924] reward:  0\n",
      "planning time:  0.4654502868652344\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9550949699163627\n",
      "planning for trajectory step:  81\n",
      "action:  [-2.1078217   0.04386047 -0.03549048 -0.10702301] reward:  0\n",
      "planning time:  0.3894340991973877\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.961512364189459\n",
      "planning for trajectory step:  82\n",
      "action:  [-1.1863339   0.04613434 -0.03207181 -0.15174015] reward:  0\n",
      "planning time:  0.38792943954467773\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9673582856063918\n",
      "planning for trajectory step:  83\n",
      "action:  [-1.0354798   0.04391947 -0.02537794 -0.21957   ] reward:  0\n",
      "planning time:  0.3752779960632324\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9726430497325027\n",
      "planning for trajectory step:  84\n",
      "action:  [ 0.80419344  0.04486758 -0.04202519 -0.06407725] reward:  0\n",
      "planning time:  0.40597057342529297\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9773795084372736\n",
      "planning for trajectory step:  85\n",
      "action:  [-1.6713623   0.04045892 -0.03817308 -0.11050995] reward:  0\n",
      "planning time:  0.3762993812561035\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9815831256048986\n",
      "planning for trajectory step:  86\n",
      "action:  [-1.0006565   0.04701838 -0.02661008 -0.25476003] reward:  0\n",
      "planning time:  0.37326931953430176\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9852720528448526\n",
      "planning for trajectory step:  87\n",
      "action:  [-0.488501    0.04621769 -0.00988172 -0.16969678] reward:  0\n",
      "planning time:  0.372969388961792\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9884672052024599\n",
      "planning for trajectory step:  88\n",
      "action:  [-0.91556704  0.04504563 -0.03831862 -0.1292613 ] reward:  0\n",
      "planning time:  0.36147022247314453\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9911923368694775\n",
      "planning for trajectory step:  89\n",
      "action:  [ 0.5224684   0.04352401  0.02023654 -0.48978683] reward:  0\n",
      "planning time:  0.6796987056732178\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9934741168946499\n",
      "planning for trajectory step:  90\n",
      "action:  [-2.2003117   0.04359509 -0.0350025  -0.05531359] reward:  0\n",
      "planning time:  0.48160696029663086\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9953422048942939\n",
      "planning for trajectory step:  91\n",
      "action:  [-1.2173516   0.0467194  -0.03019256 -0.3928955 ] reward:  0\n",
      "planning time:  0.44770169258117676\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9968293267628683\n",
      "planning for trajectory step:  92\n",
      "action:  [-2.4954033   0.04459183 -0.02347997 -0.42498717] reward:  0\n",
      "planning time:  0.4246959686279297\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9979713503835264\n",
      "planning for trajectory step:  93\n",
      "action:  [-1.8564924   0.04477174 -0.03569291 -0.17449947] reward:  0\n",
      "planning time:  0.3880436420440674\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9988073613387254\n",
      "planning for trajectory step:  94\n",
      "action:  [-1.9656539   0.04702353  0.02943382 -0.5484454 ] reward:  0\n",
      "planning time:  0.4056358337402344\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9993797386207541\n",
      "planning for trajectory step:  95\n",
      "action:  [-1.7805265   0.0441321   0.0260835  -0.50366664] reward:  0\n",
      "planning time:  0.40384578704833984\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9997342303423391\n",
      "planning for trajectory step:  96\n",
      "action:  [-2.1634471   0.04605421 -0.03514911 -0.21342431] reward:  0\n",
      "planning time:  0.3875725269317627\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9999200294471979\n",
      "planning for trajectory step:  97\n",
      "action:  [-2.3134937   0.04654512 -0.04205605 -0.13803822] reward:  0\n",
      "planning time:  0.3867621421813965\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.9999898494206105\n",
      "planning for trajectory step:  98\n",
      "action:  [-2.1530623   0.04560576 -0.04263853 -0.08606788] reward:  0\n",
      "planning time:  0.401564359664917\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 1.0\n",
      "planning for trajectory step:  99\n",
      "action:  [-1.7798936   0.04352009 -0.03145767 -0.34790403] reward:  0\n",
      "planning time:  0.3871309757232666\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 1.0\n",
      "planning for trajectory step:  100\n",
      "action:  [-2.3843875   0.03902536  0.02638388 -0.55815613] reward:  0\n",
      "planning time:  0.38916611671447754\n",
      "setpoint updated\n",
      "setting setpoint to:  3.0885587886687033e-06 2.78809448505743e-06 0.9999993467303974\n",
      "planning for trajectory step:  101\n",
      "action:  [-1.1724237   0.04858911 -0.00468326 -0.35544363] reward:  0\n",
      "planning time:  0.46643972396850586\n",
      "setpoint updated\n",
      "setting setpoint to:  2.429138695756918e-05 2.1947645931174872e-05 0.9999948753982963\n",
      "planning for trajectory step:  102\n",
      "action:  [-2.282674    0.04531462 -0.04286058 -0.31969655] reward:  0\n",
      "planning time:  0.4217703342437744\n",
      "setpoint updated\n",
      "setting setpoint to:  8.058786502468508e-05 7.287816897856016e-05 0.9999830441076508\n",
      "planning for trajectory step:  103\n",
      "action:  [-1.1815852   0.0394499  -0.04110519 -0.1015342 ] reward:  0\n",
      "planning time:  0.4199180603027344\n",
      "setpoint updated\n",
      "setting setpoint to:  0.00018774373789089745 0.00016993930187635098 0.9999606059518429\n",
      "planning for trajectory step:  104\n",
      "action:  [-2.6435406   0.0455402   0.03740239 -0.5654267 ] reward:  0\n",
      "planning time:  0.4066741466522217\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0003603379822958694 0.00032647327323746003 0.9999246021023093\n",
      "planning for trajectory step:  105\n",
      "action:  [-0.7563337   0.04536554 -0.01422743 -0.11640949] reward:  0\n",
      "planning time:  0.5874354839324951\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0006117896742739303 0.0005548273687934203 0.9998723548971691\n",
      "planning for trajectory step:  106\n",
      "action:  [-1.3490618   0.04301263 -0.04406691 -0.095388  ] reward:  0\n",
      "planning time:  0.4608616828918457\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0009543848566099622 0.0008663763981492315 0.9998014609298507\n",
      "planning for trajectory step:  107\n",
      "action:  [ 2.1059594   0.04661642  0.00836925 -0.25856036] reward:  0\n",
      "planning time:  0.4387657642364502\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0013993034062952828 0.0012715451615382034 0.9997097841377188\n",
      "planning for trajectory step:  108\n",
      "action:  [-2.2120597   0.04388724 -0.04237969 -0.1637481 ] reward:  0\n",
      "planning time:  0.3752937316894531\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0019566459019835303 0.0017798309165768023 0.9995954488907018\n",
      "planning for trajectory step:  109\n",
      "action:  [-0.7585121   0.04502582 -0.04338045 -0.09520673] reward:  0\n",
      "planning time:  0.3765852451324463\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0026354604914465493 0.002399825845019498 0.9994568330799194\n",
      "planning for trajectory step:  110\n",
      "action:  [-0.36868793  0.04512758 -0.02885844 -0.15042193] reward:  0\n",
      "planning time:  0.38700413703918457\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0034437697590302754 0.003139239519513605 0.9992925612063088\n",
      "planning for trajectory step:  111\n",
      "action:  [ 2.5188751   0.04677934 -0.0038778  -0.09392437] reward:  0\n",
      "planning time:  0.3885304927825928\n",
      "setpoint updated\n",
      "setting setpoint to:  0.004388597593110617 0.004004921370354131 0.9991014974692525\n",
      "planning for trajectory step:  112\n",
      "action:  [-2.220389    0.01281332 -0.04175307 -0.38416427] reward:  0\n",
      "planning time:  0.3900308609008789\n",
      "setpoint updated\n",
      "setting setpoint to:  0.005475996053549348 0.005002883152238623 0.9988827388552057\n",
      "planning for trajectory step:  113\n",
      "action:  [-2.33403     0.04192412  0.0185865  -0.54905194] reward:  0\n",
      "planning time:  0.39049744606018066\n",
      "setpoint updated\n",
      "setting setpoint to:  0.006711072239149982 0.006138321411022013 0.998635608226323\n",
      "planning for trajectory step:  114\n",
      "action:  [-2.0760527   0.04569631  0.019809   -0.4464264 ] reward:  0\n",
      "planning time:  0.39226579666137695\n",
      "setpoint updated\n",
      "setting setpoint to:  0.008098015155113665 0.007415639950471457 0.9983596474090861\n",
      "planning for trajectory step:  115\n",
      "action:  [-2.1240597   0.04438625  0.01221481 -0.44005615] reward:  0\n",
      "planning time:  0.37284326553344727\n",
      "setpoint updated\n",
      "setting setpoint to:  0.009640122580495054 0.008838472299021186 0.9980546102829303\n",
      "planning for trajectory step:  116\n",
      "action:  [-2.0650373e+00 -3.6029407e-05 -2.9961510e-02 -4.3511730e-01] reward:  0\n",
      "planning time:  0.389406681060791\n",
      "setpoint updated\n",
      "setting setpoint to:  0.011339827935658206 0.01040970417652735 0.9977204558688719\n",
      "planning for trajectory step:  117\n",
      "action:  [-2.076687    0.04262732 -0.02954869 -0.00883773] reward:  0\n",
      "planning time:  0.3849337100982666\n",
      "setpoint updated\n",
      "setting setpoint to:  0.013198727149732465 0.012131495961022866 0.9973573414181364\n",
      "planning for trajectory step:  118\n",
      "action:  [-2.1462622   0.0461082  -0.04267971 -0.17024861] reward:  0\n",
      "planning time:  0.3893263339996338\n",
      "setpoint updated\n",
      "setting setpoint to:  0.015217605528068345 0.014005305155472264 0.9969656155007839\n",
      "planning for trajectory step:  119\n",
      "action:  [-1.9839638   0.03986704 -0.02397323 -0.38883084] reward:  0\n",
      "planning time:  0.40487027168273926\n",
      "setpoint updated\n",
      "setting setpoint to:  0.017396464619693405 0.016031908854526523 0.9965458110943378\n",
      "planning for trajectory step:  120\n",
      "action:  [-2.6296473e+00  5.7797311e-05 -2.0991415e-02 -4.0874451e-01] reward:  0\n",
      "planning time:  0.37476491928100586\n",
      "setpoint updated\n",
      "setting setpoint to:  0.019734549084768144 0.018211426211277915 0.9960986386724111\n",
      "planning for trajectory step:  121\n",
      "action:  [-2.7199094   0.04309255 -0.0287106  -0.08235609] reward:  0\n",
      "planning time:  0.3726024627685547\n",
      "setpoint updated\n",
      "setting setpoint to:  0.022230373562041893 0.020543340904014887 0.9956249792933339\n",
      "planning for trajectory step:  122\n",
      "action:  [-2.5280025   0.02908588 -0.03751253 -0.40004078] reward:  0\n",
      "planning time:  0.39226675033569336\n",
      "setpoint updated\n",
      "setting setpoint to:  0.024881749536308683 0.02302652360297685 0.995125877688781\n",
      "planning for trajectory step:  123\n",
      "action:  [-2.3868368   0.04508102 -0.03362155 -0.37422192] reward:  0\n",
      "planning time:  0.38761329650878906\n",
      "setpoint updated\n",
      "setting setpoint to:  0.02768581220586312 0.02565925443710903 0.9946025353523982\n",
      "planning for trajectory step:  124\n",
      "action:  [-1.9554759   0.04208931 -0.03726853 -0.17564806] reward:  0\n",
      "planning time:  0.40381646156311035\n",
      "setpoint updated\n",
      "setting setpoint to:  0.030639047349956337 0.028439245460817417 0.9940563036284301\n",
      "planning for trajectory step:  125\n",
      "action:  [-0.8964322   0.0446935  -0.03253524 -0.18381742] reward:  0\n",
      "planning time:  0.407073974609375\n",
      "setpoint updated\n",
      "setting setpoint to:  0.03373731819625178 0.03136366312072344 0.9934886768003471\n",
      "planning for trajectory step:  126\n",
      "action:  [-2.2513318   0.03914266  0.00381394 -0.50458235] reward:  0\n",
      "planning time:  0.4077277183532715\n",
      "setpoint updated\n",
      "setting setpoint to:  0.036975892288281166 0.03442915072241895 0.9929012851794725\n",
      "planning for trajectory step:  127\n",
      "action:  [-1.2725363   0.04517964  0.0153827  -0.4648689 ] reward:  0\n",
      "planning time:  0.38796377182006836\n",
      "setpoint updated\n",
      "setting setpoint to:  0.040349468352900326 0.03763185089722102 0.9922958881936104\n",
      "planning for trajectory step:  128\n",
      "action:  [-1.2046797   0.04358396  0.01270589 -0.51984084] reward:  0\n",
      "planning time:  0.37194085121154785\n",
      "setpoint updated\n",
      "setting setpoint to:  0.04385220316774513 0.04096742806892672 0.9916743674756713\n",
      "planning for trajectory step:  129\n",
      "action:  [-0.38734838  0.04684872 -0.03909525 -0.16660912] reward:  0\n",
      "planning time:  0.40571022033691406\n",
      "setpoint updated\n",
      "setting setpoint to:  0.04747773842868733 0.04443109092056815 0.9910387199523011\n",
      "planning for trajectory step:  130\n",
      "action:  [-1.748694    0.04571947  0.00717714 -0.51896846] reward:  0\n",
      "planning time:  0.3873744010925293\n",
      "setpoint updated\n",
      "setting setpoint to:  0.05121922761729046 0.04801761486116703 0.9903910509325071\n",
      "planning for trajectory step:  131\n",
      "action:  [-1.9016094   0.0430626  -0.03557428 -0.30461115] reward:  0\n",
      "planning time:  0.38756799697875977\n",
      "setpoint updated\n",
      "setting setpoint to:  0.05506936286826575 0.05172136449248976 0.9897335671962857\n",
      "planning for trajectory step:  132\n",
      "action:  [-0.69927174  0.04590692 -0.02114629 -0.08593732] reward:  0\n",
      "planning time:  0.37149739265441895\n",
      "setpoint updated\n",
      "setting setpoint to:  0.05902040183692793 0.055536316075802165 0.9890685700832494\n",
      "planning for trajectory step:  133\n",
      "action:  [-2.2287755   0.0455688  -0.02109704 -0.34526977] reward:  0\n",
      "planning time:  0.37528324127197266\n",
      "setpoint updated\n",
      "setting setpoint to:  0.06306419456665127 0.0594560799986244 0.9883984485812537\n",
      "planning for trajectory step:  134\n",
      "action:  [-0.8802288   0.04056716  0.02544138 -0.47197616] reward:  0\n",
      "planning time:  0.37493276596069336\n",
      "setpoint updated\n",
      "setting setpoint to:  0.06719221035632526 0.06347392324148564 0.9877256724150251\n",
      "planning for trajectory step:  135\n",
      "action:  [ 1.1318554   0.04716304  0.03146882 -0.5659918 ] reward:  0\n",
      "planning time:  0.3880653381347656\n",
      "setpoint updated\n",
      "setting setpoint to:  0.07139556462781065 0.06758279184467916 0.9870527851347876\n",
      "planning for trajectory step:  136\n",
      "action:  [-1.765032    0.04318099 -0.03227931 -0.4224386 ] reward:  0\n",
      "planning time:  0.4164869785308838\n",
      "setpoint updated\n",
      "setting setpoint to:  0.07566504579339528 0.07177533337501707 0.9863823972048898\n",
      "planning for trajectory step:  137\n",
      "action:  [-1.6017971   0.04655661 -0.041184   -0.1100272 ] reward:  0\n",
      "planning time:  0.3864421844482422\n",
      "setpoint updated\n",
      "setting setpoint to:  0.07999114212325001 0.07604391939258504 0.9857171790924327\n",
      "planning for trajectory step:  138\n",
      "action:  [-2.1679618   0.04605974 -0.04051163 -0.49432725] reward:  0\n",
      "planning time:  0.3763110637664795\n",
      "setpoint updated\n",
      "setting setpoint to:  0.08436406861288441 0.08038066791749732 0.9850598543558967\n",
      "planning for trajectory step:  139\n",
      "action:  [-2.5658779e+00 -9.1214388e-05 -4.1322362e-02 -3.5454205e-01] reward:  0\n",
      "planning time:  0.4028613567352295\n",
      "setpoint updated\n",
      "setting setpoint to:  0.08877379385060298 0.08477746589665153 0.9844131927337679\n",
      "planning for trajectory step:  140\n",
      "action:  [-1.1065518   0.04484962  0.02164271 -0.5358667 ] reward:  0\n",
      "planning time:  0.40619730949401855\n",
      "setpoint updated\n",
      "setting setpoint to:  0.09321006688496078 0.08922599167048345 0.9837800032331668\n",
      "planning for trajectory step:  141\n",
      "action:  [-2.0815578   0.04210881 -0.02338845 -0.48538217] reward:  0\n",
      "planning time:  0.39047694206237793\n",
      "setpoint updated\n",
      "setting setpoint to:  0.09766244409221939 0.09371773743972203 0.9831631272184742\n",
      "planning for trajectory step:  142\n",
      "action:  [-1.1527451   0.04308255 -0.03606239 -0.11525256] reward:  0\n",
      "planning time:  0.43529415130615234\n",
      "setpoint updated\n",
      "setting setpoint to:  0.10212031604380278 0.09824403173214402 0.9825654314999593\n",
      "planning for trajectory step:  143\n",
      "action:  [-2.3599672   0.04398521  0.01158246 -0.43674493] reward:  0\n",
      "planning time:  0.38840365409851074\n",
      "setpoint updated\n",
      "setting setpoint to:  0.1065729343737532 0.10279606186932888 0.9819898014224061\n",
      "planning for trajectory step:  144\n",
      "action:  [-2.5732484   0.0134721  -0.04175993 -0.4722438 ] reward:  0\n",
      "planning time:  0.3893587589263916\n",
      "setpoint updated\n",
      "setting setpoint to:  0.1110094386461871 0.10736489643341374 0.9814391339537409\n",
      "planning for trajectory step:  145\n",
      "action:  [-1.9727554   0.044413   -0.03975379 -0.13575521] reward:  0\n",
      "planning time:  0.3861668109893799\n",
      "setpoint updated\n",
      "setting setpoint to:  0.11541888322275103 0.1119415077338482 0.98091633077366\n",
      "planning for trajectory step:  146\n",
      "action:  [-1.041814    0.04253672 -0.0365563  -0.08918035] reward:  0\n",
      "planning time:  0.37194371223449707\n",
      "setpoint updated\n",
      "setting setpoint to:  0.11979026413007733 0.11651679427414893 0.9804242913622563\n",
      "planning for trajectory step:  147\n",
      "action:  [-2.40788     0.0462064  -0.04039957 -0.39709783] reward:  0\n",
      "planning time:  0.3882753849029541\n",
      "setpoint updated\n",
      "setting setpoint to:  0.12411254592724025 0.12108160321865494 0.979965906088646\n",
      "planning for trajectory step:  148\n",
      "action:  [-1.7398652   0.04094458  0.01207193 -0.52544576] reward:  0\n",
      "planning time:  0.45138072967529297\n",
      "setpoint updated\n",
      "setting setpoint to:  0.12837468857321194 0.1256267528592822 0.9795440492995973\n",
      "planning for trajectory step:  149\n",
      "action:  [-2.4843986   0.04565107  0.00587285 -0.46723855] reward:  0\n",
      "planning time:  0.38897037506103516\n",
      "setpoint updated\n",
      "setting setpoint to:  0.13256567429431776 0.1301430550822784 0.9791615724081562\n",
      "planning for trajectory step:  150\n",
      "action:  [ 0.8169157   0.04513098 -0.04325976 -0.07607304] reward:  0\n",
      "planning time:  0.3764340877532959\n",
      "setpoint updated\n",
      "setting setpoint to:  0.13667453445169286 0.13462133783497787 0.9788212969822739\n",
      "planning for trajectory step:  151\n",
      "action:  [-2.1738386   0.03979197 -0.04048048 -0.08964171] reward:  0\n",
      "planning time:  0.3746631145477295\n",
      "setpoint updated\n",
      "setting setpoint to:  0.14069037640873772 0.13905246759255657 0.978526007833435\n",
      "planning for trajectory step:  152\n",
      "action:  [-1.6053283   0.04632422 -0.0262412  -0.2324366 ] reward:  0\n",
      "planning time:  0.39226746559143066\n",
      "setpoint updated\n",
      "setting setpoint to:  0.14460241039857397 0.14342737182478682 0.9782784461052833\n",
      "planning for trajectory step:  153\n",
      "action:  [-1.3988701   0.04592562  0.01681054 -0.5282732 ] reward:  0\n",
      "planning time:  0.38901400566101074\n",
      "setpoint updated\n",
      "setting setpoint to:  0.14839997639150038 0.1477370614627918 0.9780813023622499\n",
      "planning for trajectory step:  154\n",
      "action:  [-1.6538084   0.04323577 -0.03380528 -0.15634067] reward:  0\n",
      "planning time:  0.3891940116882324\n",
      "setpoint updated\n",
      "setting setpoint to:  0.15207257096244864 0.15197265336580135 0.9779372096781802\n",
      "planning for trajectory step:  155\n",
      "action:  [-0.62032366  0.04560873 -0.00447683 -0.06038933] reward:  0\n",
      "planning time:  0.38193607330322266\n",
      "setpoint updated\n",
      "setting setpoint to:  0.15560987415843974 0.15612539278790555 0.9778487367249605\n",
      "planning for trajectory step:  156\n",
      "action:  [-1.216101    0.04603941  0.01470811 -0.4071118 ] reward:  0\n",
      "planning time:  0.3913233280181885\n",
      "setpoint updated\n",
      "setting setpoint to:  0.15900177636603913 0.1601866758448107 0.9778183808611465\n",
      "planning for trajectory step:  157\n",
      "action:  [-1.9644974   0.04485423 -0.03405964 -0.16743593] reward:  0\n",
      "planning time:  0.372478723526001\n",
      "setpoint updated\n",
      "setting setpoint to:  0.16223840517881274 0.1641480719805931 0.977848561220589\n",
      "planning for trajectory step:  158\n",
      "action:  [ 0.9190264   0.04550078 -0.00132608 -0.12897483] reward:  0\n",
      "planning time:  0.3914451599121094\n",
      "setpoint updated\n",
      "setting setpoint to:  0.16531015226478368 0.16800134643445552 0.9779416118010619\n",
      "planning for trajectory step:  159\n",
      "action:  [-0.7824186   0.04439672 -0.02188784 -0.11381254] reward:  0\n",
      "planning time:  0.43466639518737793\n",
      "setpoint updated\n",
      "setting setpoint to:  0.1682077002338869 0.17173848270747996 0.9780997745528895\n",
      "planning for trajectory step:  160\n",
      "action:  [-1.5840715   0.04508125 -0.0432345  -0.0798166 ] reward:  0\n",
      "planning time:  0.4043290615081787\n",
      "setpoint updated\n",
      "setting setpoint to:  0.17092204950542592 0.17535170502938377 0.9783251924675729\n",
      "planning for trajectory step:  161\n",
      "action:  [-1.3554941   0.0414238  -0.02300189 -0.4949162 ] reward:  0\n",
      "planning time:  0.40136051177978516\n",
      "setpoint updated\n",
      "setting setpoint to:  0.17344454517552865 0.1788335008252747 0.9786199026664184\n",
      "planning for trajectory step:  162\n",
      "action:  [-1.925147    0.04571934 -0.04166657 -0.55589455] reward:  0\n",
      "planning time:  0.4361135959625244\n",
      "setpoint updated\n",
      "setting setpoint to:  0.17576690388460275 0.18217664318240473 0.9789858294891635\n",
      "planning for trajectory step:  163\n",
      "action:  [-2.2843735   0.04398487 -0.03260994 -0.279867  ] reward:  0\n",
      "planning time:  0.402310848236084\n",
      "setpoint updated\n",
      "setting setpoint to:  0.17788124068479208 0.185374213316926 0.9794247775826047\n",
      "planning for trajectory step:  164\n",
      "action:  [-0.8813435   0.04600292 -0.03761506 -0.17535359] reward:  0\n",
      "planning time:  0.3909788131713867\n",
      "setpoint updated\n",
      "setting setpoint to:  0.17978009590743227 0.18841962304064463 0.9799384249892247\n",
      "planning for trajectory step:  165\n",
      "action:  [-1.8752053   0.04371824 -0.03727246 -0.39760497] reward:  0\n",
      "planning time:  0.4360785484313965\n",
      "setpoint updated\n",
      "setting setpoint to:  0.1814564620305067 0.1913066372277767 0.9805283162358189\n",
      "planning for trajectory step:  166\n",
      "action:  [-1.8201805   0.04407371  0.02169087 -0.48693472] reward:  0\n",
      "planning time:  0.3902726173400879\n",
      "setpoint updated\n",
      "setting setpoint to:  0.18290381054610272 0.1940293962817019 0.9811958554221242\n",
      "planning for trajectory step:  167\n",
      "action:  [-1.282704    0.03947264  0.01800139 -0.49741444] reward:  0\n",
      "planning time:  0.4185292720794678\n",
      "setpoint updated\n",
      "setting setpoint to:  0.18411611882786683 0.19658243860171964 0.981942299309444\n",
      "planning for trajectory step:  168\n",
      "action:  [-2.0204012   0.04328957 -0.00585993 -0.17381471] reward:  0\n",
      "planning time:  0.3900012969970703\n",
      "setpoint updated\n",
      "setting setpoint to:  0.18508789699846073 0.1989607230498019 0.9827687504092775\n",
      "planning for trajectory step:  169\n",
      "action:  [-1.3008962   0.04595147 -0.03707629 -0.09795997] reward:  0\n",
      "planning time:  0.38490843772888184\n",
      "setpoint updated\n",
      "setting setpoint to:  0.18581421479701798 0.20115965141735098 0.9836761500719449\n",
      "planning for trajectory step:  170\n",
      "action:  [-1.7497323   0.04677942 -0.02854846 -0.44380942] reward:  0\n",
      "planning time:  0.3854711055755615\n",
      "setpoint updated\n",
      "setting setpoint to:  0.18629072844659889 0.20317509089195185 0.9846652715752162\n",
      "planning for trajectory step:  171\n",
      "action:  [-2.1507132   0.0458148   0.04410436 -0.55504274] reward:  0\n",
      "planning time:  0.40332865715026855\n",
      "setpoint updated\n",
      "setting setpoint to:  0.18651370752164687 0.20500339652412763 0.985736713212938\n",
      "planning for trajectory step:  172\n",
      "action:  [-2.394416    0.04464379 -0.04176622 -0.5067443 ] reward:  0\n",
      "planning time:  0.3879573345184326\n",
      "setpoint updated\n",
      "setting setpoint to:  0.1864800618154447 0.20664143369409593 0.9868908913836594\n",
      "planning for trajectory step:  173\n",
      "action:  [-0.14629404  0.04482374 -0.02581171 -0.07984139] reward:  0\n",
      "planning time:  0.4052574634552002\n",
      "setpoint updated\n",
      "setting setpoint to:  0.18618736820756965 0.20808660057852107 0.9881280336792615\n",
      "planning for trajectory step:  174\n",
      "action:  [-1.2413794   0.04470704 -0.0134417  -0.49623144] reward:  0\n",
      "planning time:  0.37375950813293457\n",
      "setpoint updated\n",
      "setting setpoint to:  0.18563389753134985 0.20933685061727092 0.9894481719735828\n",
      "planning for trajectory step:  175\n",
      "action:  [-1.2759771   0.04405963 -0.01885764 -0.36676836] reward:  0\n",
      "planning time:  0.41582608222961426\n",
      "setpoint updated\n",
      "setting setpoint to:  0.18481864144131943 0.21039071498017142 0.9908511355110462\n",
      "planning for trajectory step:  176\n",
      "action:  [-0.03903729  0.04121792 -0.04247211 -0.0537832 ] reward:  0\n",
      "planning time:  0.4194605350494385\n",
      "setpoint updated\n",
      "setting setpoint to:  0.18374133928067538 0.21124732503376054 0.9923365439952885\n",
      "planning for trajectory step:  177\n",
      "action:  [-0.5081067   0.04369001 -0.02143623 -0.17447281] reward:  0\n",
      "planning time:  0.4236013889312744\n",
      "setpoint updated\n",
      "setting setpoint to:  0.18240250494873378 0.21190643480804439 0.9939038006777843\n",
      "planning for trajectory step:  178\n",
      "action:  [-2.1073985   0.03833929  0.01350251 -0.4498445 ] reward:  0\n",
      "planning time:  0.3871798515319824\n",
      "setpoint updated\n",
      "setting setpoint to:  0.18080345376838414 0.21236844346325057 0.9955520854464759\n",
      "planning for trajectory step:  179\n",
      "action:  [-1.9289851   0.04473561 -0.01593871 -0.44139907] reward:  0\n",
      "planning time:  0.38650965690612793\n",
      "setpoint updated\n",
      "setting setpoint to:  0.17894632935354593 0.2126344177565851 0.9972803479143988\n",
      "planning for trajectory step:  180\n",
      "action:  [-1.4265853   0.04172814  0.01016568 -0.30194122] reward:  0\n",
      "planning time:  0.3898618221282959\n",
      "setpoint updated\n",
      "setting setpoint to:  0.17683413047662555 0.2127061145089848 0.9990873005083105\n",
      "planning for trajectory step:  181\n",
      "action:  [ 1.6254992   0.04616414  0.02623552 -0.16683438] reward:  0\n",
      "planning time:  0.3860948085784912\n",
      "setpoint updated\n",
      "setting setpoint to:  0.17447073793597057 0.21258600307187336 1.0009714115573154\n",
      "planning for trajectory step:  182\n",
      "action:  [-1.7016888   0.04681268 -0.03619327 -0.23456234] reward:  0\n",
      "planning time:  0.39119386672973633\n",
      "setpoint updated\n",
      "setting setpoint to:  0.17186094142332664 0.21227728779391764 1.0029308983814944\n",
      "planning for trajectory step:  183\n",
      "action:  [-2.0053272   0.04339657 -0.03632268 -0.4654185 ] reward:  0\n",
      "planning time:  0.3930037021636963\n",
      "setpoint updated\n",
      "setting setpoint to:  0.1690104663912927 0.21178393048777955 1.0049637203805302\n",
      "planning for trajectory step:  184\n",
      "action:  [-2.1601834   0.04095184 -0.04037956 -0.10880998] reward:  0\n",
      "planning time:  0.4006359577178955\n",
      "setpoint updated\n",
      "setting setpoint to:  0.16592600092077803 0.21111067289687324 1.0070675721223363\n",
      "planning for trajectory step:  185\n",
      "action:  [-1.4711059   0.03405643 -0.041791   -0.12563895] reward:  0\n",
      "planning time:  0.3882155418395996\n",
      "setpoint updated\n",
      "setting setpoint to:  0.1626152225884565 0.21026305916211863 1.0092398764316821\n",
      "planning for trajectory step:  186\n",
      "action:  [-1.6793686   0.04344914 -0.0278523  -0.5303632 ] reward:  0\n",
      "planning time:  0.3908390998840332\n",
      "setpoint updated\n",
      "setting setpoint to:  0.1590868253342237 0.20924745828869673 1.0114777774788215\n",
      "planning for trajectory step:  187\n",
      "action:  [-2.1789958   0.00852999 -0.03306825 -0.3532639 ] reward:  0\n",
      "planning time:  0.37221431732177734\n",
      "setpoint updated\n",
      "setting setpoint to:  0.15535054632865286 0.2080710866128047 1.01377813386812\n",
      "planning for trajectory step:  188\n",
      "action:  [ 0.3428851   0.04616124 -0.03471877 -0.34236807] reward:  0\n",
      "planning time:  0.3883676528930664\n",
      "setpoint updated\n",
      "setting setpoint to:  0.15141719284044952 0.2067420302684111 1.0161375117266813\n",
      "planning for trajectory step:  189\n",
      "action:  [-2.1641707   0.04655948  0.01080718 -0.41647723] reward:  0\n",
      "planning time:  0.38827991485595703\n",
      "setpoint updated\n",
      "setting setpoint to:  0.14729866910390887 0.20526926765400821 1.0185521777929751\n",
      "planning for trajectory step:  190\n",
      "action:  [-2.7873182   0.04331851 -0.01646877 -0.4943581 ] reward:  0\n",
      "planning time:  0.3891119956970215\n",
      "setpoint updated\n",
      "setting setpoint to:  0.14300800318637008 0.20366269189937092 1.021018092505464\n",
      "planning for trajectory step:  191\n",
      "action:  [-2.1873307   0.0442572  -0.03242867 -0.40907773] reward:  0\n",
      "planning time:  0.38806962966918945\n",
      "setpoint updated\n",
      "setting setpoint to:  0.13855937385567474 0.2019331333323091 1.0235309030912294\n",
      "planning for trajectory step:  192\n",
      "action:  [-1.8813814   0.04476246 -0.04691349 -0.22630256] reward:  0\n",
      "planning time:  0.37535977363586426\n",
      "setpoint updated\n",
      "setting setpoint to:  0.13396813744761826 0.20009238194542034 1.0260859366546022\n",
      "planning for trajectory step:  193\n",
      "action:  [-0.47732973  0.04716193 -0.02426036 -0.07534838] reward:  0\n",
      "planning time:  0.3785586357116699\n",
      "setpoint updated\n",
      "setting setpoint to:  0.1292508547334128 0.1981532098628518 1.0286781932657862\n",
      "planning for trajectory step:  194\n",
      "action:  [-2.1475418   0.04306343 -0.02675508 -0.08941554] reward:  0\n",
      "planning time:  0.3721432685852051\n",
      "setpoint updated\n",
      "setting setpoint to:  0.12442531778713373 0.1961293938070452 1.0313023390494873\n",
      "planning for trajectory step:  195\n",
      "action:  [-1.7147827   0.04292215 -0.02068859 -0.41045928] reward:  0\n",
      "planning time:  0.3858945369720459\n",
      "setpoint updated\n",
      "setting setpoint to:  0.11951057685318545 0.1940357375655013 1.03395269927354\n",
      "planning for trajectory step:  196\n",
      "action:  [ 3.1158445   0.04043824  0.03766745 -0.26785415] reward:  0\n",
      "planning time:  0.40230512619018555\n",
      "setpoint updated\n",
      "setting setpoint to:  0.11452696721374944 0.19188809445752786 1.0366232514375349\n",
      "planning for trajectory step:  197\n",
      "action:  [-1.4701709   0.03580228 -0.03760321 -0.07009062] reward:  0\n",
      "planning time:  0.39069628715515137\n",
      "setpoint updated\n",
      "setting setpoint to:  0.10949613605624586 0.1897033898009992 1.039307618361446\n",
      "planning for trajectory step:  198\n",
      "action:  [-2.334577   -0.04519879 -0.04422885 -0.11183054] reward:  0\n",
      "planning time:  0.40340280532836914\n",
      "setpoint updated\n",
      "setting setpoint to:  0.1044410693407829 0.18749964337910607 1.0419990612742573\n",
      "planning for trajectory step:  199\n",
      "action:  [ 0.07374255  0.00591172  0.04152317 -0.0493837 ] reward:  0\n",
      "planning time:  0.3891885280609131\n",
      "resetting environment, and starting trial : 1\n",
      "Connected!\n",
      "4 DoF [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "No propeller. Is it a magic box?\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 1.0150579389358555e-05\n",
      "Number of stored transitions:  10000\n",
      "Training model\n",
      "training loss:  -46.56162939935722\n",
      "validation loss:  1.305552723351866e-05\n",
      "training time:  58.104952812194824\n",
      "Model trained\n",
      "planning for trajectory step:  1\n",
      "action:  [ 0.72651714 -0.00200282  0.01683695 -0.2785252 ] reward:  0\n",
      "planning time:  0.396587610244751\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 7.997055280231069e-05\n",
      "planning for trajectory step:  2\n",
      "action:  [-0.04232252 -0.02124675  0.00625264 -0.26795578] reward:  0\n",
      "planning time:  0.38872718811035156\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.00026576965766001126\n",
      "planning for trajectory step:  3\n",
      "action:  [ 1.7294539  -0.00303015  0.01373402 -0.3004473 ] reward:  0\n",
      "planning time:  0.3993363380432129\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0006202613792457937\n",
      "planning for trajectory step:  4\n",
      "action:  [-0.23662335 -0.01730093 -0.00425588 -0.14516333] reward:  0\n",
      "planning time:  0.31496262550354004\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0011926386612764923\n",
      "planning for trajectory step:  5\n",
      "action:  [-0.17160049 -0.0094277  -0.00267948 -0.24849805] reward:  0\n",
      "planning time:  0.32500720024108887\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.002028649616473763\n",
      "planning for trajectory step:  6\n",
      "action:  [-0.22128412 -0.00877326 -0.01182381 -0.20951018] reward:  0\n",
      "planning time:  0.391237735748291\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.003170673237135412\n",
      "planning for trajectory step:  7\n",
      "action:  [ 0.9062188  -0.02732274  0.0162122  -0.33632037] reward:  0\n",
      "planning time:  0.34409117698669434\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.004657795105706712\n",
      "planning for trajectory step:  8\n",
      "action:  [-0.76768786 -0.00822128 -0.00400907 -0.37170222] reward:  0\n",
      "planning time:  0.3392946720123291\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.006525883105351723\n",
      "planning for trajectory step:  9\n",
      "action:  [ 0.24193354 -0.01129494 -0.01063608 -0.12060161] reward:  0\n",
      "planning time:  0.3603823184967041\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.008807663130524624\n",
      "planning for trajectory step:  10\n",
      "action:  [-0.28499243  0.00600354 -0.00503808 -0.1765872 ] reward:  0\n",
      "planning time:  0.3706998825073242\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.011532794797541032\n",
      "planning for trajectory step:  11\n",
      "action:  [ 0.59632605 -0.00580644 -0.03147676 -0.20075977] reward:  0\n",
      "planning time:  0.31279873847961426\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.014727947155149303\n",
      "planning for trajectory step:  12\n",
      "action:  [ 1.6583169  -0.02161459  0.00463268 -0.4347659 ] reward:  0\n",
      "planning time:  0.3289332389831543\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.018416874395101913\n",
      "planning for trajectory step:  13\n",
      "action:  [ 0.862428   -0.02690783  0.04506199 -0.49335107] reward:  0\n",
      "planning time:  0.6191272735595703\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.022620491562726712\n",
      "planning for trajectory step:  14\n",
      "action:  [-1.7014247  -0.01040186 -0.00293351 -0.34069154] reward:  0\n",
      "planning time:  0.477733850479126\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.027356950267498275\n",
      "planning for trajectory step:  15\n",
      "action:  [ 2.4758053   0.00352215  0.03211624 -0.06724628] reward:  0\n",
      "planning time:  0.5434796810150146\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.03264171439360926\n",
      "planning for trajectory step:  16\n",
      "action:  [-0.25065374  0.02425697 -0.02794984 -0.37385368] reward:  0\n",
      "planning time:  0.5608446598052979\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.03848763581054164\n",
      "planning for trajectory step:  17\n",
      "action:  [ 0.60345596  0.00102151  0.04110926 -0.50535345] reward:  0\n",
      "planning time:  0.5729563236236572\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.044905030083638116\n",
      "planning for trajectory step:  18\n",
      "action:  [ 3.0600135  -0.0038408  -0.01816028 -0.43384475] reward:  0\n",
      "planning time:  0.49431657791137695\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.05190175218467344\n",
      "planning for trajectory step:  19\n",
      "action:  [ 1.011624   -0.03741529  0.02965232 -0.5134343 ] reward:  0\n",
      "planning time:  0.4784820079803467\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.05948327220242562\n",
      "planning for trajectory step:  20\n",
      "action:  [ 0.58210224 -0.02800018 -0.02044403 -0.17250824] reward:  0\n",
      "planning time:  0.6186883449554443\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0676527510532474\n",
      "planning for trajectory step:  21\n",
      "action:  [ 2.831082    0.00500818  0.03573776 -0.08510298] reward:  0\n",
      "planning time:  0.5729610919952393\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.07641111619163748\n",
      "planning for trajectory step:  22\n",
      "action:  [ 2.317053    0.03304343 -0.03995115 -0.4233069 ] reward:  0\n",
      "planning time:  0.46747660636901855\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.0857571373208119\n",
      "planning for trajectory step:  23\n",
      "action:  [ 0.02242631 -0.03527487 -0.00921232 -0.3387455 ] reward:  0\n",
      "planning time:  0.31043004989624023\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.09568750210327515\n",
      "planning for trajectory step:  24\n",
      "action:  [-1.2000564e+00 -3.9269786e-02 -9.8601577e-04 -3.1798294e-01] reward:  0\n",
      "planning time:  0.3287086486816406\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.10619689187139202\n",
      "planning for trajectory step:  25\n",
      "action:  [ 0.48542568 -0.01860432  0.00516088 -0.10625623] reward:  0\n",
      "planning time:  0.32483768463134766\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.11727805733795829\n",
      "planning for trajectory step:  26\n",
      "action:  [-0.55670315 -0.00943065 -0.03148746 -0.1956549 ] reward:  0\n",
      "planning time:  0.3293185234069824\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.12892189430677242\n",
      "planning for trajectory step:  27\n",
      "action:  [ 1.2119596  -0.03463358  0.02086134 -0.54901034] reward:  0\n",
      "planning time:  0.3448495864868164\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.14111751938320677\n",
      "planning for trajectory step:  28\n",
      "action:  [ 0.05943269 -0.03399607  0.02398764 -0.34991333] reward:  0\n",
      "planning time:  0.3599865436553955\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.15385234568477898\n",
      "planning for trajectory step:  29\n",
      "action:  [ 1.3843316   0.03204296  0.02125588 -0.5264593 ] reward:  0\n",
      "planning time:  0.3122842311859131\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.1671121585517233\n",
      "planning for trajectory step:  30\n",
      "action:  [-0.0172533  -0.03380382  0.00071026 -0.39155498] reward:  0\n",
      "planning time:  0.3430297374725342\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.1808811912575617\n",
      "planning for trajectory step:  31\n",
      "action:  [-1.8423157  -0.03700669 -0.04418996 -0.37467852] reward:  0\n",
      "planning time:  0.3141450881958008\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.1951422007196756\n",
      "planning for trajectory step:  32\n",
      "action:  [ 2.02675     0.00228693  0.02039707 -0.10209604] reward:  0\n",
      "planning time:  0.32945847511291504\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.2098765432098766\n",
      "planning for trajectory step:  33\n",
      "action:  [ 1.6591359   0.00711548  0.03516668 -0.22456123] reward:  0\n",
      "planning time:  0.31589603424072266\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.22506425006497865\n",
      "planning for trajectory step:  34\n",
      "action:  [ 0.12242538 -0.00578043 -0.03697754 -0.11186031] reward:  0\n",
      "planning time:  0.31131410598754883\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.24068410339736837\n",
      "planning for trajectory step:  35\n",
      "action:  [ 1.3479862  -0.03675589  0.00163541 -0.19409148] reward:  0\n",
      "planning time:  0.33087801933288574\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.25671371180557717\n",
      "planning for trajectory step:  36\n",
      "action:  [ 3.0112176e+00  8.7425445e-04 -4.3323286e-02 -4.7670200e-02] reward:  0\n",
      "planning time:  0.35982179641723633\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.2731295860848524\n",
      "planning for trajectory step:  37\n",
      "action:  [ 1.0234166  -0.02868553 -0.01765518 -0.08938564] reward:  0\n",
      "planning time:  0.47782301902770996\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.2899072149377282\n",
      "planning for trajectory step:  38\n",
      "action:  [ 0.23881598 -0.03570624 -0.03590456 -0.06550514] reward:  0\n",
      "planning time:  0.4689466953277588\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.3070211406845973\n",
      "planning for trajectory step:  39\n",
      "action:  [-2.0442607e+00 -9.8264916e-04 -2.5129094e-04 -4.1506514e-01] reward:  0\n",
      "planning time:  0.4384434223175049\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.3244450349742825\n",
      "planning for trajectory step:  40\n",
      "action:  [-0.464277   -0.00837792 -0.00318665 -0.41565275] reward:  0\n",
      "planning time:  0.4356720447540283\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.3421517744946073\n",
      "planning for trajectory step:  41\n",
      "action:  [-2.2103140e+00  2.7520820e-03 -1.6439344e-03 -4.8887748e-01] reward:  0\n",
      "planning time:  0.43616819381713867\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.360113516682968\n",
      "planning for trajectory step:  42\n",
      "action:  [-0.89736164  0.03016872 -0.01525175 -0.5165748 ] reward:  0\n",
      "planning time:  0.32918858528137207\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.37830177543690424\n",
      "planning for trajectory step:  43\n",
      "action:  [-0.8901461   0.02857797 -0.02199731 -0.03255555] reward:  0\n",
      "planning time:  0.3121683597564697\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.39668749682467125\n",
      "planning for trajectory step:  44\n",
      "action:  [ 1.8525132   0.04254447  0.0124734  -0.48887587] reward:  0\n",
      "planning time:  0.32701587677001953\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.41524113479581026\n",
      "planning for trajectory step:  45\n",
      "action:  [-0.97357345 -0.02886515 -0.00407201 -0.47163045] reward:  0\n",
      "planning time:  0.35733819007873535\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.4339327268917206\n",
      "planning for trajectory step:  46\n",
      "action:  [-1.4881957  -0.0025787  -0.03633962 -0.47649434] reward:  0\n",
      "planning time:  0.48256945610046387\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.45273196995622983\n",
      "planning for trajectory step:  47\n",
      "action:  [ 2.3312964   0.03985083  0.01481839 -0.4522545 ] reward:  0\n",
      "planning time:  0.48142004013061523\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.4716082958461666\n",
      "planning for trajectory step:  48\n",
      "action:  [ 0.0948153  -0.03538385 -0.0311649  -0.3787555 ] reward:  0\n",
      "planning time:  0.4510915279388428\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.4905309471419318\n",
      "planning for trajectory step:  49\n",
      "action:  [-0.1263795  -0.03882585 -0.01068275 -0.45170945] reward:  0\n",
      "planning time:  0.511136531829834\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5094690528580682\n",
      "planning for trajectory step:  50\n",
      "action:  [-0.96285886 -0.00941952 -0.02644745 -0.11936035] reward:  0\n",
      "planning time:  0.31117796897888184\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5283917041538334\n",
      "planning for trajectory step:  51\n",
      "action:  [-0.37225458 -0.02729252 -0.00843999 -0.09458192] reward:  0\n",
      "planning time:  0.29793357849121094\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5472680300437707\n",
      "planning for trajectory step:  52\n",
      "action:  [-0.29525506 -0.02148406  0.01437066 -0.23161544] reward:  0\n",
      "planning time:  0.3159620761871338\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5660672731082802\n",
      "planning for trajectory step:  53\n",
      "action:  [ 2.0571265  -0.00561174  0.03178308 -0.14249255] reward:  0\n",
      "planning time:  0.3419303894042969\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.5847588652041902\n",
      "planning for trajectory step:  54\n",
      "action:  [ 0.25118253 -0.03513151 -0.01685126 -0.20498204] reward:  0\n",
      "planning time:  0.4658699035644531\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.603312503175329\n",
      "planning for trajectory step:  55\n",
      "action:  [ 2.4705267e+00  1.8945248e-03  4.0694479e-02 -9.9174030e-02] reward:  0\n",
      "planning time:  0.37581539154052734\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.621698224563096\n",
      "planning for trajectory step:  56\n",
      "action:  [-1.73645     0.00292873 -0.01238533 -0.523944  ] reward:  0\n",
      "planning time:  0.3908679485321045\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.6398864833170326\n",
      "planning for trajectory step:  57\n",
      "action:  [ 0.04984305 -0.01036885 -0.00024168 -0.20695612] reward:  0\n",
      "planning time:  0.3877568244934082\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.657848225505393\n",
      "planning for trajectory step:  58\n",
      "action:  [-0.6553284  -0.01865841 -0.03390954 -0.1505855 ] reward:  0\n",
      "planning time:  0.37004876136779785\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.6755549650257178\n",
      "planning for trajectory step:  59\n",
      "action:  [ 1.329085    0.02699975 -0.0266748  -0.4010913 ] reward:  0\n",
      "planning time:  0.34371495246887207\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.6929788593154029\n",
      "planning for trajectory step:  60\n",
      "action:  [-2.022571   -0.00560621 -0.00877958 -0.51320696] reward:  0\n",
      "planning time:  0.3575916290283203\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.7100927850622721\n",
      "planning for trajectory step:  61\n",
      "action:  [-1.2958444  -0.03958179 -0.03916227 -0.55526507] reward:  0\n",
      "planning time:  0.34252214431762695\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.7268704139151478\n",
      "planning for trajectory step:  62\n",
      "action:  [-0.46810955 -0.00912032  0.00414358 -0.49943203] reward:  0\n",
      "planning time:  0.3582944869995117\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.7432862881944238\n",
      "planning for trajectory step:  63\n",
      "action:  [-0.8447681  -0.00397575  0.00691882 -0.5269606 ] reward:  0\n",
      "planning time:  0.3415350914001465\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.7593158966026328\n",
      "planning for trajectory step:  64\n",
      "action:  [-1.4044712   0.00218382  0.01286785 -0.5262239 ] reward:  0\n",
      "planning time:  0.6836893558502197\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.7749357499350218\n",
      "planning for trajectory step:  65\n",
      "action:  [-0.6000746  -0.01224676 -0.03594431 -0.36229566] reward:  0\n",
      "planning time:  0.495009183883667\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.7901234567901237\n",
      "planning for trajectory step:  66\n",
      "action:  [ 2.248853   -0.02917202  0.02761134 -0.0858499 ] reward:  0\n",
      "planning time:  0.436368465423584\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8048577992803249\n",
      "planning for trajectory step:  67\n",
      "action:  [-2.5783198   0.00585818  0.04018133 -0.07043052] reward:  0\n",
      "planning time:  0.384918212890625\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8191188087424394\n",
      "planning for trajectory step:  68\n",
      "action:  [ 2.2680547  -0.00251082  0.03640866 -0.13452467] reward:  0\n",
      "planning time:  0.3563804626464844\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8328878414482771\n",
      "planning for trajectory step:  69\n",
      "action:  [-0.17784171 -0.04410113 -0.00280849 -0.1921761 ] reward:  0\n",
      "planning time:  0.46449923515319824\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8461476543152218\n",
      "planning for trajectory step:  70\n",
      "action:  [-1.937067   -0.00381399 -0.01686827 -0.3864982 ] reward:  0\n",
      "planning time:  0.466381311416626\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8588824806167938\n",
      "planning for trajectory step:  71\n",
      "action:  [-1.6550443  -0.00534242 -0.01747899 -0.48575842] reward:  0\n",
      "planning time:  0.4817640781402588\n",
      "setpoint updated\n",
      "setting setpoint to:  0.0 0.0 0.8710781056932277\n",
      "planning for trajectory step:  72\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 102\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplanning for trajectory step: \u001b[39m\u001b[38;5;124m\"\u001b[39m, model_env\u001b[38;5;241m.\u001b[39mtrajectory_step)\n\u001b[0;32m    101\u001b[0m planning_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 102\u001b[0m next_obs, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcommon_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_env_and_add_to_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplanning time: \u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m planning_start_time)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# update_axes(\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m#     axs, env.render(), ax_text, trial, steps_trial, all_rewards)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\ksubh\\onedrive\\documents\\swarmslab_rl\\summer 25\\implementations\\mbrl_lib_maintained\\mbrl-lib-experiments\\mbrl\\util\\common.py:606\u001b[0m, in \u001b[0;36mstep_env_and_add_to_buffer\u001b[1;34m(env, obs, agent, agent_kwargs, replay_buffer, callback, agent_uses_low_dim_obs)\u001b[0m\n\u001b[0;32m    604\u001b[0m     agent_obs \u001b[38;5;241m=\u001b[39m obs\n\u001b[0;32m    605\u001b[0m \u001b[38;5;66;03m# env.pause_simulation()\u001b[39;00m\n\u001b[1;32m--> 606\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(agent_obs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39magent_kwargs)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;66;03m# env.resume_simulation()\u001b[39;00m\n\u001b[0;32m    608\u001b[0m next_obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32mc:\\users\\ksubh\\onedrive\\documents\\swarmslab_rl\\summer 25\\implementations\\mbrl_lib_maintained\\mbrl-lib-experiments\\mbrl\\planning\\trajectory_opt.py:686\u001b[0m, in \u001b[0;36mTrajectoryOptimizerAgent.act\u001b[1;34m(self, obs, optimizer_callback, **_kwargs)\u001b[0m\n\u001b[0;32m    683\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrajectory_eval_fn(obs, action_sequences)\n\u001b[0;32m    685\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 686\u001b[0m plan \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrajectory_eval_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_callback\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    689\u001b[0m plan_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions_to_use\u001b[38;5;241m.\u001b[39mextend([a \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m plan[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplan_freq]])\n",
      "File \u001b[1;32mc:\\users\\ksubh\\onedrive\\documents\\swarmslab_rl\\summer 25\\implementations\\mbrl_lib_maintained\\mbrl-lib-experiments\\mbrl\\planning\\trajectory_opt.py:558\u001b[0m, in \u001b[0;36mTrajectoryOptimizer.optimize\u001b[1;34m(self, trajectory_eval_fn, callback)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    540\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    541\u001b[0m     trajectory_eval_fn: Callable[[torch\u001b[38;5;241m.\u001b[39mTensor], torch\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[0;32m    542\u001b[0m     callback: Optional[Callable] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    543\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m    544\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Runs the trajectory optimization.\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \n\u001b[0;32m    546\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;124;03m        (tuple of np.ndarray and float): the best action sequence.\u001b[39;00m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 558\u001b[0m     best_solution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrajectory_eval_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprevious_solution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    561\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    563\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_last_solution:\n\u001b[0;32m    564\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprevious_solution \u001b[38;5;241m=\u001b[39m best_solution\u001b[38;5;241m.\u001b[39mroll(\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplan_freq, dims\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\users\\ksubh\\onedrive\\documents\\swarmslab_rl\\summer 25\\implementations\\mbrl_lib_maintained\\mbrl-lib-experiments\\mbrl\\planning\\trajectory_opt.py:172\u001b[0m, in \u001b[0;36mCEMOptimizer.optimize\u001b[1;34m(self, obj_fun, x0, callback, **kwargs)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_iterations):\n\u001b[0;32m    171\u001b[0m     population \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample_population(mu, dispersion, population)\n\u001b[1;32m--> 172\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mobj_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpopulation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m         callback(population, values, i)\n",
      "File \u001b[1;32mc:\\users\\ksubh\\onedrive\\documents\\swarmslab_rl\\summer 25\\implementations\\mbrl_lib_maintained\\mbrl-lib-experiments\\mbrl\\planning\\trajectory_opt.py:683\u001b[0m, in \u001b[0;36mTrajectoryOptimizerAgent.act.<locals>.trajectory_eval_fn\u001b[1;34m(action_sequences)\u001b[0m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrajectory_eval_fn\u001b[39m(action_sequences):\n\u001b[1;32m--> 683\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrajectory_eval_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_sequences\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\ksubh\\onedrive\\documents\\swarmslab_rl\\summer 25\\implementations\\mbrl_lib_maintained\\mbrl-lib-experiments\\mbrl\\planning\\trajectory_opt.py:746\u001b[0m, in \u001b[0;36mcreate_trajectory_optim_agent_for_model.<locals>.trajectory_eval_fn\u001b[1;34m(initial_state, action_sequences)\u001b[0m\n\u001b[0;32m    745\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrajectory_eval_fn\u001b[39m(initial_state, action_sequences):\n\u001b[1;32m--> 746\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_action_sequences\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_sequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_particles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_particles\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ksubh\\OneDrive\\Documents\\SwarmsLab_RL\\Summer 25\\Implementations\\mbrl_lib_maintained\\mbrl-lib-experiments\\modquad_exp\\HModQuad_untracked\\models\\modquad_ModelEnv.py:185\u001b[0m, in \u001b[0;36mmodquad_ModelEnv.evaluate_action_sequences\u001b[1;34m(self, action_sequences, initial_state, num_particles)\u001b[0m\n\u001b[0;32m    181\u001b[0m action_for_step \u001b[38;5;241m=\u001b[39m action_sequences[:, time_step, :]\n\u001b[0;32m    182\u001b[0m action_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrepeat_interleave(\n\u001b[0;32m    183\u001b[0m     action_for_step, num_particles, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    184\u001b[0m )\n\u001b[1;32m--> 185\u001b[0m _, rewards, dones, model_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43maction_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplanning_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplanning_step\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m rewards[terminated] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    189\u001b[0m terminated \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m dones\n",
      "File \u001b[1;32mc:\\Users\\ksubh\\OneDrive\\Documents\\SwarmsLab_RL\\Summer 25\\Implementations\\mbrl_lib_maintained\\mbrl-lib-experiments\\modquad_exp\\HModQuad_untracked\\models\\modquad_ModelEnv.py:118\u001b[0m, in \u001b[0;36mmodquad_ModelEnv.step\u001b[1;34m(self, actions, model_state, sample, planning_step)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(actions, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m    112\u001b[0m     actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(actions)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    113\u001b[0m (\n\u001b[0;32m    114\u001b[0m     next_observs,\n\u001b[0;32m    115\u001b[0m     pred_rewards,\n\u001b[0;32m    116\u001b[0m     pred_terminals,\n\u001b[0;32m    117\u001b[0m     next_model_state,\n\u001b[1;32m--> 118\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdynamics_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m rewards \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    125\u001b[0m     pred_rewards\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward_fn(actions, next_observs, planning_step)\n\u001b[0;32m    128\u001b[0m )\n\u001b[0;32m    129\u001b[0m dones \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtermination_fn(actions, next_observs, planning_step)\n",
      "File \u001b[1;32mc:\\users\\ksubh\\onedrive\\documents\\swarmslab_rl\\summer 25\\implementations\\mbrl_lib_maintained\\mbrl-lib-experiments\\mbrl\\models\\one_dim_tr_model.py:278\u001b[0m, in \u001b[0;36mOneDTransitionRewardModel.sample\u001b[1;34m(self, act, model_state, deterministic, rng)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_1d\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOneDTransitionRewardModel requires wrapped model to define method sample_1d\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    277\u001b[0m     )\n\u001b[1;32m--> 278\u001b[0m preds, next_model_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m next_observs \u001b[38;5;241m=\u001b[39m preds[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearned_rewards \u001b[38;5;28;01melse\u001b[39;00m preds\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_is_delta:\n",
      "File \u001b[1;32mc:\\users\\ksubh\\onedrive\\documents\\swarmslab_rl\\summer 25\\implementations\\mbrl_lib_maintained\\mbrl-lib-experiments\\mbrl\\models\\model.py:468\u001b[0m, in \u001b[0;36mEnsemble.sample_1d\u001b[1;34m(self, model_input, model_state, deterministic, rng)\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    460\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\n\u001b[0;32m    461\u001b[0m             model_input,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    465\u001b[0m         model_state,\n\u001b[0;32m    466\u001b[0m     )\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m rng \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 468\u001b[0m means, logvars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpropagation_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpropagation_indices\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    471\u001b[0m variances \u001b[38;5;241m=\u001b[39m logvars\u001b[38;5;241m.\u001b[39mexp()\n\u001b[0;32m    472\u001b[0m stds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(variances)\n",
      "File \u001b[1;32mc:\\users\\ksubh\\onedrive\\documents\\swarmslab_rl\\summer 25\\implementations\\mbrl_lib_maintained\\mbrl-lib-experiments\\mbrl\\models\\gaussian_mlp.py:278\u001b[0m, in \u001b[0;36mGaussianMLP.forward\u001b[1;34m(self, x, rng, propagation_indices, use_propagation)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Computes mean and logvar predictions for the given input.\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \n\u001b[0;32m    227\u001b[0m \u001b[38;5;124;03mWhen ``self.num_members > 1``, the model supports uncertainty propagation options\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m \n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_propagation:\n\u001b[1;32m--> 278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_ensemble\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpropagation_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpropagation_indices\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_forward(x)\n",
      "File \u001b[1;32mc:\\users\\ksubh\\onedrive\\documents\\swarmslab_rl\\summer 25\\implementations\\mbrl_lib_maintained\\mbrl-lib-experiments\\mbrl\\models\\gaussian_mlp.py:212\u001b[0m, in \u001b[0;36mGaussianMLP._forward_ensemble\u001b[1;34m(self, x, rng, propagation_indices)\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m propagation_indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    210\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen using propagation=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfixed_model\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, `propagation_indices` must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    211\u001b[0m         )\n\u001b[1;32m--> 212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_from_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpropagation_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpropagation_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpectation\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    214\u001b[0m     mean, logvar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_forward(x, only_elite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\users\\ksubh\\onedrive\\documents\\swarmslab_rl\\summer 25\\implementations\\mbrl_lib_maintained\\mbrl-lib-experiments\\mbrl\\models\\gaussian_mlp.py:168\u001b[0m, in \u001b[0;36mGaussianMLP._forward_from_indices\u001b[1;34m(self, x, model_shuffle_indices)\u001b[0m\n\u001b[0;32m    161\u001b[0m num_models \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39melite_models) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39melite_models \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    163\u001b[0m )\n\u001b[0;32m    164\u001b[0m shuffled_x \u001b[38;5;241m=\u001b[39m x[:, model_shuffle_indices, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\n\u001b[0;32m    165\u001b[0m     num_models, batch_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_models, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    166\u001b[0m )\n\u001b[1;32m--> 168\u001b[0m mean, logvar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_default_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshuffled_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monly_elite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# note that mean and logvar are shuffled\u001b[39;00m\n\u001b[0;32m    170\u001b[0m mean \u001b[38;5;241m=\u001b[39m mean\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\users\\ksubh\\onedrive\\documents\\swarmslab_rl\\summer 25\\implementations\\mbrl_lib_maintained\\mbrl-lib-experiments\\mbrl\\models\\gaussian_mlp.py:144\u001b[0m, in \u001b[0;36mGaussianMLP._default_forward\u001b[1;34m(self, x, only_elite, **_kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_default_forward\u001b[39m(\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor, only_elite: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs\n\u001b[0;32m    142\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, Optional[torch\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_toggle_layers_use_only_elite(only_elite)\n\u001b[1;32m--> 144\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m     mean_and_logvar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_and_logvar(x)\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_toggle_layers_use_only_elite(only_elite)\n",
      "File \u001b[1;32mc:\\Users\\ksubh\\anaconda3\\envs\\modquad_exp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ksubh\\anaconda3\\envs\\modquad_exp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ksubh\\anaconda3\\envs\\modquad_exp\\lib\\site-packages\\torch\\nn\\modules\\container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ksubh\\anaconda3\\envs\\modquad_exp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ksubh\\anaconda3\\envs\\modquad_exp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ksubh\\anaconda3\\envs\\modquad_exp\\lib\\site-packages\\torch\\nn\\modules\\container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ksubh\\anaconda3\\envs\\modquad_exp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ksubh\\anaconda3\\envs\\modquad_exp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\ksubh\\onedrive\\documents\\swarmslab_rl\\summer 25\\implementations\\mbrl_lib_maintained\\mbrl-lib-experiments\\mbrl\\models\\util.py:61\u001b[0m, in \u001b[0;36mEnsembleLinearLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m xw\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 61\u001b[0m     xw \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_bias:\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m xw \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x375 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_scores = []\n",
    "train_time = []\n",
    "plan_time = []\n",
    "\n",
    "def train_callback(_model, _total_calls, _epoch, tr_loss, val_score, _best_val):\n",
    "    train_losses.append(tr_loss)\n",
    "    val_scores.append(val_score.mean().item())   # this returns val score per ensemble model\n",
    "\n",
    "\n",
    "# Create a trainer for the model\n",
    "model_trainer = models.ModelTrainer(dynamics_model, optim_lr=5e-5, weight_decay=5e-5)\n",
    "env.initialize_target_trajectory(traj = \"random trajectory\", num_waypoints=3) \n",
    "# Create visualization objects\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 3.75), gridspec_kw={\"width_ratios\": [1, 1]})\n",
    "ax_text = axs[0].text(300, 50, \"\")\n",
    "    \n",
    "# the states will encapsulate the desired trajectory \\\n",
    "\n",
    "# Main PETS loop\n",
    "all_rewards = [0]\n",
    "trajectory_length, total_time, pos_traj, orient_traj = env.initialize_target_trajectory(traj = \"random trajectory\", std_position_change=0.5, num_waypoints=3, num_hover_points=2, time_step_duration=5)\n",
    "#     trajectory parameters, default arguments used for random trajectory\n",
    "#     start_pos=[0.0, 0.0, 1.0], start_yaw=0.0, \n",
    "#     start_vel=[0.0, 0.0, 0.0], start_yaw_rate=0.0,\n",
    "#     std_position_change=0.2,\n",
    "#     std_orientation_change=0.1,\n",
    "#     std_velocity_change=0.05,\n",
    "#     std_angular_velocity_change=0.0,\n",
    "#     std_acceleration_change=0.0,\n",
    "#     std_angular_acceleration_change=0.0,\n",
    "#     num_waypoints=20, \n",
    "#     num_hover_points=3,\n",
    "#     time_step_duration=10\n",
    "\n",
    "model_env.set_desired_trajectory(total_time, pos_traj, orient_traj)\n",
    "model_env.trajectory_step = 0\n",
    "# logger._write_to_log(\"\\nStarting main experiment loop...\")\n",
    "\n",
    "for trial in range(num_trials):\n",
    "    \n",
    "    print(\"resetting environment, and starting trial :\", trial)\n",
    "    obs, _ = env.reset()    \n",
    "    agent.reset()\n",
    "    \n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    total_reward = 0.0\n",
    "    steps_trial = 0\n",
    "\n",
    "    traj_step = 0 \n",
    "    model_env.trajectory_step = 0\n",
    "    last_setpoint_set = time.time()\n",
    "    time.sleep(0.5)\n",
    "    # env.pause_simulation()\n",
    "\n",
    "    # update_axes(axs, env.render(), ax_text, trial, steps_trial, all_rewards)\n",
    "    while not (terminated or truncated or (model_env.trajectory_step >= trajectory_length-1)):\n",
    "\n",
    "        if (time.time() - last_setpoint_set > total_time/trajectory_length) :\n",
    "            # env.resume_simulation()\n",
    "            model_env.trajectory_step += 1\n",
    "            last_setpoint_set = time.time()\n",
    "            print(\"setpoint updated\")\n",
    "            env.update_setpoint(model_env.trajectory_step)\n",
    "            \n",
    "            \n",
    "\n",
    "            # env.pause_simulation()\n",
    "\n",
    "        \n",
    "        # --------------- Model Training -----------------\n",
    "        if steps_trial == 0:\n",
    "            print(\"Number of stored transitions: \", replay_buffer.num_stored)\n",
    "            dynamics_model.update_normalizer(replay_buffer.get_all())  # update normalizer stats\n",
    "            \n",
    "            dataset_train, dataset_val = common_util.get_basic_buffer_iterators(\n",
    "                replay_buffer,\n",
    "                batch_size=cfg.overrides.model_batch_size,\n",
    "                val_ratio=cfg.overrides.validation_ratio,\n",
    "                ensemble_size=ensemble_size,\n",
    "                shuffle_each_epoch=True,\n",
    "                bootstrap_permutes=False,  # build bootstrap dataset using sampling with replacement \n",
    "            )\n",
    "            print(\"Training model\")\n",
    "            training_start_time = time.time()\n",
    "            model_trainer.train(\n",
    "                dataset_train, \n",
    "                dataset_val=dataset_val, \n",
    "                num_epochs=50, \n",
    "                patience=50, \n",
    "                callback=train_callback,\n",
    "                silent=True)\n",
    "            print(\"training loss: \", train_losses[-1])\n",
    "            print(\"validation loss: \", val_scores[-1])\n",
    "            print(\"training time: \", time.time() - training_start_time)\n",
    "            print(\"Model trained\")\n",
    "\n",
    "        # --- Doing env step using the agent and adding to model dataset ---\n",
    "        print(\"planning for trajectory step: \", model_env.trajectory_step)\n",
    "        planning_start_time = time.time()\n",
    "        next_obs, reward, terminated, truncated, _ = common_util.step_env_and_add_to_buffer(\n",
    "            env, obs, agent, {}, replay_buffer)\n",
    "        print(\"planning time: \", time.time() - planning_start_time)\n",
    "        \n",
    "            \n",
    "        # update_axes(\n",
    "        #     axs, env.render(), ax_text, trial, steps_trial, all_rewards)\n",
    "        \n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "        steps_trial += 1\n",
    "\n",
    "    env.end_simulation()\n",
    "        \n",
    "    # if steps_trial == trial_length:\n",
    "    #         break\n",
    "    \n",
    "    all_rewards.append(total_reward)\n",
    "\n",
    "# update_axes(axs, env.render(), ax_text, trial, steps_trial, all_rewards, force_update=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd428bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modquad_exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
